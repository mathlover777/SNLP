<?xml version="1.0" encoding="UTF-8"?>
<paper>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The precision, recall and average parsing time for the Penn treebank (Section 23) were 87.85%, 86.85%, and 360 ms, respectively.</prevsent>
            <prevsent>We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank.</prevsent>
         </prevsection>
         <citsent citstr=" W97-0302 ">
            We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997
            <papid>W97-0302</papid>
            ), and iterative parsing (Tsuruoka and Tsujii, 2005b).
         </citsent>
         <aftsection>
            <nextsent>
               Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al, 2000), large constituent inhibition (Kaplan et al, 2004
               <papid>N04-1013</papid>
               ) and hybrid parsing with a CFG chunk parser (Daum et al, 2003
               <papid>E03-1052</papid>
               ; Frank et al, 2003
               <papid>P03-1014</papid>
               ; Frank, 2004
               <papid>C04-1185</papid>
               ).The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank.
            </nextsent>
            <nextsent>Unification-based grammars have been extensively studied in terms of linguistic formulation and computation efficiency.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank.</prevsent>
            <prevsent>
               We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997
               <papid>W97-0302</papid>
               ), and iterative parsing (Tsuruoka and Tsujii, 2005b).
            </prevsent>
         </prevsection>
         <citsent citstr=" N04-1013 ">
            Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al, 2000), large constituent inhibition (Kaplan et al, 2004
            <papid>N04-1013</papid>
            ) and hybrid parsing with a CFG chunk parser (Daum et al, 2003
            <papid>E03-1052</papid>
            ; Frank et al, 2003
            <papid>P03-1014</papid>
            ; Frank, 2004
            <papid>C04-1185</papid>
            ).The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank.
         </citsent>
         <aftsection>
            <nextsent>Unification-based grammars have been extensively studied in terms of linguistic formulation and computation efficiency.</nextsent>
            <nextsent>Although they provide precise linguistic structures of sentences, their processing is considered expensive because of the detailed descriptions.</nextsent>
            <nextsent>Since efficiency is of particular concern in practical applications, a number of studies have focused on improving the parsing efficiency of unification based grammars (Oepen et al, 2002).</nextsent>
            <nextsent>Although significant improvements inefficiency have been made, parsing speed is still not high enough for practical applications.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank.</prevsent>
            <prevsent>
               We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997
               <papid>W97-0302</papid>
               ), and iterative parsing (Tsuruoka and Tsujii, 2005b).
            </prevsent>
         </prevsection>
         <citsent citstr=" E03-1052 ">
            Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al, 2000), large constituent inhibition (Kaplan et al, 2004
            <papid>N04-1013</papid>
            ) and hybrid parsing with a CFG chunk parser (Daum et al, 2003
            <papid>E03-1052</papid>
            ; Frank et al, 2003
            <papid>P03-1014</papid>
            ; Frank, 2004
            <papid>C04-1185</papid>
            ).The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank.
         </citsent>
         <aftsection>
            <nextsent>Unification-based grammars have been extensively studied in terms of linguistic formulation and computation efficiency.</nextsent>
            <nextsent>Although they provide precise linguistic structures of sentences, their processing is considered expensive because of the detailed descriptions.</nextsent>
            <nextsent>Since efficiency is of particular concern in practical applications, a number of studies have focused on improving the parsing efficiency of unification based grammars (Oepen et al, 2002).</nextsent>
            <nextsent>Although significant improvements inefficiency have been made, parsing speed is still not high enough for practical applications.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank.</prevsent>
            <prevsent>
               We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997
               <papid>W97-0302</papid>
               ), and iterative parsing (Tsuruoka and Tsujii, 2005b).
            </prevsent>
         </prevsection>
         <citsent citstr=" P03-1014 ">
            Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al, 2000), large constituent inhibition (Kaplan et al, 2004
            <papid>N04-1013</papid>
            ) and hybrid parsing with a CFG chunk parser (Daum et al, 2003
            <papid>E03-1052</papid>
            ; Frank et al, 2003
            <papid>P03-1014</papid>
            ; Frank, 2004
            <papid>C04-1185</papid>
            ).The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank.
         </citsent>
         <aftsection>
            <nextsent>Unification-based grammars have been extensively studied in terms of linguistic formulation and computation efficiency.</nextsent>
            <nextsent>Although they provide precise linguistic structures of sentences, their processing is considered expensive because of the detailed descriptions.</nextsent>
            <nextsent>Since efficiency is of particular concern in practical applications, a number of studies have focused on improving the parsing efficiency of unification based grammars (Oepen et al, 2002).</nextsent>
            <nextsent>Although significant improvements inefficiency have been made, parsing speed is still not high enough for practical applications.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank.</prevsent>
            <prevsent>
               We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997
               <papid>W97-0302</papid>
               ), and iterative parsing (Tsuruoka and Tsujii, 2005b).
            </prevsent>
         </prevsection>
         <citsent citstr=" C04-1185 ">
            Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al, 2000), large constituent inhibition (Kaplan et al, 2004
            <papid>N04-1013</papid>
            ) and hybrid parsing with a CFG chunk parser (Daum et al, 2003
            <papid>E03-1052</papid>
            ; Frank et al, 2003
            <papid>P03-1014</papid>
            ; Frank, 2004
            <papid>C04-1185</papid>
            ).The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank.
         </citsent>
         <aftsection>
            <nextsent>Unification-based grammars have been extensively studied in terms of linguistic formulation and computation efficiency.</nextsent>
            <nextsent>Although they provide precise linguistic structures of sentences, their processing is considered expensive because of the detailed descriptions.</nextsent>
            <nextsent>Since efficiency is of particular concern in practical applications, a number of studies have focused on improving the parsing efficiency of unification based grammars (Oepen et al, 2002).</nextsent>
            <nextsent>Although significant improvements inefficiency have been made, parsing speed is still not high enough for practical applications.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Since efficiency is of particular concern in practical applications, a number of studies have focused on improving the parsing efficiency of unification based grammars (Oepen et al, 2002).</prevsent>
            <prevsent>Although significant improvements inefficiency have been made, parsing speed is still not high enough for practical applications.</prevsent>
         </prevsection>
         <citsent citstr=" N04-1013 ">
            The recent introduction of probabilistic models of wide-coverage unification-based grammars (Malouf and van Noord, 2004; Kaplan et al, 2004
            <papid>N04-1013</papid>
            ; Miyaoand Tsujii, 2005) has opened up the novel possibility of increasing parsing speed by guiding the search path using probabilities.
         </citsent>
         <aftsection>
            <nextsent>That is, since we often require only the most probable parse result, we can compute partial parse results that are likely to contribute to the final parse result.</nextsent>
            <nextsent>This approach has been extensively studied in the field of probabilistic 103 CFG (PCFG) parsing, such as Viterbi parsing and beam thresholding.</nextsent>
            <nextsent>While many methods of probabilistic parsing for unification-based grammars have been developed,their strategy is to first perform exhaustive parsing without using probabilities and then select the highest probability parse.</nextsent>
            <nextsent>The behavior of their algorithms is like that of the Viterbi algorithm forPCFG parsing, so the correct parse with the highest probability is guaranteed.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The behavior of their algorithms is like that of the Viterbi algorithm forPCFG parsing, so the correct parse with the highest probability is guaranteed.</prevsent>
            <prevsent>The interesting pointof this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest.</prevsent>
         </prevsection>
         <citsent citstr=" J93-1002 ">
            Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al, 1996; Briscoe and Carroll, 1993
            <papid>J93-1002</papid>
            ; Kiefer et al, 2002
            <papid>C02-1075</papid>
            ), and the most probable parse is found by PCFG parsing.This model is based on PCFG and not probabilistic unification-based grammar parsing.
         </citsent>
         <aftsection>
            <nextsent>
               Geman and johnson (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest.
            </nextsent>
            <nextsent>However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The behavior of their algorithms is like that of the Viterbi algorithm forPCFG parsing, so the correct parse with the highest probability is guaranteed.</prevsent>
            <prevsent>The interesting pointof this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest.</prevsent>
         </prevsection>
         <citsent citstr=" C02-1075 ">
            Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al, 1996; Briscoe and Carroll, 1993
            <papid>J93-1002</papid>
            ; Kiefer et al, 2002
            <papid>C02-1075</papid>
            ), and the most probable parse is found by PCFG parsing.This model is based on PCFG and not probabilistic unification-based grammar parsing.
         </citsent>
         <aftsection>
            <nextsent>
               Geman and johnson (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest.
            </nextsent>
            <nextsent>However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The interesting pointof this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest.</prevsent>
            <prevsent>
               Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al, 1996; Briscoe and Carroll, 1993
               <papid>J93-1002</papid>
               ; Kiefer et al, 2002
               <papid>C02-1075</papid>
               ), and the most probable parse is found by PCFG parsing.This model is based on PCFG and not probabilistic unification-based grammar parsing.
            </prevsent>
         </prevsection>
         <citsent citstr=" P02-1036 ">
            Geman and johnson (Geman and Johnson, 2002
            <papid>P02-1036</papid>
            ) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest.
         </citsent>
         <aftsection>
            <nextsent>However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing.</nextsent>
            <nextsent>In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-scale corpora, the Penn treebank.</nextsent>
            <nextsent>We show how techniques developed for efficient deep parsing can improve the efficiency of probabilistic parsing.</nextsent>
            <nextsent>
               These techniques were evaluated in experiments on the Penn Treebank (Marcuset al, 1994) with the wide-coverage HPSG parser developed by Miyao et al (Miyao et al, 2005; Miyao and Tsujii, 2005
               <papid>P05-1011</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-scale corpora, the Penn treebank.</prevsent>
            <prevsent>We show how techniques developed for efficient deep parsing can improve the efficiency of probabilistic parsing.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1011 ">
            These techniques were evaluated in experiments on the Penn Treebank (Marcuset al, 1994) with the wide-coverage HPSG parser developed by Miyao et al (Miyao et al, 2005; Miyao and Tsujii, 2005
            <papid>P05-1011</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               HPSG (Pollard and Sag, 1994
               <papid>P85-1021</papid>
               ) is a syntactic theory based on lexicalized grammar formalism.
            </nextsent>
            <nextsent>In HPSG,a small number of schemata describe general construction rules, and a large number of lexical entries express word-specific characteristics.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>HPSG and probabilistic models.</section>
      <citcontext>
         <prevsection>
            <prevsent>The sign of the larger constituent is obtained by repeatedly applying schemata to lexi cal/phrasal signs.</prevsent>
            <prevsent>Finally, the parse result is output as a phrasal sign that dominates the sentence.Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = L,R?, where L = {l = w,F ?|w ? W, F ? F} is a set of lexical entries, and R is a set of schemata, i.e., r ? R is a partial function: F ? F ? F . Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing.</prevsent>
         </prevsection>
         <citsent citstr=" J97-4005 ">
            Previous studies (Abney, 1997
            <papid>J97-4005</papid>
            ; Johnson et al,1999; Riezler et al, 2000
            <papid>P00-1061</papid>
            ; Miyao et al, 2003; Malouf and van Noord, 2004; Kaplan et al, 2004
            <papid>N04-1013</papid>
            ; Miyao and Tsujii, 2005
            <papid>P05-1011</papid>
            ) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al, 1996
            <papid>J96-1002</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The probability of parse result T assigned to given sentence w = w1, . . .</nextsent>
            <nextsent>, wn?</nextsent>
            <nextsent>
               is p(T |w) = 1Zw exp ( ? i ifi(T ) ) Zw = ? T ? exp ( ? i ifi(T ?) ) , where i is a model parameter, and fi is a feature function that represents a characteristic of parse treeT . Intuitively, the probability is defined as the normalized product of the weights exp(i) when a characteristic corresponding to fi appears in parse resultT . Model parameters i are estimated using numer 104 ical optimization methods (Malouf, 2002
               <papid>W02-2018</papid>
               ) so as to maximize the log-likelihood of the training data.However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences.To make the model estimation tractable, Ge man and Johnson (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w).
            </nextsent>
            <nextsent>They assumed that features are functions on nodes in a packed parse forest.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>HPSG and probabilistic models.</section>
      <citcontext>
         <prevsection>
            <prevsent>The sign of the larger constituent is obtained by repeatedly applying schemata to lexi cal/phrasal signs.</prevsent>
            <prevsent>Finally, the parse result is output as a phrasal sign that dominates the sentence.Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = L,R?, where L = {l = w,F ?|w ? W, F ? F} is a set of lexical entries, and R is a set of schemata, i.e., r ? R is a partial function: F ? F ? F . Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing.</prevsent>
         </prevsection>
         <citsent citstr=" P00-1061 ">
            Previous studies (Abney, 1997
            <papid>J97-4005</papid>
            ; Johnson et al,1999; Riezler et al, 2000
            <papid>P00-1061</papid>
            ; Miyao et al, 2003; Malouf and van Noord, 2004; Kaplan et al, 2004
            <papid>N04-1013</papid>
            ; Miyao and Tsujii, 2005
            <papid>P05-1011</papid>
            ) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al, 1996
            <papid>J96-1002</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The probability of parse result T assigned to given sentence w = w1, . . .</nextsent>
            <nextsent>, wn?</nextsent>
            <nextsent>
               is p(T |w) = 1Zw exp ( ? i ifi(T ) ) Zw = ? T ? exp ( ? i ifi(T ?) ) , where i is a model parameter, and fi is a feature function that represents a characteristic of parse treeT . Intuitively, the probability is defined as the normalized product of the weights exp(i) when a characteristic corresponding to fi appears in parse resultT . Model parameters i are estimated using numer 104 ical optimization methods (Malouf, 2002
               <papid>W02-2018</papid>
               ) so as to maximize the log-likelihood of the training data.However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences.To make the model estimation tractable, Ge man and Johnson (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w).
            </nextsent>
            <nextsent>They assumed that features are functions on nodes in a packed parse forest.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>HPSG and probabilistic models.</section>
      <citcontext>
         <prevsection>
            <prevsent>The sign of the larger constituent is obtained by repeatedly applying schemata to lexi cal/phrasal signs.</prevsent>
            <prevsent>Finally, the parse result is output as a phrasal sign that dominates the sentence.Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = L,R?, where L = {l = w,F ?|w ? W, F ? F} is a set of lexical entries, and R is a set of schemata, i.e., r ? R is a partial function: F ? F ? F . Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing.</prevsent>
         </prevsection>
         <citsent citstr=" N04-1013 ">
            Previous studies (Abney, 1997
            <papid>J97-4005</papid>
            ; Johnson et al,1999; Riezler et al, 2000
            <papid>P00-1061</papid>
            ; Miyao et al, 2003; Malouf and van Noord, 2004; Kaplan et al, 2004
            <papid>N04-1013</papid>
            ; Miyao and Tsujii, 2005
            <papid>P05-1011</papid>
            ) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al, 1996
            <papid>J96-1002</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The probability of parse result T assigned to given sentence w = w1, . . .</nextsent>
            <nextsent>, wn?</nextsent>
            <nextsent>
               is p(T |w) = 1Zw exp ( ? i ifi(T ) ) Zw = ? T ? exp ( ? i ifi(T ?) ) , where i is a model parameter, and fi is a feature function that represents a characteristic of parse treeT . Intuitively, the probability is defined as the normalized product of the weights exp(i) when a characteristic corresponding to fi appears in parse resultT . Model parameters i are estimated using numer 104 ical optimization methods (Malouf, 2002
               <papid>W02-2018</papid>
               ) so as to maximize the log-likelihood of the training data.However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences.To make the model estimation tractable, Ge man and Johnson (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w).
            </nextsent>
            <nextsent>They assumed that features are functions on nodes in a packed parse forest.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>HPSG and probabilistic models.</section>
      <citcontext>
         <prevsection>
            <prevsent>The sign of the larger constituent is obtained by repeatedly applying schemata to lexi cal/phrasal signs.</prevsent>
            <prevsent>Finally, the parse result is output as a phrasal sign that dominates the sentence.Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = L,R?, where L = {l = w,F ?|w ? W, F ? F} is a set of lexical entries, and R is a set of schemata, i.e., r ? R is a partial function: F ? F ? F . Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1011 ">
            Previous studies (Abney, 1997
            <papid>J97-4005</papid>
            ; Johnson et al,1999; Riezler et al, 2000
            <papid>P00-1061</papid>
            ; Miyao et al, 2003; Malouf and van Noord, 2004; Kaplan et al, 2004
            <papid>N04-1013</papid>
            ; Miyao and Tsujii, 2005
            <papid>P05-1011</papid>
            ) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al, 1996
            <papid>J96-1002</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The probability of parse result T assigned to given sentence w = w1, . . .</nextsent>
            <nextsent>, wn?</nextsent>
            <nextsent>
               is p(T |w) = 1Zw exp ( ? i ifi(T ) ) Zw = ? T ? exp ( ? i ifi(T ?) ) , where i is a model parameter, and fi is a feature function that represents a characteristic of parse treeT . Intuitively, the probability is defined as the normalized product of the weights exp(i) when a characteristic corresponding to fi appears in parse resultT . Model parameters i are estimated using numer 104 ical optimization methods (Malouf, 2002
               <papid>W02-2018</papid>
               ) so as to maximize the log-likelihood of the training data.However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences.To make the model estimation tractable, Ge man and Johnson (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w).
            </nextsent>
            <nextsent>They assumed that features are functions on nodes in a packed parse forest.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>HPSG and probabilistic models.</section>
      <citcontext>
         <prevsection>
            <prevsent>The probability of parse result T assigned to given sentence w = w1, . . .</prevsent>
            <prevsent>, wn?</prevsent>
         </prevsection>
         <citsent citstr=" W02-2018 ">
            is p(T |w) = 1Zw exp ( ? i ifi(T ) ) Zw = ? T ? exp ( ? i ifi(T ?) ) , where i is a model parameter, and fi is a feature function that represents a characteristic of parse treeT . Intuitively, the probability is defined as the normalized product of the weights exp(i) when a characteristic corresponding to fi appears in parse resultT . Model parameters i are estimated using numer 104 ical optimization methods (Malouf, 2002
            <papid>W02-2018</papid>
            ) so as to maximize the log-likelihood of the training data.However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences.To make the model estimation tractable, Ge man and Johnson (Geman and Johnson, 2002
            <papid>P02-1036</papid>
            ) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w).
         </citsent>
         <aftsection>
            <nextsent>They assumed that features are functions on nodes in a packed parse forest.</nextsent>
            <nextsent>That is, parse tree T is represented by a set of nodes, i.e., T = {c}, and the parse forest is represented by an and/or graph of the nodes.</nextsent>
            <nextsent>From this assumption, we can redefine the probability as p(T |w) = 1Zw exp ( ? cT ? i ifi(c) ) Zw = ? T ? exp ( ? cT ? ?</nextsent>
            <nextsent>i ifi(c) ) . A packed parse forest has a structure similar to a chart of CFG parsing, and c corresponds to an edge in the chart.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>HPSG and probabilistic models.</section>
      <citcontext>
         <prevsection>
            <prevsent>The probability of parse result T assigned to given sentence w = w1, . . .</prevsent>
            <prevsent>, wn?</prevsent>
         </prevsection>
         <citsent citstr=" P02-1036 ">
            is p(T |w) = 1Zw exp ( ? i ifi(T ) ) Zw = ? T ? exp ( ? i ifi(T ?) ) , where i is a model parameter, and fi is a feature function that represents a characteristic of parse treeT . Intuitively, the probability is defined as the normalized product of the weights exp(i) when a characteristic corresponding to fi appears in parse resultT . Model parameters i are estimated using numer 104 ical optimization methods (Malouf, 2002
            <papid>W02-2018</papid>
            ) so as to maximize the log-likelihood of the training data.However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences.To make the model estimation tractable, Ge man and Johnson (Geman and Johnson, 2002
            <papid>P02-1036</papid>
            ) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w).
         </citsent>
         <aftsection>
            <nextsent>They assumed that features are functions on nodes in a packed parse forest.</nextsent>
            <nextsent>That is, parse tree T is represented by a set of nodes, i.e., T = {c}, and the parse forest is represented by an and/or graph of the nodes.</nextsent>
            <nextsent>From this assumption, we can redefine the probability as p(T |w) = 1Zw exp ( ? cT ? i ifi(c) ) Zw = ? T ? exp ( ? cT ? ?</nextsent>
            <nextsent>i ifi(c) ) . A packed parse forest has a structure similar to a chart of CFG parsing, and c corresponds to an edge in the chart.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Techniques for efficient deep.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               However, previous research (Miyao et al, 2003; Clark and Curran, 2004
               <papid>P04-1014</papid>
               ; Kaplan et al, 2004) showed that predicate-argument relations can be represented under the assumption of feature locality.
            </prevsent>
            <prevsent>We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results.</prevsent>
         </prevsection>
         <citsent citstr=" P85-1021 ">
            parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al, 1988), HPSG (Pollard and Sag, 1994
            <papid>P85-1021</papid>
            )or combinatory categorial grammar (CCG) (Steedman, 2000).
         </citsent>
         <aftsection>
            <nextsent>
               Most of them were developed forex haustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al, 1983; Maxwell and Kaplan, 1993
               <papid>J93-4001</papid>
               ; van Noord, 1997; Kiefer et al, 1999
               <papid>P99-1061</papid>
               ; Malouf et al, 2000; Torisawa et al, 2000; Oepen et al, 2002; Penn and Munteanu, 2003
               <papid>P03-1026</papid>
               ).
            </nextsent>
            <nextsent>The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Techniques for efficient deep.</section>
      <citcontext>
         <prevsection>
            <prevsent>We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results.</prevsent>
            <prevsent>
               parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al, 1988), HPSG (Pollard and Sag, 1994
               <papid>P85-1021</papid>
               )or combinatory categorial grammar (CCG) (Steedman, 2000).
            </prevsent>
         </prevsection>
         <citsent citstr=" J93-4001 ">
            Most of them were developed forex haustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al, 1983; Maxwell and Kaplan, 1993
            <papid>J93-4001</papid>
            ; van Noord, 1997; Kiefer et al, 1999
            <papid>P99-1061</papid>
            ; Malouf et al, 2000; Torisawa et al, 2000; Oepen et al, 2002; Penn and Munteanu, 2003
            <papid>P03-1026</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models.</nextsent>
            <nextsent>We tested three of these techniques.</nextsent>
            <nextsent>Quick check Quick check filters out non-unifiable feature structures (Malouf et al, 2000).</nextsent>
            <nextsent>Suppose we have two non-unifiable feature structures.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Techniques for efficient deep.</section>
      <citcontext>
         <prevsection>
            <prevsent>We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results.</prevsent>
            <prevsent>
               parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al, 1988), HPSG (Pollard and Sag, 1994
               <papid>P85-1021</papid>
               )or combinatory categorial grammar (CCG) (Steedman, 2000).
            </prevsent>
         </prevsection>
         <citsent citstr=" P99-1061 ">
            Most of them were developed forex haustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al, 1983; Maxwell and Kaplan, 1993
            <papid>J93-4001</papid>
            ; van Noord, 1997; Kiefer et al, 1999
            <papid>P99-1061</papid>
            ; Malouf et al, 2000; Torisawa et al, 2000; Oepen et al, 2002; Penn and Munteanu, 2003
            <papid>P03-1026</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models.</nextsent>
            <nextsent>We tested three of these techniques.</nextsent>
            <nextsent>Quick check Quick check filters out non-unifiable feature structures (Malouf et al, 2000).</nextsent>
            <nextsent>Suppose we have two non-unifiable feature structures.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Techniques for efficient deep.</section>
      <citcontext>
         <prevsection>
            <prevsent>We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results.</prevsent>
            <prevsent>
               parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al, 1988), HPSG (Pollard and Sag, 1994
               <papid>P85-1021</papid>
               )or combinatory categorial grammar (CCG) (Steedman, 2000).
            </prevsent>
         </prevsection>
         <citsent citstr=" P03-1026 ">
            Most of them were developed forex haustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al, 1983; Maxwell and Kaplan, 1993
            <papid>J93-4001</papid>
            ; van Noord, 1997; Kiefer et al, 1999
            <papid>P99-1061</papid>
            ; Malouf et al, 2000; Torisawa et al, 2000; Oepen et al, 2002; Penn and Munteanu, 2003
            <papid>P03-1026</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models.</nextsent>
            <nextsent>We tested three of these techniques.</nextsent>
            <nextsent>Quick check Quick check filters out non-unifiable feature structures (Malouf et al, 2000).</nextsent>
            <nextsent>Suppose we have two non-unifiable feature structures.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Techniques for efficient deep.</section>
      <citcontext>
         <prevsection>
            <prevsent>If it fails, then quick check returns a unification failure.</prevsent>
            <prevsent>If it succeeds, the signs are unified with the schemata, and there sult of unification is returned.</prevsent>
         </prevsection>
         <citsent citstr=" N04-1013 ">
            Large constituent inhibition (Kaplan et al, 2004
            <papid>N04-1013</papid>
            ) It is unlikely for a large medial edge to contribute to the final parsing result if it spans more than 20 words and is not adjacent to the beginning or ending of the sentence.
         </citsent>
         <aftsection>
            <nextsent>Large constituent inhibition prevents the parser from generating medial edges that span more than some word length.</nextsent>
            <nextsent>
               HPSG parsing with a CFG chunk parser A hybrid of deep parsing and shallow parsing was recently found to improve the efficiency of deep parsing (Daum et al, 2003
               <papid>E03-1052</papid>
               ; Frank et al., 2003; Frank, 2004
               <papid>C04-1185</papid>
               ).
            </nextsent>
            <nextsent>As a pre processor, the shallow parsing must be very fast and achieve high precision but not high recall so that the 105 procedure Viterbi(w1, . . .</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Techniques for efficient deep.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Large constituent inhibition (Kaplan et al, 2004
               <papid>N04-1013</papid>
               ) It is unlikely for a large medial edge to contribute to the final parsing result if it spans more than 20 words and is not adjacent to the beginning or ending of the sentence.
            </prevsent>
            <prevsent>Large constituent inhibition prevents the parser from generating medial edges that span more than some word length.</prevsent>
         </prevsection>
         <citsent citstr=" E03-1052 ">
            HPSG parsing with a CFG chunk parser A hybrid of deep parsing and shallow parsing was recently found to improve the efficiency of deep parsing (Daum et al, 2003
            <papid>E03-1052</papid>
            ; Frank et al., 2003; Frank, 2004
            <papid>C04-1185</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>As a pre processor, the shallow parsing must be very fast and achieve high precision but not high recall so that the 105 procedure Viterbi(w1, . . .</nextsent>
            <nextsent>, wn?, L?, R?, ?, ?, ?) for i = 1 to n foreach Fu ? {F |wi, F ? ?</nextsent>
            <nextsent>L} ? = ? i ifi(Fu) pi[i? 1, i]?</nextsent>
            <nextsent>pi[i? 1, i] ? {Fu} if (?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Techniques for efficient deep.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Large constituent inhibition (Kaplan et al, 2004
               <papid>N04-1013</papid>
               ) It is unlikely for a large medial edge to contribute to the final parsing result if it spans more than 20 words and is not adjacent to the beginning or ending of the sentence.
            </prevsent>
            <prevsent>Large constituent inhibition prevents the parser from generating medial edges that span more than some word length.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1185 ">
            HPSG parsing with a CFG chunk parser A hybrid of deep parsing and shallow parsing was recently found to improve the efficiency of deep parsing (Daum et al, 2003
            <papid>E03-1052</papid>
            ; Frank et al., 2003; Frank, 2004
            <papid>C04-1185</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>As a pre processor, the shallow parsing must be very fast and achieve high precision but not high recall so that the 105 procedure Viterbi(w1, . . .</nextsent>
            <nextsent>, wn?, L?, R?, ?, ?, ?) for i = 1 to n foreach Fu ? {F |wi, F ? ?</nextsent>
            <nextsent>L} ? = ? i ifi(Fu) pi[i? 1, i]?</nextsent>
            <nextsent>pi[i? 1, i] ? {Fu} if (?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Beam thresholding for HPSG.</section>
      <citcontext>
         <prevsection>
            <prevsent>parsing 4.1 Simple beam thresholding.</prevsent>
            <prevsent>Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated.</prevsent>
         </prevsection>
         <citsent citstr=" C94-1068 ">
            They include grammar compilation (Tomita, 1986
            <papid>C94-1068</papid>
            ; Nederhof, 2000
            <papid>J00-1003</papid>
            ), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or head corner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990
            <papid>W98-1115</papid>
            ;Caraballo and Charniak, 1998
            <papid>J98-2004</papid>
            ; Collins, 1999
            <papid>J03-4003</papid>
            ; Rat naparkhi, 1999; Charniak, 2000
            <papid>A00-2018</papid>
            ; Roark, 2001
            <papid>J01-2004</papid>
            ; Klein and Manning, 2003).
         </citsent>
         <aftsection>
            <nextsent>The beam search and best first search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse.The CYK algorithm, which is essentially a bottom up parser, is a natural choice for non-probabilisticHPSG parsers.</nextsent>
            <nextsent>Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing.For PCFG, extending the CYK algorithm to out put the Viterbi parse is straightforward (Ney, 1991;Jurafsky and Martin, 2000).</nextsent>
            <nextsent>The parser can efficiently calculate the Viterbi parse by taking the maximum of the probabilities of the same nonterminal symbol in each cell.</nextsent>
            <nextsent>
               With the probabilistic model defined in Section 2, we can also define the Viterbi search for unification-based grammars (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Beam thresholding for HPSG.</section>
      <citcontext>
         <prevsection>
            <prevsent>parsing 4.1 Simple beam thresholding.</prevsent>
            <prevsent>Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated.</prevsent>
         </prevsection>
         <citsent citstr=" J00-1003 ">
            They include grammar compilation (Tomita, 1986
            <papid>C94-1068</papid>
            ; Nederhof, 2000
            <papid>J00-1003</papid>
            ), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or head corner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990
            <papid>W98-1115</papid>
            ;Caraballo and Charniak, 1998
            <papid>J98-2004</papid>
            ; Collins, 1999
            <papid>J03-4003</papid>
            ; Rat naparkhi, 1999; Charniak, 2000
            <papid>A00-2018</papid>
            ; Roark, 2001
            <papid>J01-2004</papid>
            ; Klein and Manning, 2003).
         </citsent>
         <aftsection>
            <nextsent>The beam search and best first search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse.The CYK algorithm, which is essentially a bottom up parser, is a natural choice for non-probabilisticHPSG parsers.</nextsent>
            <nextsent>Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing.For PCFG, extending the CYK algorithm to out put the Viterbi parse is straightforward (Ney, 1991;Jurafsky and Martin, 2000).</nextsent>
            <nextsent>The parser can efficiently calculate the Viterbi parse by taking the maximum of the probabilities of the same nonterminal symbol in each cell.</nextsent>
            <nextsent>
               With the probabilistic model defined in Section 2, we can also define the Viterbi search for unification-based grammars (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Beam thresholding for HPSG.</section>
      <citcontext>
         <prevsection>
            <prevsent>parsing 4.1 Simple beam thresholding.</prevsent>
            <prevsent>Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated.</prevsent>
         </prevsection>
         <citsent citstr=" W98-1115 ">
            They include grammar compilation (Tomita, 1986
            <papid>C94-1068</papid>
            ; Nederhof, 2000
            <papid>J00-1003</papid>
            ), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or head corner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990
            <papid>W98-1115</papid>
            ;Caraballo and Charniak, 1998
            <papid>J98-2004</papid>
            ; Collins, 1999
            <papid>J03-4003</papid>
            ; Rat naparkhi, 1999; Charniak, 2000
            <papid>A00-2018</papid>
            ; Roark, 2001
            <papid>J01-2004</papid>
            ; Klein and Manning, 2003).
         </citsent>
         <aftsection>
            <nextsent>The beam search and best first search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse.The CYK algorithm, which is essentially a bottom up parser, is a natural choice for non-probabilisticHPSG parsers.</nextsent>
            <nextsent>Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing.For PCFG, extending the CYK algorithm to out put the Viterbi parse is straightforward (Ney, 1991;Jurafsky and Martin, 2000).</nextsent>
            <nextsent>The parser can efficiently calculate the Viterbi parse by taking the maximum of the probabilities of the same nonterminal symbol in each cell.</nextsent>
            <nextsent>
               With the probabilistic model defined in Section 2, we can also define the Viterbi search for unification-based grammars (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Beam thresholding for HPSG.</section>
      <citcontext>
         <prevsection>
            <prevsent>parsing 4.1 Simple beam thresholding.</prevsent>
            <prevsent>Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated.</prevsent>
         </prevsection>
         <citsent citstr=" J98-2004 ">
            They include grammar compilation (Tomita, 1986
            <papid>C94-1068</papid>
            ; Nederhof, 2000
            <papid>J00-1003</papid>
            ), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or head corner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990
            <papid>W98-1115</papid>
            ;Caraballo and Charniak, 1998
            <papid>J98-2004</papid>
            ; Collins, 1999
            <papid>J03-4003</papid>
            ; Rat naparkhi, 1999; Charniak, 2000
            <papid>A00-2018</papid>
            ; Roark, 2001
            <papid>J01-2004</papid>
            ; Klein and Manning, 2003).
         </citsent>
         <aftsection>
            <nextsent>The beam search and best first search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse.The CYK algorithm, which is essentially a bottom up parser, is a natural choice for non-probabilisticHPSG parsers.</nextsent>
            <nextsent>Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing.For PCFG, extending the CYK algorithm to out put the Viterbi parse is straightforward (Ney, 1991;Jurafsky and Martin, 2000).</nextsent>
            <nextsent>The parser can efficiently calculate the Viterbi parse by taking the maximum of the probabilities of the same nonterminal symbol in each cell.</nextsent>
            <nextsent>
               With the probabilistic model defined in Section 2, we can also define the Viterbi search for unification-based grammars (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Beam thresholding for HPSG.</section>
      <citcontext>
         <prevsection>
            <prevsent>parsing 4.1 Simple beam thresholding.</prevsent>
            <prevsent>Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated.</prevsent>
         </prevsection>
         <citsent citstr=" J03-4003 ">
            They include grammar compilation (Tomita, 1986
            <papid>C94-1068</papid>
            ; Nederhof, 2000
            <papid>J00-1003</papid>
            ), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or head corner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990
            <papid>W98-1115</papid>
            ;Caraballo and Charniak, 1998
            <papid>J98-2004</papid>
            ; Collins, 1999
            <papid>J03-4003</papid>
            ; Rat naparkhi, 1999; Charniak, 2000
            <papid>A00-2018</papid>
            ; Roark, 2001
            <papid>J01-2004</papid>
            ; Klein and Manning, 2003).
         </citsent>
         <aftsection>
            <nextsent>The beam search and best first search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse.The CYK algorithm, which is essentially a bottom up parser, is a natural choice for non-probabilisticHPSG parsers.</nextsent>
            <nextsent>Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing.For PCFG, extending the CYK algorithm to out put the Viterbi parse is straightforward (Ney, 1991;Jurafsky and Martin, 2000).</nextsent>
            <nextsent>The parser can efficiently calculate the Viterbi parse by taking the maximum of the probabilities of the same nonterminal symbol in each cell.</nextsent>
            <nextsent>
               With the probabilistic model defined in Section 2, we can also define the Viterbi search for unification-based grammars (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Beam thresholding for HPSG.</section>
      <citcontext>
         <prevsection>
            <prevsent>parsing 4.1 Simple beam thresholding.</prevsent>
            <prevsent>Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated.</prevsent>
         </prevsection>
         <citsent citstr=" A00-2018 ">
            They include grammar compilation (Tomita, 1986
            <papid>C94-1068</papid>
            ; Nederhof, 2000
            <papid>J00-1003</papid>
            ), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or head corner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990
            <papid>W98-1115</papid>
            ;Caraballo and Charniak, 1998
            <papid>J98-2004</papid>
            ; Collins, 1999
            <papid>J03-4003</papid>
            ; Rat naparkhi, 1999; Charniak, 2000
            <papid>A00-2018</papid>
            ; Roark, 2001
            <papid>J01-2004</papid>
            ; Klein and Manning, 2003).
         </citsent>
         <aftsection>
            <nextsent>The beam search and best first search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse.The CYK algorithm, which is essentially a bottom up parser, is a natural choice for non-probabilisticHPSG parsers.</nextsent>
            <nextsent>Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing.For PCFG, extending the CYK algorithm to out put the Viterbi parse is straightforward (Ney, 1991;Jurafsky and Martin, 2000).</nextsent>
            <nextsent>The parser can efficiently calculate the Viterbi parse by taking the maximum of the probabilities of the same nonterminal symbol in each cell.</nextsent>
            <nextsent>
               With the probabilistic model defined in Section 2, we can also define the Viterbi search for unification-based grammars (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Beam thresholding for HPSG.</section>
      <citcontext>
         <prevsection>
            <prevsent>parsing 4.1 Simple beam thresholding.</prevsent>
            <prevsent>Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated.</prevsent>
         </prevsection>
         <citsent citstr=" J01-2004 ">
            They include grammar compilation (Tomita, 1986
            <papid>C94-1068</papid>
            ; Nederhof, 2000
            <papid>J00-1003</papid>
            ), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or head corner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990
            <papid>W98-1115</papid>
            ;Caraballo and Charniak, 1998
            <papid>J98-2004</papid>
            ; Collins, 1999
            <papid>J03-4003</papid>
            ; Rat naparkhi, 1999; Charniak, 2000
            <papid>A00-2018</papid>
            ; Roark, 2001
            <papid>J01-2004</papid>
            ; Klein and Manning, 2003).
         </citsent>
         <aftsection>
            <nextsent>The beam search and best first search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse.The CYK algorithm, which is essentially a bottom up parser, is a natural choice for non-probabilisticHPSG parsers.</nextsent>
            <nextsent>Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing.For PCFG, extending the CYK algorithm to out put the Viterbi parse is straightforward (Ney, 1991;Jurafsky and Martin, 2000).</nextsent>
            <nextsent>The parser can efficiently calculate the Viterbi parse by taking the maximum of the probabilities of the same nonterminal symbol in each cell.</nextsent>
            <nextsent>
               With the probabilistic model defined in Section 2, we can also define the Viterbi search for unification-based grammars (Geman and Johnson, 2002
               <papid>P02-1036</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Beam thresholding for HPSG.</section>
      <citcontext>
         <prevsection>
            <prevsent>Feature functions are defined over lexical entries and results of rule applications, which correspond to conjunctive nodes in a feature forest.</prevsent>
            <prevsent>The FOM of a newly created partial parse, F , is computed by summing the values of ? of the daughters and an additional FOM of F . The Viterbi algorithm enables various pruning techniques to be used for efficient parsing.</prevsent>
         </prevsection>
         <citsent citstr=" W97-0302 ">
            Beam thresholding (Goodman, 1997
            <papid>W97-0302</papid>
            ) is a simple and effective technique for pruning edges during parsing.
         </citsent>
         <aftsection>
            <nextsent>Ineach cell of the chart, the method keeps only a portion of the edges which have higher FOMs compared to the other edges in the same cell.</nextsent>
            <nextsent>106 procedure BeamThresholding(w1, . . .</nextsent>
            <nextsent>, wn?, L?, R?, ?, ?, ?) for i = 1 to n foreach Fu ? {F |wi, F ? ?</nextsent>
            <nextsent>L} ? = ? i ifi(Fu) pi[i? 1, i]?</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Evaluation.</section>
      <citcontext>
         <prevsection>
            <prevsent>The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank(Marcus et al, 1994) (39,832 sentences).</prevsent>
            <prevsent>The grammar consisted of 2,284 lexical entry templates for 10,536 words1.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1011 ">
            The probabilistic disambiguation model of the grammar was trained using the same portion of the treebank (Miyao and Tsujii, 2005
            <papid>P05-1011</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>1Lexical entry templates for POS are also developed.</nextsent>
            <nextsent>They are assigned to unknown words.The model included 529,856 features.</nextsent>
            <nextsent>The parameters for beam searching were determined manually by trial and error using Section 22; 0 = 12,??</nextsent>
            <nextsent>= 6, last = 30, 0 = 6.0,??</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W05-1511.xml">efficacy of beam thresholding unification filtering and hybrid parsing in probabilistic hpsg parsing</title>
      <section>Evaluation.</section>
      <citcontext>
         <prevsection>
            <prevsent>A predicate-argument relation is defined as a tuple??,wh, a, wa?, where ? is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1,..., ARG4), and wa is the head word of the argument.</prevsent>
            <prevsent>Precision/recall is the ratio of tuples correctly identified by the parser.</prevsent>
         </prevsection>
         <citsent citstr=" P03-1046 ">
            This evaluation scheme wasthe same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003
            <papid>P03-1046</papid>
            ; Clark and Curran, 2004; Miyao and Tsujii, 2005
            <papid>P05-1011</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU.</nextsent>
            <nextsent>Section 22 of the Treebank was usedas the development set, and performance was evaluated using sentences of less than 40 words in Section23 (2,164 sentences, 20.3 words/sentence).</nextsent>
            <nextsent>The performance of each parsing technique was analyzed using the sentences in Section 24 of less than 15 words(305 sentences) and less than 40 words (1145 sen tences).</nextsent>
            <nextsent>Table 2 shows the parsing performance using all 110</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>This gives our model stronger expressive power than other reported models.</prevsent>
            <prevsent>Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            Phrase-based modeling method (Koehn et al, 2003
            <papid>N03-1017</papid>
            ; Och and Ney, 2004
            <papid>J04-4002</papid>
            a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.
         </citsent>
         <aftsection>
            <nextsent>
               However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006
               <papid>N06-1002</papid>
               ).
            </nextsent>
            <nextsent>
               Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997
               <papid>J97-3002</papid>
               ; Chiang, 2005; Eisner, 2003
               <papid>P03-2041</papid>
               ; Ding and Palmer, 2005
               <papid>P05-1067</papid>
               ; Quirk et al 2005
               <papid>P05-1034</papid>
               ; Cowan et al, 2006; Zhang et al, 2007; Bod, 2007; Yamada and Knight, 2001
               <papid>P01-1067</papid>
               ; Liu et al, 2006
               <papid>P06-1077</papid>
               ; Liu et al, 2007
               <papid>P07-1089</papid>
               ; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>This gives our model stronger expressive power than other reported models.</prevsent>
            <prevsent>Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems.</prevsent>
         </prevsection>
         <citsent citstr=" J04-4002 ">
            Phrase-based modeling method (Koehn et al, 2003
            <papid>N03-1017</papid>
            ; Och and Ney, 2004
            <papid>J04-4002</papid>
            a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.
         </citsent>
         <aftsection>
            <nextsent>
               However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006
               <papid>N06-1002</papid>
               ).
            </nextsent>
            <nextsent>
               Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997
               <papid>J97-3002</papid>
               ; Chiang, 2005; Eisner, 2003
               <papid>P03-2041</papid>
               ; Ding and Palmer, 2005
               <papid>P05-1067</papid>
               ; Quirk et al 2005
               <papid>P05-1034</papid>
               ; Cowan et al, 2006; Zhang et al, 2007; Bod, 2007; Yamada and Knight, 2001
               <papid>P01-1067</papid>
               ; Liu et al, 2006
               <papid>P06-1077</papid>
               ; Liu et al, 2007
               <papid>P07-1089</papid>
               ; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems.</prevsent>
            <prevsent>
               Phrase-based modeling method (Koehn et al, 2003
               <papid>N03-1017</papid>
               ; Och and Ney, 2004
               <papid>J04-4002</papid>
               a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.
            </prevsent>
         </prevsection>
         <citsent citstr=" N06-1002 ">
            However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006
            <papid>N06-1002</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997
               <papid>J97-3002</papid>
               ; Chiang, 2005; Eisner, 2003
               <papid>P03-2041</papid>
               ; Ding and Palmer, 2005
               <papid>P05-1067</papid>
               ; Quirk et al 2005
               <papid>P05-1034</papid>
               ; Cowan et al, 2006; Zhang et al, 2007; Bod, 2007; Yamada and Knight, 2001
               <papid>P01-1067</papid>
               ; Liu et al, 2006
               <papid>P06-1077</papid>
               ; Liu et al, 2007
               <papid>P07-1089</papid>
               ; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003).
            </nextsent>
            <nextsent>Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Phrase-based modeling method (Koehn et al, 2003
               <papid>N03-1017</papid>
               ; Och and Ney, 2004
               <papid>J04-4002</papid>
               a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.
            </prevsent>
            <prevsent>
               However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006
               <papid>N06-1002</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" J97-3002 ">
            Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997
            <papid>J97-3002</papid>
            ; Chiang, 2005; Eisner, 2003
            <papid>P03-2041</papid>
            ; Ding and Palmer, 2005
            <papid>P05-1067</papid>
            ; Quirk et al 2005
            <papid>P05-1034</papid>
            ; Cowan et al, 2006; Zhang et al, 2007; Bod, 2007; Yamada and Knight, 2001
            <papid>P01-1067</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Liu et al, 2007
            <papid>P07-1089</papid>
            ; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003).
         </citsent>
         <aftsection>
            <nextsent>Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying.</nextsent>
            <nextsent>In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment.</nextsent>
            <nextsent>It is designed to combine the strengths of phrase-based and syntax-based methods.</nextsent>
            <nextsent>The proposed model adopts tree sequence 1 as the basic translation unit and utilizes tree sequence alignments to model the translation process.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Phrase-based modeling method (Koehn et al, 2003
               <papid>N03-1017</papid>
               ; Och and Ney, 2004
               <papid>J04-4002</papid>
               a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.
            </prevsent>
            <prevsent>
               However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006
               <papid>N06-1002</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P03-2041 ">
            Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997
            <papid>J97-3002</papid>
            ; Chiang, 2005; Eisner, 2003
            <papid>P03-2041</papid>
            ; Ding and Palmer, 2005
            <papid>P05-1067</papid>
            ; Quirk et al 2005
            <papid>P05-1034</papid>
            ; Cowan et al, 2006; Zhang et al, 2007; Bod, 2007; Yamada and Knight, 2001
            <papid>P01-1067</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Liu et al, 2007
            <papid>P07-1089</papid>
            ; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003).
         </citsent>
         <aftsection>
            <nextsent>Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying.</nextsent>
            <nextsent>In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment.</nextsent>
            <nextsent>It is designed to combine the strengths of phrase-based and syntax-based methods.</nextsent>
            <nextsent>The proposed model adopts tree sequence 1 as the basic translation unit and utilizes tree sequence alignments to model the translation process.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Phrase-based modeling method (Koehn et al, 2003
               <papid>N03-1017</papid>
               ; Och and Ney, 2004
               <papid>J04-4002</papid>
               a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.
            </prevsent>
            <prevsent>
               However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006
               <papid>N06-1002</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P05-1067 ">
            Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997
            <papid>J97-3002</papid>
            ; Chiang, 2005; Eisner, 2003
            <papid>P03-2041</papid>
            ; Ding and Palmer, 2005
            <papid>P05-1067</papid>
            ; Quirk et al 2005
            <papid>P05-1034</papid>
            ; Cowan et al, 2006; Zhang et al, 2007; Bod, 2007; Yamada and Knight, 2001
            <papid>P01-1067</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Liu et al, 2007
            <papid>P07-1089</papid>
            ; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003).
         </citsent>
         <aftsection>
            <nextsent>Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying.</nextsent>
            <nextsent>In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment.</nextsent>
            <nextsent>It is designed to combine the strengths of phrase-based and syntax-based methods.</nextsent>
            <nextsent>The proposed model adopts tree sequence 1 as the basic translation unit and utilizes tree sequence alignments to model the translation process.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Phrase-based modeling method (Koehn et al, 2003
               <papid>N03-1017</papid>
               ; Och and Ney, 2004
               <papid>J04-4002</papid>
               a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.
            </prevsent>
            <prevsent>
               However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006
               <papid>N06-1002</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P05-1034 ">
            Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997
            <papid>J97-3002</papid>
            ; Chiang, 2005; Eisner, 2003
            <papid>P03-2041</papid>
            ; Ding and Palmer, 2005
            <papid>P05-1067</papid>
            ; Quirk et al 2005
            <papid>P05-1034</papid>
            ; Cowan et al, 2006; Zhang et al, 2007; Bod, 2007; Yamada and Knight, 2001
            <papid>P01-1067</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Liu et al, 2007
            <papid>P07-1089</papid>
            ; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003).
         </citsent>
         <aftsection>
            <nextsent>Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying.</nextsent>
            <nextsent>In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment.</nextsent>
            <nextsent>It is designed to combine the strengths of phrase-based and syntax-based methods.</nextsent>
            <nextsent>The proposed model adopts tree sequence 1 as the basic translation unit and utilizes tree sequence alignments to model the translation process.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Phrase-based modeling method (Koehn et al, 2003
               <papid>N03-1017</papid>
               ; Och and Ney, 2004
               <papid>J04-4002</papid>
               a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.
            </prevsent>
            <prevsent>
               However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006
               <papid>N06-1002</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P01-1067 ">
            Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997
            <papid>J97-3002</papid>
            ; Chiang, 2005; Eisner, 2003
            <papid>P03-2041</papid>
            ; Ding and Palmer, 2005
            <papid>P05-1067</papid>
            ; Quirk et al 2005
            <papid>P05-1034</papid>
            ; Cowan et al, 2006; Zhang et al, 2007; Bod, 2007; Yamada and Knight, 2001
            <papid>P01-1067</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Liu et al, 2007
            <papid>P07-1089</papid>
            ; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003).
         </citsent>
         <aftsection>
            <nextsent>Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying.</nextsent>
            <nextsent>In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment.</nextsent>
            <nextsent>It is designed to combine the strengths of phrase-based and syntax-based methods.</nextsent>
            <nextsent>The proposed model adopts tree sequence 1 as the basic translation unit and utilizes tree sequence alignments to model the translation process.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Phrase-based modeling method (Koehn et al, 2003
               <papid>N03-1017</papid>
               ; Och and Ney, 2004
               <papid>J04-4002</papid>
               a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.
            </prevsent>
            <prevsent>
               However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006
               <papid>N06-1002</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P06-1077 ">
            Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997
            <papid>J97-3002</papid>
            ; Chiang, 2005; Eisner, 2003
            <papid>P03-2041</papid>
            ; Ding and Palmer, 2005
            <papid>P05-1067</papid>
            ; Quirk et al 2005
            <papid>P05-1034</papid>
            ; Cowan et al, 2006; Zhang et al, 2007; Bod, 2007; Yamada and Knight, 2001
            <papid>P01-1067</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Liu et al, 2007
            <papid>P07-1089</papid>
            ; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003).
         </citsent>
         <aftsection>
            <nextsent>Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying.</nextsent>
            <nextsent>In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment.</nextsent>
            <nextsent>It is designed to combine the strengths of phrase-based and syntax-based methods.</nextsent>
            <nextsent>The proposed model adopts tree sequence 1 as the basic translation unit and utilizes tree sequence alignments to model the translation process.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Phrase-based modeling method (Koehn et al, 2003
               <papid>N03-1017</papid>
               ; Och and Ney, 2004
               <papid>J04-4002</papid>
               a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.
            </prevsent>
            <prevsent>
               However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006
               <papid>N06-1002</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P07-1089 ">
            Recently, many syntax-based models have been proposed to address the above deficiencies (Wu, 1997
            <papid>J97-3002</papid>
            ; Chiang, 2005; Eisner, 2003
            <papid>P03-2041</papid>
            ; Ding and Palmer, 2005
            <papid>P05-1067</papid>
            ; Quirk et al 2005
            <papid>P05-1034</papid>
            ; Cowan et al, 2006; Zhang et al, 2007; Bod, 2007; Yamada and Knight, 2001
            <papid>P01-1067</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Liu et al, 2007
            <papid>P07-1089</papid>
            ; Gildea, 2003; Poutsma, 2000; Hearne and Way, 2003).
         </citsent>
         <aftsection>
            <nextsent>Although good progress has been reported, the fundamental issues in applying linguistic syntax to SMT, such as non-isomorphic tree alignment, structure reordering and non-syntactic phrase modeling, are still worth well studying.</nextsent>
            <nextsent>In this paper, we propose a tree-to-tree translation model that is based on tree sequence alignment.</nextsent>
            <nextsent>It is designed to combine the strengths of phrase-based and syntax-based methods.</nextsent>
            <nextsent>The proposed model adopts tree sequence 1 as the basic translation unit and utilizes tree sequence alignments to model the translation process.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al, 2003
               <papid>N03-1017</papid>
               ).
            </prevsent>
            <prevsent>The formally syntax-based model for SMT was first advocated by Wu (1997).</prevsent>
         </prevsection>
         <citsent citstr=" J97-3002 ">
            Xiong et al (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997
            <papid>J97-3002</papid>
            ) while Setiawan et al (2007)
            <papid>P07-1090</papid>
            propose a function word-based reordering model for BTG.
         </citsent>
         <aftsection>
            <nextsent>Chiang (2005)s hierarchal phrase-based model achieves significant performance improvement.</nextsent>
            <nextsent>However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005).</nextsent>
            <nextsent>In the last two years, many research efforts were devoted to integrating the strengths of phrase based and syntax-based methods.</nextsent>
            <nextsent>In the following, we review four representatives of them.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               However, most of them fail to utilize non-syntactic phrases well that are proven useful in the phrase-based methods (Koehn et al, 2003
               <papid>N03-1017</papid>
               ).
            </prevsent>
            <prevsent>The formally syntax-based model for SMT was first advocated by Wu (1997).</prevsent>
         </prevsection>
         <citsent citstr=" P07-1090 ">
            Xiong et al (2006) propose a MaxEnt-based reordering model for BTG (Wu, 1997
            <papid>J97-3002</papid>
            ) while Setiawan et al (2007)
            <papid>P07-1090</papid>
            propose a function word-based reordering model for BTG.
         </citsent>
         <aftsection>
            <nextsent>Chiang (2005)s hierarchal phrase-based model achieves significant performance improvement.</nextsent>
            <nextsent>However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005).</nextsent>
            <nextsent>In the last two years, many research efforts were devoted to integrating the strengths of phrase based and syntax-based methods.</nextsent>
            <nextsent>In the following, we review four representatives of them.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>1) Hassan et al (2007) integrate supertags (a kind of lexicalized syntactic description) into the target side of translation model and language model under the phrase-based translation framework, resulting in good performance improvement.</prevsent>
            <prevsent>How ever, neither source side syntactic knowledge nor reordering model is further explored.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1121 ">
            2) Galley et al (2006)
            <papid>P06-1121</papid>
            handle non-syntactic phrasal translations by traversing the tree upwards until a node that subsumes the phrase is reached.
         </citsent>
         <aftsection>
            <nextsent>
               This solution requires larger applicability contexts (Marcu et al, 2006
               <papid>W06-1606</papid>
               ).
            </nextsent>
            <nextsent>However, phrases are utilized independently in the phrase-based method without depending on any contexts.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>How ever, neither source side syntactic knowledge nor reordering model is further explored.</prevsent>
            <prevsent>
               2) Galley et al (2006)
               <papid>P06-1121</papid>
               handle non-syntactic phrasal translations by traversing the tree upwards until a node that subsumes the phrase is reached.
            </prevsent>
         </prevsection>
         <citsent citstr=" W06-1606 ">
            This solution requires larger applicability contexts (Marcu et al, 2006
            <papid>W06-1606</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, phrases are utilized independently in the phrase-based method without depending on any contexts.</nextsent>
            <nextsent>
               3) Addressing the issues in Galley et al (2006)
               <papid>P06-1121</papid>
               , Marcu et al (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multi headed syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees.
            </nextsent>
            <nextsent>The name of the pseudo non-terminal is designed to reflect the full realization of the corresponding rule.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               This solution requires larger applicability contexts (Marcu et al, 2006
               <papid>W06-1606</papid>
               ).
            </prevsent>
            <prevsent>However, phrases are utilized independently in the phrase-based method without depending on any contexts.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1121 ">
            3) Addressing the issues in Galley et al (2006)
            <papid>P06-1121</papid>
            , Marcu et al (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multi headed syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees.
         </citsent>
         <aftsection>
            <nextsent>The name of the pseudo non-terminal is designed to reflect the full realization of the corresponding rule.</nextsent>
            <nextsent>
               The problem in this method is that it neglects alignment consistency in creating sibling rules and the naming mechanism faces challenges in describing more complicated phenomena (Liu et al, 2007
               <papid>P07-1089</papid>
               ).
            </nextsent>
            <nextsent>4) Liu et al (2006) treat all bilingual phrases as lexicalized tree-to-string rules, including those non-syntactic phrases in training corpus.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               3) Addressing the issues in Galley et al (2006)
               <papid>P06-1121</papid>
               , Marcu et al (2006) create an xRS rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multi headed syntactic structure; and one sibling xRS rule that explains how the pseudo symbol can be combined with other genuine non-terminals for acquiring the genuine parse trees.
            </prevsent>
            <prevsent>The name of the pseudo non-terminal is designed to reflect the full realization of the corresponding rule.</prevsent>
         </prevsection>
         <citsent citstr=" P07-1089 ">
            The problem in this method is that it neglects alignment consistency in creating sibling rules and the naming mechanism faces challenges in describing more complicated phenomena (Liu et al, 2007
            <papid>P07-1089</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>4) Liu et al (2006) treat all bilingual phrases as lexicalized tree-to-string rules, including those non-syntactic phrases in training corpus.</nextsent>
            <nextsent>Although the solution shows effective empirically, it only utilizes the source side syntactic phrases of the in put parse tree during decoding.</nextsent>
            <nextsent>Furthermore, the translation probabilities of the bilingual phrases and other tree-to-string rules are not compatible since they are estimated independently, thus having different parameter spaces.</nextsent>
            <nextsent>To address the above problems, Liu et al (2007) propose to use forest-to-string rules to enhance the expressive power of their tree-to-string model.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>So, to balance the usage of different kinds of rules, they use a very simple feature counting the number of auxiliary rules used in a derivation for penalizing the use of forest-to-string and auxiliary rules.</prevsent>
            <prevsent>In this paper, an alternative solution is presented to combine the strengths of phrase-based and syn 560 1( ) IT e 1( ) JT f AFigure 1: A word-aligned parse tree pairs of a Chinese sentence and its English translation Figure 2: Two Examples of tree sequences Figure 3: Two examples of translation rules tax-based methods.</prevsent>
         </prevsection>
         <citsent citstr=" W06-1606 ">
            Unlike previous work, our solution neither requires larger applicability contexts (Galley et al, 2006), nor depends on pseudo nodes (Marcu et al, 2006
            <papid>W06-1606</papid>
            ) or auxiliary rules (Liu et al, 2007
            <papid>P07-1089</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We go beyond the single sub-tree mapping model to propose a tree sequence alignment-based translation model.</nextsent>
            <nextsent>To the best of our knowledge, this is the first attempt to empirically explore the tree sequence alignment based model in SMT.</nextsent>
            <nextsent>3.1 Tree Sequence Translation Rule.</nextsent>
            <nextsent>The leaf nodes of a sub-tree in a tree sequence can be either non-terminal symbols (grammar tags) or terminal symbols (lexical words).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>So, to balance the usage of different kinds of rules, they use a very simple feature counting the number of auxiliary rules used in a derivation for penalizing the use of forest-to-string and auxiliary rules.</prevsent>
            <prevsent>In this paper, an alternative solution is presented to combine the strengths of phrase-based and syn 560 1( ) IT e 1( ) JT f AFigure 1: A word-aligned parse tree pairs of a Chinese sentence and its English translation Figure 2: Two Examples of tree sequences Figure 3: Two examples of translation rules tax-based methods.</prevsent>
         </prevsection>
         <citsent citstr=" P07-1089 ">
            Unlike previous work, our solution neither requires larger applicability contexts (Galley et al, 2006), nor depends on pseudo nodes (Marcu et al, 2006
            <papid>W06-1606</papid>
            ) or auxiliary rules (Liu et al, 2007
            <papid>P07-1089</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We go beyond the single sub-tree mapping model to propose a tree sequence alignment-based translation model.</nextsent>
            <nextsent>To the best of our knowledge, this is the first attempt to empirically explore the tree sequence alignment based model in SMT.</nextsent>
            <nextsent>3.1 Tree Sequence Translation Rule.</nextsent>
            <nextsent>The leaf nodes of a sub-tree in a tree sequence can be either non-terminal symbols (grammar tags) or terminal symbols (lexical words).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Tree Sequence Alignment Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>3 are used to map the two source tree sequences to two target tree sequences, which are then combined to generate a target parse tree.</prevsent>
            <prevsent>Finally, a target translation is yielded from the target tree.</prevsent>
         </prevsection>
         <citsent citstr=" P02-1038 ">
            Our model is implemented under log-linear framework (Och and Ney, 2002
            <papid>P02-1038</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical rule translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words.</nextsent>
            <nextsent>In addition, we define two new features: 1) the number of lexical words in a rule to control the models preference for lexicalized rules over un-lexicalized rules and 2) the average tree depth in a rule to balance the usage of hierarchical rules and flat rules.</nextsent>
            <nextsent>Note that we do not distinguish between larger (tal ler) and shorter source side tree sequences, i.e. we let these rules compete directly with each other.</nextsent>
            <nextsent>Rules are extracted from word-aligned, bi-parsed sentence pairs 1 1( ), ( ), J IT f T e A&lt; &gt; , which are classified into two categories: z initial rule, if all leaf nodes of the rule are terminals (i.e. lexical word), and z abstract rule, otherwise, i.e. at least one leaf node is a non-terminal (POS or phrase tag).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>We used the Stanford parser (Klein and Manning, 2003) to parse bilingual sentences on the training set and Chinese sentences on the development and test sets.</prevsent>
            <prevsent>The evaluation metric is case-sensitive BLEU-4 (Papi neni et al, 2002).</prevsent>
         </prevsection>
         <citsent citstr=" J04-4002 ">
            We used GIZA++ (Och and Ney, 2004
            <papid>J04-4002</papid>
            ) and the heuristics grow-diag-final?
         </citsent>
         <aftsection>
            <nextsent>to generate m-to-n word alignments.</nextsent>
            <nextsent>
               For the MER training (Och, 2003
               <papid>P03-1021</papid>
               ), we modified Koehns MER trainer (Koehn, 2004) for our tree sequence-based system.
            </nextsent>
            <nextsent>For significance test, we used Zhang et als implementation (Zhang et al 2004).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P08-1064.xml">a tree sequence alignment based treetotree translation model</title>
      <section>Conclusions and Future Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>In addition, word alignment is a hard constraint in our rule extraction.</prevsent>
            <prevsent>We will study direct structure alignments to reduce the impact of word alignment errors.</prevsent>
         </prevsection>
         <citsent citstr=" P07-1089 ">
            We are also interested in comparing our method with the forest to-string model (Liu et al, 2007
            <papid>P07-1089</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Finally, we would also like to study unsupervised learning based bilingual parsing for SMT.</nextsent>
            <nextsent>566</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Constituency parser performance is primarily interpreted through a single metric, F-scoreon WSJ section 23, that conveys no linguistic information regarding the remaining errors.We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together.</prevsent>
            <prevsent>Weuse this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by re rankers, and what types are introduced when parsing out-of-domain text.</prevsent>
         </prevsection>
         <citsent citstr=" N07-1051 ">
            Parsing has been a major area of research within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007
            <papid>N07-1051</papid>
            ), and 92% when using self-training and reranking (McClosky et al 2006
            <papid>N06-1020</papid>
            ; Charniak and Johnson, 2005
            <papid>P05-1022</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors.</nextsent>
            <nextsent>
               Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al 1991
               <papid>H91-1060</papid>
               ) have either focused on specific parsers, e.g. Collins (2003)
               <papid>J03-4003</papid>
               , or have involved conversion to dependencies (Carroll et al 1998; King et al 2003).
            </nextsent>
            <nextsent>In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node errors.We propose a new method of error classification using tree transformations.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Constituency parser performance is primarily interpreted through a single metric, F-scoreon WSJ section 23, that conveys no linguistic information regarding the remaining errors.We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together.</prevsent>
            <prevsent>Weuse this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by re rankers, and what types are introduced when parsing out-of-domain text.</prevsent>
         </prevsection>
         <citsent citstr=" N06-1020 ">
            Parsing has been a major area of research within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007
            <papid>N07-1051</papid>
            ), and 92% when using self-training and reranking (McClosky et al 2006
            <papid>N06-1020</papid>
            ; Charniak and Johnson, 2005
            <papid>P05-1022</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors.</nextsent>
            <nextsent>
               Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al 1991
               <papid>H91-1060</papid>
               ) have either focused on specific parsers, e.g. Collins (2003)
               <papid>J03-4003</papid>
               , or have involved conversion to dependencies (Carroll et al 1998; King et al 2003).
            </nextsent>
            <nextsent>In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node errors.We propose a new method of error classification using tree transformations.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Constituency parser performance is primarily interpreted through a single metric, F-scoreon WSJ section 23, that conveys no linguistic information regarding the remaining errors.We classify errors within a set of linguistically meaningful types using tree transformations that repair groups of errors together.</prevsent>
            <prevsent>Weuse this analysis to answer a range of questions about parser behaviour, including what linguistic constructions are difficult for stateof-the-art parsers, what types of errors are being resolved by re rankers, and what types are introduced when parsing out-of-domain text.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1022 ">
            Parsing has been a major area of research within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007
            <papid>N07-1051</papid>
            ), and 92% when using self-training and reranking (McClosky et al 2006
            <papid>N06-1020</papid>
            ; Charniak and Johnson, 2005
            <papid>P05-1022</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors.</nextsent>
            <nextsent>
               Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al 1991
               <papid>H91-1060</papid>
               ) have either focused on specific parsers, e.g. Collins (2003)
               <papid>J03-4003</papid>
               , or have involved conversion to dependencies (Carroll et al 1998; King et al 2003).
            </nextsent>
            <nextsent>In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node errors.We propose a new method of error classification using tree transformations.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Parsing has been a major area of research within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007
               <papid>N07-1051</papid>
               ), and 92% when using self-training and reranking (McClosky et al 2006
               <papid>N06-1020</papid>
               ; Charniak and Johnson, 2005
               <papid>P05-1022</papid>
               ).
            </prevsent>
            <prevsent>While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors.</prevsent>
         </prevsection>
         <citsent citstr=" H91-1060 ">
            Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al 1991
            <papid>H91-1060</papid>
            ) have either focused on specific parsers, e.g. Collins (2003)
            <papid>J03-4003</papid>
            , or have involved conversion to dependencies (Carroll et al 1998; King et al 2003).
         </citsent>
         <aftsection>
            <nextsent>In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node errors.We propose a new method of error classification using tree transformations.</nextsent>
            <nextsent>Errors in the parse tree are repaired using subtree movement, node creation, and node deletion.</nextsent>
            <nextsent>Each step in the process isthen associated with a linguistically meaningful error type, based on factors such as the node that is moved, its siblings, and parents.Using our method we analyse the output of thirteen constituency parsers on newswire.</nextsent>
            <nextsent>Some of the frequent error types that we identify are widely recognised as challenging, such as prepositional phrase (PP) attachment.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Parsing has been a major area of research within computational linguistics for decades, and constituent parser F-scores on WSJ section 23 have exceeded 90% (Petrov and Klein, 2007
               <papid>N07-1051</papid>
               ), and 92% when using self-training and reranking (McClosky et al 2006
               <papid>N06-1020</papid>
               ; Charniak and Johnson, 2005
               <papid>P05-1022</papid>
               ).
            </prevsent>
            <prevsent>While these results give a useful measure of overall performance, they provide no information about the nature, or relative importance, of the remaining errors.</prevsent>
         </prevsection>
         <citsent citstr=" J03-4003 ">
            Broad investigations of parser errors beyond the PARSEVAL metric (Abney et al 1991
            <papid>H91-1060</papid>
            ) have either focused on specific parsers, e.g. Collins (2003)
            <papid>J03-4003</papid>
            , or have involved conversion to dependencies (Carroll et al 1998; King et al 2003).
         </citsent>
         <aftsection>
            <nextsent>In all of these cases, the analysis has not taken into consideration how a set of errors can have a common cause, e.g. a single mis-attachment can create multiple node errors.We propose a new method of error classification using tree transformations.</nextsent>
            <nextsent>Errors in the parse tree are repaired using subtree movement, node creation, and node deletion.</nextsent>
            <nextsent>Each step in the process isthen associated with a linguistically meaningful error type, based on factors such as the node that is moved, its siblings, and parents.Using our method we analyse the output of thirteen constituency parsers on newswire.</nextsent>
            <nextsent>Some of the frequent error types that we identify are widely recognised as challenging, such as prepositional phrase (PP) attachment.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account.</prevsent>
            <prevsent>For example, a VP node could be missing because of incorrect PP attachment, a coordination error, or a unary production mistake.</prevsent>
         </prevsection>
         <citsent citstr=" W07-2202 ">
            There has been some work that addresses these issues by analysing the output of constituency parsers on linguistically motivated error types, but only by hand on sets of around 100 sentences (Hara et al 2007
            <papid>W07-2202</papid>
            ; Yu et al 2011).
         </citsent>
         <aftsection>
            <nextsent>
               By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences.The second major parser evaluation method involves extraction of grammatical relations (King etal., 2003; Briscoe and Carroll, 2006
               <papid>P06-2006</papid>
               ) or dependencies (Lin, 1998; Briscoe et al 2002).
            </nextsent>
            <nextsent>These metrics have been argued to be more informative and generally applicable (Carroll et al 1998), and have the advantage that the breakdown over dependency types is more informative than over node types.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences.The second major parser evaluation method involves extraction of grammatical relations (King etal., 2003; Briscoe and Carroll, 2006
               <papid>P06-2006</papid>
               ) or dependencies (Lin, 1998; Briscoe et al 2002).
            </prevsent>
            <prevsent>These metrics have been argued to be more informative and generally applicable (Carroll et al 1998), and have the advantage that the breakdown over dependency types is more informative than over node types.</prevsent>
         </prevsection>
         <citsent citstr=" C10-1094 ">
            There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al 2010
            <papid>C10-1094</papid>
            ;Cer et al 2010), as well as work on finding relations between errors (Hara et al 2009
            <papid>D09-1121</papid>
            ), and breaking down errors by a range of factors (McDonald and Nivre, 2007
            <papid>D07-1013</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parsers output (Clark and Hockenmaier, 2002).</nextsent>
            <nextsent>
               Our approach does not have this disadvantage, as we analyse parser output directly.The third major approach involves extrinsic evaluation, where the parsers output is used in a down stream task, such as machine translation (Quirk and Corston-Oliver, 2006
               <papid>W06-1608</papid>
               ), information extraction (Miyao et al 2008
               <papid>P08-1006</papid>
               ), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011).
            </nextsent>
            <nextsent>While some of these approaches givea better sense of the impact of parse errors, theyre quire integration into a larger system, making it less clear where a given error originates.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences.The second major parser evaluation method involves extraction of grammatical relations (King etal., 2003; Briscoe and Carroll, 2006
               <papid>P06-2006</papid>
               ) or dependencies (Lin, 1998; Briscoe et al 2002).
            </prevsent>
            <prevsent>These metrics have been argued to be more informative and generally applicable (Carroll et al 1998), and have the advantage that the breakdown over dependency types is more informative than over node types.</prevsent>
         </prevsection>
         <citsent citstr=" D09-1121 ">
            There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al 2010
            <papid>C10-1094</papid>
            ;Cer et al 2010), as well as work on finding relations between errors (Hara et al 2009
            <papid>D09-1121</papid>
            ), and breaking down errors by a range of factors (McDonald and Nivre, 2007
            <papid>D07-1013</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parsers output (Clark and Hockenmaier, 2002).</nextsent>
            <nextsent>
               Our approach does not have this disadvantage, as we analyse parser output directly.The third major approach involves extrinsic evaluation, where the parsers output is used in a down stream task, such as machine translation (Quirk and Corston-Oliver, 2006
               <papid>W06-1608</papid>
               ), information extraction (Miyao et al 2008
               <papid>P08-1006</papid>
               ), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011).
            </nextsent>
            <nextsent>While some of these approaches givea better sense of the impact of parse errors, theyre quire integration into a larger system, making it less clear where a given error originates.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               By automatically classifying parse errors we are able to consider the output of multiple parsers on thousands of sentences.The second major parser evaluation method involves extraction of grammatical relations (King etal., 2003; Briscoe and Carroll, 2006
               <papid>P06-2006</papid>
               ) or dependencies (Lin, 1998; Briscoe et al 2002).
            </prevsent>
            <prevsent>These metrics have been argued to be more informative and generally applicable (Carroll et al 1998), and have the advantage that the breakdown over dependency types is more informative than over node types.</prevsent>
         </prevsection>
         <citsent citstr=" D07-1013 ">
            There have been comparisons of multiple parsers (Foster and van Genabith, 2008; Nivre et al 2010
            <papid>C10-1094</papid>
            ;Cer et al 2010), as well as work on finding relations between errors (Hara et al 2009
            <papid>D09-1121</papid>
            ), and breaking down errors by a range of factors (McDonald and Nivre, 2007
            <papid>D07-1013</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, one challenge is that results for constituency parsers are strongly influenced by the dependency scheme being used and how easy it is to extract the dependencies from a given parsers output (Clark and Hockenmaier, 2002).</nextsent>
            <nextsent>
               Our approach does not have this disadvantage, as we analyse parser output directly.The third major approach involves extrinsic evaluation, where the parsers output is used in a down stream task, such as machine translation (Quirk and Corston-Oliver, 2006
               <papid>W06-1608</papid>
               ), information extraction (Miyao et al 2008
               <papid>P08-1006</papid>
               ), textual entailment (Yuret et al., 2010), or semantic dependencies (Dridan and Oepen, 2011).
            </nextsent>
            <nextsent>While some of these approaches givea better sense of the impact of parse errors, theyre quire integration into a larger system, making it less clear where a given error originates.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Parsers.</section>
      <citcontext>
         <prevsection>
            <prevsent>Our evaluation is over a wide range of PTB constituency parsers and their variants from the past fifteen years.</prevsent>
            <prevsent>For all parsers we used the publicly available version, with the standard parameter settings.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1055 ">
            Berkeley (Petrov et al 2006
            <papid>P06-1055</papid>
            ; Petrov and Klein, 2007
            <papid>N07-1051</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>An unlexicalised parser with a grammar constructed with automatic state splitting.</nextsent>
            <nextsent>
               Bikel (2004)
               <papid>J04-4004</papid>
               implementation of Collins (1997)
               <papid>P97-1003</papid>
               .
            </nextsent>
            <nextsent>
               BUBS (Dunlop et al 2011; Boden stab et al 2011
               <papid>P11-1045</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>5.1 Reranking.</prevsent>
            <prevsent>The standard dynamic programming approach to parsing limits the range of features that can be employed.</prevsent>
         </prevsection>
         <citsent citstr=" J05-1003 ">
            One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000
            <papid>J05-1003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               While re-ranking has led to gains in performance (Charniak and Johnson, 2005
               <papid>P05-1022</papid>
               ), there has been limited analysis of how effectivelyrerankers are using the set of available options.
            </nextsent>
            <nextsent>
               Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008
               <papid>P08-1067</papid>
               ; Ng et al 2010; Auli and Lopez, 2011
               <papid>P11-1048</papid>
               ; Ng and Curran, 2012).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>The standard dynamic programming approach to parsing limits the range of features that can be employed.</prevsent>
            <prevsent>
               One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000
               <papid>J05-1003</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P05-1022 ">
            While re-ranking has led to gains in performance (Charniak and Johnson, 2005
            <papid>P05-1022</papid>
            ), there has been limited analysis of how effectivelyrerankers are using the set of available options.
         </citsent>
         <aftsection>
            <nextsent>
               Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008
               <papid>P08-1067</papid>
               ; Ng et al 2010; Auli and Lopez, 2011
               <papid>P11-1048</papid>
               ; Ng and Curran, 2012).
            </nextsent>
            <nextsent>In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000
               <papid>J05-1003</papid>
               ).
            </prevsent>
            <prevsent>
               While re-ranking has led to gains in performance (Charniak and Johnson, 2005
               <papid>P05-1022</papid>
               ), there has been limited analysis of how effectivelyrerankers are using the set of available options.
            </prevsent>
         </prevsection>
         <citsent citstr=" P08-1067 ">
            Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008
            <papid>P08-1067</papid>
            ; Ng et al 2010; Auli and Lopez, 2011
            <papid>P11-1048</papid>
            ; Ng and Curran, 2012).
         </citsent>
         <aftsection>
            <nextsent>In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker.</nextsent>
            <nextsent>The oracle results use the parse in each K-best list with the highest F-score.</nextsent>
            <nextsent>While this may not give the true oracle result, as F-scoredoes not factor over sentences, it gives a close approximation.</nextsent>
            <nextsent>The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets.While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first 5-10 parses.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               One way to deal with this issue is to modify the parser to produce the top K parses, rather than just the 1-best, then use a model with more sophisticated features to choose the best parse from this list (Collins, 2000
               <papid>J05-1003</papid>
               ).
            </prevsent>
            <prevsent>
               While re-ranking has led to gains in performance (Charniak and Johnson, 2005
               <papid>P05-1022</papid>
               ), there has been limited analysis of how effectivelyrerankers are using the set of available options.
            </prevsent>
         </prevsection>
         <citsent citstr=" P11-1048 ">
            Recent work has explored this question in more depth, but focusing on how variation in the parameters impacts performance on standard metrics (Huang, 2008
            <papid>P08-1067</papid>
            ; Ng et al 2010; Auli and Lopez, 2011
            <papid>P11-1048</papid>
            ; Ng and Curran, 2012).
         </citsent>
         <aftsection>
            <nextsent>In Table 4 we present a breakdown over error types for the Charniak parser, using the self-trained model and reranker.</nextsent>
            <nextsent>The oracle results use the parse in each K-best list with the highest F-score.</nextsent>
            <nextsent>While this may not give the true oracle result, as F-scoredoes not factor over sentences, it gives a close approximation.</nextsent>
            <nextsent>The table has the same columns as Table 2, but the ranges on the bars now reflect the min and max for these sets.While there is improvement on all errors when using the reranker, there is very little additional gain beyond the first 5-10 parses.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>The other interesting outlier is NP internal structure, which continues to make improvements for longer lists, unlike the other error types.</prevsent>
            <prevsent>5.2 Out-of-Domain.</prevsent>
         </prevsection>
         <citsent citstr=" W01-0521 ">
            Parsing performance drops considerably when shifting outside of the domain a parser was trained on(Gildea, 2001
            <papid>W01-0521</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Clegg and Shepherd (2005)
               <papid>W05-1102</papid>
               evaluated parsers qualitatively on node types and rule pro ductions.
            </nextsent>
            <nextsent>Bender et al(2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena.To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al Corpus Description Sentences Av.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D12-1096.xml">parser showdown at the wall street corral an empirical investigation of error types in parser output</title>
      <section>Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>5.2 Out-of-Domain.</prevsent>
            <prevsent>
               Parsing performance drops considerably when shifting outside of the domain a parser was trained on(Gildea, 2001
               <papid>W01-0521</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" W05-1102 ">
            Clegg and Shepherd (2005)
            <papid>W05-1102</papid>
            evaluated parsers qualitatively on node types and rule pro ductions.
         </citsent>
         <aftsection>
            <nextsent>Bender et al(2011) designed a Wikipedia test set to evaluate parsers on dependencies representing ten specific linguistic phenomena.To provide a deeper understanding of the errors arising when parsing outside of the newswire domain, we analyse performance of the Charniak parser with reranker and self-trained model on the eight parts of the Brown corpus (Marcus et al Corpus Description Sentences Av.</nextsent>
            <nextsent>Length WSJ 23 Newswire 2416 23.5 Brown F Popular 3164 23.4 Brown G Biographies 3279 25.5 Brown K General 3881 17.2 Brown L Mystery 3714 15.7 Brown M Science 881 16.6 Brown N Adventure 4415 16.0 Brown P Romance 3942 17.4 Brown R Humour 967 22.7 G-Web Blogs Blogs 1016 23.6 G-Web Email E-mail 2450 11.9 Table 6: Variation in size and contents of the domains we consider.</nextsent>
            <nextsent>The variation in average sentence lengths skews the results for errors per sentences, and so in Table 5 we consider errors per word.</nextsent>
            <nextsent>1993), and two parts of the Google Web corpus(Petrov and McDonald, 2012).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Paraphrases are semantically equivalent expressions in the same language.</prevsent>
            <prevsent>Because equivalence?</prevsent>
         </prevsection>
         <citsent citstr=" J10-3003 ">
            is the most fundamental semantic relationship, techniques for generating and recognizing paraphrases play an important role in a wide range of natural language processing tasks (Madnani and Dorr, 2010
            <papid>J10-3003</papid>
            ).In the last decade, automatic acquisition of knowledge about paraphrases from corpora has been drawing the attention of many researchers.
         </citsent>
         <aftsection>
            <nextsent>Typically, the acquired knowledge is simply represented as pairs of semantically equivalent sub-sentential expressions as in (1).</nextsent>
            <nextsent>(1) a. look like ? resemble b. control system ? controller The challenge in acquiring paraphrases is to ensure good coverage of the targeted classes of paraphrases along with a low proportion of incorrect pairs.</nextsent>
            <nextsent>How ever, no matter what type of resource has been used, it has proven difficult to acquire paraphrase pairs with both high recall and high precision.</nextsent>
            <nextsent>Among various types of corpora, monolingual corpora can be considered the best source for high coverage paraphrase acquisition, because there isfar more monolingual than bilingual text available.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Most methods that exploit monolingual corpora relyon the Distributional Hypothesis (Harris, 1968): expressions that appear in similar contexts are expected to have similar meaning.</prevsent>
            <prevsent>However,if one uses purely distributional criteria, it is difficult to distinguish real paraphrases from pairs of expressions that are related in other ways, such as antonyms and cousin words.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1074 ">
            In contrast, since the work in (Bannard and Callison-Burch, 2005
            <papid>P05-1074</papid>
            ), bilingual parallel corpora have been acknowledged as a good source of high quality paraphrases: paraphrases are obtained by putting together expressions that receive the same translation in the other language (pivot language).
         </citsent>
         <aftsection>
            <nextsent>Because translation expresses a specific meaning more directly than context in the aforementioned approach, pairs of expressions acquired in this manner tend to be correct paraphrases.</nextsent>
            <nextsent>However, the coverage problem remains: there is much less bilingual parallel than monolingual text available.Our objective in this paper is to obtain paraphrases that have high quality (like those extracted from bilingual parallel corpora via pivoting) but can be generated in large quantity (like those extracted 631from monolingual corpora via contextual similarity).</nextsent>
            <nextsent>To achieve this, we propose a method that exploits general patterns underlying paraphrases and uses both bilingual parallel and monolingual sources of information.</nextsent>
            <nextsent>Given a relatively high-quality set of paraphrases obtained from a bilingual parallel corpus, a set of paraphrase patterns is first induced.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Literature on Paraphrase Acquisition.</section>
      <citcontext>
         <prevsection>
            <prevsent>2.1 Similarity-based Methods.</prevsent>
            <prevsent>Techniques that use monolingual (non-parallel) corpora mostly relyon the Distributional Hypothesis(Harris, 1968).</prevsent>
         </prevsection>
         <citsent citstr=" I05-1011 ">
            Because a large quantity of monolingual data is available for many languages, a large number of paraphrase candidates can be acquired(Lin and Pantel, 2001; Pasca and Dienes, 2005
            <papid>I05-1011</papid>
            ; Bhagat and Ravichandran, 2008
            <papid>P08-1077</papid>
            , etc.).
         </citsent>
         <aftsection>
            <nextsent>The recipes proposed so far are based on three main ingredients, i.e.,features used for representing context of target expression (contextual features), criteria for weighting and filtering features, and aggregation functions.A drawback of relying only on contextual similarity is that it tends to give high scores to semantically related but non-equivalent expressions, such as antonyms and cousin words.</nextsent>
            <nextsent>To enhance the precision of the results, filtering mechanisms need to be introduced (Marton et al2011).</nextsent>
            <nextsent>2.2 Alignment-based Methods.</nextsent>
            <nextsent>Pairs of expressions that get translated to the same expression in a different language can be regarded asparaphrases.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Literature on Paraphrase Acquisition.</section>
      <citcontext>
         <prevsection>
            <prevsent>2.1 Similarity-based Methods.</prevsent>
            <prevsent>Techniques that use monolingual (non-parallel) corpora mostly relyon the Distributional Hypothesis(Harris, 1968).</prevsent>
         </prevsection>
         <citsent citstr=" P08-1077 ">
            Because a large quantity of monolingual data is available for many languages, a large number of paraphrase candidates can be acquired(Lin and Pantel, 2001; Pasca and Dienes, 2005
            <papid>I05-1011</papid>
            ; Bhagat and Ravichandran, 2008
            <papid>P08-1077</papid>
            , etc.).
         </citsent>
         <aftsection>
            <nextsent>The recipes proposed so far are based on three main ingredients, i.e.,features used for representing context of target expression (contextual features), criteria for weighting and filtering features, and aggregation functions.A drawback of relying only on contextual similarity is that it tends to give high scores to semantically related but non-equivalent expressions, such as antonyms and cousin words.</nextsent>
            <nextsent>To enhance the precision of the results, filtering mechanisms need to be introduced (Marton et al2011).</nextsent>
            <nextsent>2.2 Alignment-based Methods.</nextsent>
            <nextsent>Pairs of expressions that get translated to the same expression in a different language can be regarded asparaphrases.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Literature on Paraphrase Acquisition.</section>
      <citcontext>
         <prevsection>
            <prevsent>The likelihood of e2 being a paraphrase of e1 is given by p(e2|e1) = ? fTr(e1,e2) p(e2|f)p(f |e1), (1)where Tr(e1, e2) stands for the set of shared translations of e1 and e2.</prevsent>
            <prevsent>Each factor p(e|f) and p(f |e) is estimated from the number of times e and f are aligned and the number of occurrences of each expression in each language.</prevsent>
         </prevsection>
         <citsent citstr=" N10-1017 ">
            Kok and Brockett (2010)
            <papid>N10-1017</papid>
            showed how one can discover paraphrases that donot share any translation in one language by traversing a graph created from multiple translation tables, each corresponding to a bilingual parallel corpus.This approach, however, suffers from a cover age problem, because both monolingual parallel and bilingual parallel corpora tend to be significantly smaller than monolingual non-parallel corpora.
         </citsent>
         <aftsection>
            <nextsent>The acquired pairs of expressions include some non paraphrases as well.</nextsent>
            <nextsent>Many of these come from erroneous alignments, which are particularly frequent when the given corpus is small.</nextsent>
            <nextsent>Monolingual comparable corpora have also been exploited as sources of paraphrases using alignment based methods.</nextsent>
            <nextsent>
               For instance, multiple news articles covering the same event (Shinyama et al2002; Barzilay and Lee, 2003
               <papid>N03-1003</papid>
               ; Dolan et al2004; Wubben et al2009) have been used.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Literature on Paraphrase Acquisition.</section>
      <citcontext>
         <prevsection>
            <prevsent>Many of these come from erroneous alignments, which are particularly frequent when the given corpus is small.</prevsent>
            <prevsent>Monolingual comparable corpora have also been exploited as sources of paraphrases using alignment based methods.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1003 ">
            For instance, multiple news articles covering the same event (Shinyama et al2002; Barzilay and Lee, 2003
            <papid>N03-1003</papid>
            ; Dolan et al2004; Wubben et al2009) have been used.
         </citsent>
         <aftsection>
            <nextsent>
               Such corpora have also been created manually through crowdsourcing(Chen and Dolan, 2011
               <papid>P11-1020</papid>
               ).
            </nextsent>
            <nextsent>However, the availability of monolingual comparable corpora is very limited for most languages; thus, approaches relying on these corpora have typically produced only very 632 small collections of paraphrases.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Literature on Paraphrase Acquisition.</section>
      <citcontext>
         <prevsection>
            <prevsent>Monolingual comparable corpora have also been exploited as sources of paraphrases using alignment based methods.</prevsent>
            <prevsent>
               For instance, multiple news articles covering the same event (Shinyama et al2002; Barzilay and Lee, 2003
               <papid>N03-1003</papid>
               ; Dolan et al2004; Wubben et al2009) have been used.
            </prevsent>
         </prevsection>
         <citsent citstr=" P11-1020 ">
            Such corpora have also been created manually through crowdsourcing(Chen and Dolan, 2011
            <papid>P11-1020</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, the availability of monolingual comparable corpora is very limited for most languages; thus, approaches relying on these corpora have typically produced only very 632 small collections of paraphrases.</nextsent>
            <nextsent>Hashimoto et al(2011) found a way around this limitation by collecting sentences that constitute explicit definitions of particular words or phrases from monolingual non parallel Web documents, pairing sentences that define the same noun phrase, and then finding corresponding phrases in each sentence pair.</nextsent>
            <nextsent>One limitation of this approach is that it requires a considerable amount of labeled data for both the corpus construction and the paraphrase extraction steps.</nextsent>
            <nextsent>2.3 Summary.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Proposed Method.</section>
      <citcontext>
         <prevsection>
            <prevsent>is a more likely source than lp, i.e., p(lp ?|rp) &gt; p(lp|rp) The two directions of filtering are separately applied and the intersection of their results is retained.</prevsent>
            <prevsent>2cf.</prevsent>
         </prevsection>
         <citsent citstr=" W11-2107 ">
            Denkowski and Lavie (2011)
            <papid>W11-2107</papid>
            ; they only compared each RHS phrase to its corresponding LHS phrase.
         </citsent>
         <aftsection>
            <nextsent>Candidate pairs are finally filtered on the basis of their reliability score.</nextsent>
            <nextsent>Traditionally, a threshold (thp) on the conditional probability given by Eq.</nextsent>
            <nextsent>
               (1) is used (Du et al2010; Max, 2010
               <papid>D10-1064</papid>
               ; Denkowskiand Lavie, 2011, etc.).
            </nextsent>
            <nextsent>Furthermore, we also require that LHS and RHS phrases exceed a threshold (ths ) on their contextual similarity in a monolingual corpus.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Proposed Method.</section>
      <citcontext>
         <prevsection>
            <prevsent>Candidate pairs are finally filtered on the basis of their reliability score.</prevsent>
            <prevsent>Traditionally, a threshold (thp) on the conditional probability given by Eq.</prevsent>
         </prevsection>
         <citsent citstr=" D10-1064 ">
            (1) is used (Du et al2010; Max, 2010
            <papid>D10-1064</papid>
            ; Denkowskiand Lavie, 2011, etc.).
         </citsent>
         <aftsection>
            <nextsent>Furthermore, we also require that LHS and RHS phrases exceed a threshold (ths ) on their contextual similarity in a monolingual corpus.</nextsent>
            <nextsent>This paper neither proposes a specific recipe nor makes a comprehensive comparison of existing recipes for computing contextual similarity, although one particular recipe is used in our experiments (see Section 4.1).</nextsent>
            <nextsent>3.2 Step 2.</nextsent>
            <nextsent>Paraphrase Pattern Induction.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Proposed Method.</section>
      <citcontext>
         <prevsection>
            <prevsent>and apparatus?</prevsent>
            <prevsent>in (3a), could be captured, this would require the use of rich language resources, thereby making the method less portable to resource-poor languages.</prevsent>
         </prevsection>
         <citsent citstr=" P99-1044 ">
            634Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999
            <papid>P99-1044</papid>
            ;Fujita et al2007).
         </citsent>
         <aftsection>
            <nextsent>
               This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001; Szpektor and Dagan, 2008
               <papid>C08-1107</papid>
               ) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008
               <papid>D08-1021</papid>
               ; Zhao et al2009).
            </nextsent>
            <nextsent>3.3 Step 3.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Proposed Method.</section>
      <citcontext>
         <prevsection>
            <prevsent>in (3a), could be captured, this would require the use of rich language resources, thereby making the method less portable to resource-poor languages.</prevsent>
            <prevsent>
               634Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999
               <papid>P99-1044</papid>
               ;Fujita et al2007).
            </prevsent>
         </prevsection>
         <citsent citstr=" C08-1107 ">
            This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001; Szpektor and Dagan, 2008
            <papid>C08-1107</papid>
            ) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008
            <papid>D08-1021</papid>
            ; Zhao et al2009).
         </citsent>
         <aftsection>
            <nextsent>3.3 Step 3.</nextsent>
            <nextsent>Paraphrase Instance Acquisition.</nextsent>
            <nextsent>Given a set of paraphrase patterns, such as those shown in (4), a set of novel instances, i.e., novel paraphrases, PHvst , will now be harvested from monolingual non-parallel corpora.</nextsent>
            <nextsent>In other words, a set of appropriate slot-fillers will be extracted.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Proposed Method.</section>
      <citcontext>
         <prevsection>
            <prevsent>in (3a), could be captured, this would require the use of rich language resources, thereby making the method less portable to resource-poor languages.</prevsent>
            <prevsent>
               634Note that our aim is to automatically capture general paraphrase patterns of the kind that have sometimes been manually described (Jacquemin, 1999
               <papid>P99-1044</papid>
               ;Fujita et al2007).
            </prevsent>
         </prevsection>
         <citsent citstr=" D08-1021 ">
            This is different from approaches that attach variable slots to paraphrases for calculating their similarity (Lin and Pantel, 2001; Szpektor and Dagan, 2008
            <papid>C08-1107</papid>
            ) or for constraining the context in which they are regarded legitimate (Callison-Burch, 2008
            <papid>D08-1021</papid>
            ; Zhao et al2009).
         </citsent>
         <aftsection>
            <nextsent>3.3 Step 3.</nextsent>
            <nextsent>Paraphrase Instance Acquisition.</nextsent>
            <nextsent>Given a set of paraphrase patterns, such as those shown in (4), a set of novel instances, i.e., novel paraphrases, PHvst , will now be harvested from monolingual non-parallel corpora.</nextsent>
            <nextsent>In other words, a set of appropriate slot-fillers will be extracted.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Quantitative Impact.</section>
      <citcontext>
         <prevsection>
            <prevsent>The resulting translation pairs were then filtered with the significance pruning technique of (Johnson et al2007), using ? + ? as threshold.</prevsent>
            <prevsent>As contextual features for computing similarity of each paraphrase pair, all of the 1- to 4-grams of words adjacent to each occurrence of a phrase werecounted.</prevsent>
         </prevsection>
         <citsent citstr=" C08-1107 ">
            This is a compromise between less expensive but noisier approaches, such as bag-of-words, and more accurate but more expensive approaches that incorporate syntactic features (Lin and Pantel, 2001; Shinyama et al2002; Pang et al2003; Szpektor and Dagan, 2008
            <papid>C08-1107</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Contextual similarity is finally measured by taking cosine between two feature vectors.</nextsent>
            <nextsent>4.2 Statistics on Acquired Paraphrases.</nextsent>
            <nextsent>Seed Paraphrases (PSeed ) Figure 4 shows the number of paraphrase pairs PSeed obtained from the bilingual parallel corpora.The general trend is simply that the larger the corpus is, the more paraphrases are acquired.</nextsent>
            <nextsent>Given the initial set of paraphrases, PRaw (???), our filtering techniques (?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Human Evaluation of Quality.</section>
      <citcontext>
         <prevsection>
            <prevsent>In this experiment, we showed five paraphrases per source phrase, assuming that evaluators would get confused if too large a number of paraphrase candidates were presented at the same time.</prevsent>
            <prevsent>5.1 Data for Evaluation.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1021 ">
            As in previous work (Callison-Burch, 2008
            <papid>D08-1021</papid>
            ; Chan et al2011), we evaluated paraphrases acquired from the Europarl corpus on news sentences.
         </citsent>
         <aftsection>
            <nextsent>Paraphrase examples were automatically generated from the English part ofWMT 2008-2011 newstest?</nextsent>
            <nextsent>data (10,050 unique sentences) by applying the union ofPSeed and PHvst of the Europarl setting (19.3M paraphrases for 5.95M phrases).</nextsent>
            <nextsent>On the other hand, paraphrases acquired from patent documents are much more difficult to evaluate due to the following reasons.</nextsent>
            <nextsent>First, they may be too domain-specific to be of any use in general areas such as news sentences.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1058.xml">enlarging paraphrase collections through generalization and instantiation</title>
      <section>Human Evaluation of Quality.</section>
      <citcontext>
         <prevsection>
            <prevsent>score and precision of binary classification.</prevsent>
            <prevsent>5.2 Results.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1021 ">
            Table 1 shows the average of the original 5-point scale scores and the percentage of examples that are judged correct based on a binary judgment (Callison-Burch, 2008
            <papid>D08-1021</papid>
            ): an example is considered to be correct iff the grammaticality score is 4 or above and/or the meaning score is 3 or above.
         </citsent>
         <aftsection>
            <nextsent>Paraphrases based on PSeed achieved a quite high performance in both grammaticality (G?) and meaning (M?) inpart because of the effectiveness of our filtering techniques.</nextsent>
            <nextsent>The performance of paraphrases drawn from PHvst was reasonably high and similar to the scores 0.68 for grammaticality, 0.61 for meaning, and 0.55for both, of the best model reported in (Callison Burch, 2008), although it was inferior to PSeed . Despite the fact that all of our evaluators had a high-level command of English, the agreement wasnot very high.</nextsent>
            <nextsent>This was true even when the collected scores were mapped into binary classes.</nextsent>
            <nextsent>Inthis case, the ? values (Cohen, 1960) for each criterion were 0.45 and 0.45, respectively, which indicate the agreement was fair?.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Features are often implied by a choice of formalism.dence assumptions they imply) from inference algorithms in MT; this separation is widely appreciated in machine learning.</prevsent>
            <prevsent>Here we take first steps toward such a universal?</prevsent>
         </prevsection>
         <citsent citstr=" P02-1038 ">
            decoder, making the following contributions:Arbitrary feature model (2): We define a single, direct log-linear translation model (Papineni et al, 1997; Och and Ney, 2002
            <papid>P02-1038</papid>
            ) that encodes most popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments.
         </citsent>
         <aftsection>
            <nextsent>The trees are optional and can be easily removed, allowing simulation of string-to-tree,?</nextsent>
            <nextsent>tree-to-string,?</nextsent>
            <nextsent>tree to-tree,?</nextsent>
            <nextsent>and phrase-based?</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>models, among many others.</prevsent>
            <prevsent>We follow the widespread use of log-linearmodeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model.</prevsent>
         </prevsection>
         <citsent citstr=" W06-3104 ">
            Decoding as QG parsing (34): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006
            <papid>W06-3104</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>2 Further, we exploit generic approximate inference techniques to incorporate arbitrary nonlocal?</nextsent>
            <nextsent>
               features in the dynamic programming algorithm (Chiang, 2007
               <papid>J07-2003</papid>
               ; Gimpel and Smith, 2009
               <papid>E09-1037</papid>
               ).Parameter estimation (5): We exploit similar approximate inference methods in regularized pseudo likelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.
            </nextsent>
            <nextsent>Because we start with inference(the key subroutine in training), many other learning algorithms are possible.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Decoding as QG parsing (34): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006
               <papid>W06-3104</papid>
               ).
            </prevsent>
            <prevsent>2 Further, we exploit generic approximate inference techniques to incorporate arbitrary nonlocal?</prevsent>
         </prevsection>
         <citsent citstr=" J07-2003 ">
            features in the dynamic programming algorithm (Chiang, 2007
            <papid>J07-2003</papid>
            ; Gimpel and Smith, 2009
            <papid>E09-1037</papid>
            ).Parameter estimation (5): We exploit similar approximate inference methods in regularized pseudo likelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.
         </citsent>
         <aftsection>
            <nextsent>Because we start with inference(the key subroutine in training), many other learning algorithms are possible.</nextsent>
            <nextsent>Experimental platform (6): The flexibility of our model/decoder permits carefully controlledexperiments.</nextsent>
            <nextsent>
               We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006
               <papid>W06-3104</papid>
               ), adaptation and projection in parsing(Smith and Eisner, 2009
               <papid>D09-1086</papid>
               ), and various monolingual recognition and scoring tasks (Wang et al, 2007
               <papid>D07-1003</papid>
               ; Das and Smith, 2009
               <papid>P09-1053</papid>
               ); this paper represents its first application to MT. 219 ?, T source and target language vocabularies, respectively Trans : ? ?
            </nextsent>
            <nextsent>{NULL} ? 2 T function mapping each source word to target words to which it may translate s = s 0 , . . .</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Decoding as QG parsing (34): We present anovel decoder based on lattice parsing with quasi synchronous grammar (QG; Smith and Eisner, 2006
               <papid>W06-3104</papid>
               ).
            </prevsent>
            <prevsent>2 Further, we exploit generic approximate inference techniques to incorporate arbitrary nonlocal?</prevsent>
         </prevsection>
         <citsent citstr=" E09-1037 ">
            features in the dynamic programming algorithm (Chiang, 2007
            <papid>J07-2003</papid>
            ; Gimpel and Smith, 2009
            <papid>E09-1037</papid>
            ).Parameter estimation (5): We exploit similar approximate inference methods in regularized pseudo likelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model.
         </citsent>
         <aftsection>
            <nextsent>Because we start with inference(the key subroutine in training), many other learning algorithms are possible.</nextsent>
            <nextsent>Experimental platform (6): The flexibility of our model/decoder permits carefully controlledexperiments.</nextsent>
            <nextsent>
               We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006
               <papid>W06-3104</papid>
               ), adaptation and projection in parsing(Smith and Eisner, 2009
               <papid>D09-1086</papid>
               ), and various monolingual recognition and scoring tasks (Wang et al, 2007
               <papid>D07-1003</papid>
               ; Das and Smith, 2009
               <papid>P09-1053</papid>
               ); this paper represents its first application to MT. 219 ?, T source and target language vocabularies, respectively Trans : ? ?
            </nextsent>
            <nextsent>{NULL} ? 2 T function mapping each source word to target words to which it may translate s = s 0 , . . .</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Because we start with inference(the key subroutine in training), many other learning algorithms are possible.</prevsent>
            <prevsent>Experimental platform (6): The flexibility of our model/decoder permits carefully controlledexperiments.</prevsent>
         </prevsection>
         <citsent citstr=" W06-3104 ">
            We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006
            <papid>W06-3104</papid>
            ), adaptation and projection in parsing(Smith and Eisner, 2009
            <papid>D09-1086</papid>
            ), and various monolingual recognition and scoring tasks (Wang et al, 2007
            <papid>D07-1003</papid>
            ; Das and Smith, 2009
            <papid>P09-1053</papid>
            ); this paper represents its first application to MT. 219 ?, T source and target language vocabularies, respectively Trans : ? ?
         </citsent>
         <aftsection>
            <nextsent>{NULL} ? 2 T function mapping each source word to target words to which it may translate s = s 0 , . . .</nextsent>
            <nextsent>, s n ? ?</nextsent>
            <nextsent>n source language sentence (s 0 is the NULL word) t = t 1 , . . .</nextsent>
            <nextsent>, t m ? ?</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Because we start with inference(the key subroutine in training), many other learning algorithms are possible.</prevsent>
            <prevsent>Experimental platform (6): The flexibility of our model/decoder permits carefully controlledexperiments.</prevsent>
         </prevsection>
         <citsent citstr=" D09-1086 ">
            We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006
            <papid>W06-3104</papid>
            ), adaptation and projection in parsing(Smith and Eisner, 2009
            <papid>D09-1086</papid>
            ), and various monolingual recognition and scoring tasks (Wang et al, 2007
            <papid>D07-1003</papid>
            ; Das and Smith, 2009
            <papid>P09-1053</papid>
            ); this paper represents its first application to MT. 219 ?, T source and target language vocabularies, respectively Trans : ? ?
         </citsent>
         <aftsection>
            <nextsent>{NULL} ? 2 T function mapping each source word to target words to which it may translate s = s 0 , . . .</nextsent>
            <nextsent>, s n ? ?</nextsent>
            <nextsent>n source language sentence (s 0 is the NULL word) t = t 1 , . . .</nextsent>
            <nextsent>, t m ? ?</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Because we start with inference(the key subroutine in training), many other learning algorithms are possible.</prevsent>
            <prevsent>Experimental platform (6): The flexibility of our model/decoder permits carefully controlledexperiments.</prevsent>
         </prevsection>
         <citsent citstr=" D07-1003 ">
            We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006
            <papid>W06-3104</papid>
            ), adaptation and projection in parsing(Smith and Eisner, 2009
            <papid>D09-1086</papid>
            ), and various monolingual recognition and scoring tasks (Wang et al, 2007
            <papid>D07-1003</papid>
            ; Das and Smith, 2009
            <papid>P09-1053</papid>
            ); this paper represents its first application to MT. 219 ?, T source and target language vocabularies, respectively Trans : ? ?
         </citsent>
         <aftsection>
            <nextsent>{NULL} ? 2 T function mapping each source word to target words to which it may translate s = s 0 , . . .</nextsent>
            <nextsent>, s n ? ?</nextsent>
            <nextsent>n source language sentence (s 0 is the NULL word) t = t 1 , . . .</nextsent>
            <nextsent>, t m ? ?</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Because we start with inference(the key subroutine in training), many other learning algorithms are possible.</prevsent>
            <prevsent>Experimental platform (6): The flexibility of our model/decoder permits carefully controlledexperiments.</prevsent>
         </prevsection>
         <citsent citstr=" P09-1053 ">
            We compare lexical phrase and dependency syntax features, as well as a novel com 2 To date, QG has been used for word alignment (Smith and Eisner, 2006
            <papid>W06-3104</papid>
            ), adaptation and projection in parsing(Smith and Eisner, 2009
            <papid>D09-1086</papid>
            ), and various monolingual recognition and scoring tasks (Wang et al, 2007
            <papid>D07-1003</papid>
            ; Das and Smith, 2009
            <papid>P09-1053</papid>
            ); this paper represents its first application to MT. 219 ?, T source and target language vocabularies, respectively Trans : ? ?
         </citsent>
         <aftsection>
            <nextsent>{NULL} ? 2 T function mapping each source word to target words to which it may translate s = s 0 , . . .</nextsent>
            <nextsent>, s n ? ?</nextsent>
            <nextsent>n source language sentence (s 0 is the NULL word) t = t 1 , . . .</nextsent>
            <nextsent>, t m ? ?</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>(respectively, t ? ) is a substring of s (t).</prevsent>
            <prevsent>A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into disjoint parts of the source and target sentences 4There are two conventional definitions of feature functions.</prevsent>
         </prevsection>
         <citsent citstr=" P02-1038 ">
            One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002
            <papid>P02-1038</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               These estimates are usually heuristic and inconsistent (Koehn et al, 2003
               <papid>N03-1017</papid>
               ).
            </nextsent>
            <nextsent>
               An alternative is to instantiate features for different structural patterns (Liang et al, 2006
               <papid>P06-1096</papid>
               ; Blunsom et al, 2008
               <papid>P08-1024</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>A major difference between the phrase features used in this work and those used elsewhere is that we do not assume that phrases segment into disjoint parts of the source and target sentences 4There are two conventional definitions of feature functions.</prevsent>
            <prevsent>
               One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002
               <papid>P02-1038</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            These estimates are usually heuristic and inconsistent (Koehn et al, 2003
            <papid>N03-1017</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               An alternative is to instantiate features for different structural patterns (Liang et al, 2006
               <papid>P06-1096</papid>
               ; Blunsom et al, 2008
               <papid>P08-1024</papid>
               ).
            </nextsent>
            <nextsent>This offers more expressive power but may require much more training data to avoid overfitting.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002
               <papid>P02-1038</papid>
               ).
            </prevsent>
            <prevsent>
               These estimates are usually heuristic and inconsistent (Koehn et al, 2003
               <papid>N03-1017</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P06-1096 ">
            An alternative is to instantiate features for different structural patterns (Liang et al, 2006
            <papid>P06-1096</papid>
            ; Blunsom et al, 2008
            <papid>P08-1024</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>This offers more expressive power but may require much more training data to avoid overfitting.</nextsent>
            <nextsent>For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice.</nextsent>
            <nextsent>
               220 (Koehn et al, 2003
               <papid>N03-1017</papid>
               ); they can overlap.
            </nextsent>
            <nextsent>
               5Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005
               <papid>P12-2057</papid>
               ; Ittycheriah and Roukos, 2007
               <papid>N07-1008</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002
               <papid>P02-1038</papid>
               ).
            </prevsent>
            <prevsent>
               These estimates are usually heuristic and inconsistent (Koehn et al, 2003
               <papid>N03-1017</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P08-1024 ">
            An alternative is to instantiate features for different structural patterns (Liang et al, 2006
            <papid>P06-1096</papid>
            ; Blunsom et al, 2008
            <papid>P08-1024</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>This offers more expressive power but may require much more training data to avoid overfitting.</nextsent>
            <nextsent>For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice.</nextsent>
            <nextsent>
               220 (Koehn et al, 2003
               <papid>N03-1017</papid>
               ); they can overlap.
            </nextsent>
            <nextsent>
               5Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005
               <papid>P12-2057</papid>
               ; Ittycheriah and Roukos, 2007
               <papid>N07-1008</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>This offers more expressive power but may require much more training data to avoid overfitting.</prevsent>
            <prevsent>For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            220 (Koehn et al, 2003
            <papid>N03-1017</papid>
            ); they can overlap.
         </citsent>
         <aftsection>
            <nextsent>
               5Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005
               <papid>P12-2057</papid>
               ; Ittycheriah and Roukos, 2007
               <papid>N07-1008</papid>
               ).
            </nextsent>
            <nextsent>Lexical translation features factor as in Eq.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice.</prevsent>
            <prevsent>
               220 (Koehn et al, 2003
               <papid>N03-1017</papid>
               ); they can overlap.
            </prevsent>
         </prevsection>
         <citsent citstr=" P12-2057 ">
            5Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005
            <papid>P12-2057</papid>
            ; Ittycheriah and Roukos, 2007
            <papid>N07-1008</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Lexical translation features factor as in Eq.</nextsent>
            <nextsent>3 (Tab.</nextsent>
            <nextsent>2).</nextsent>
            <nextsent>We score all phrase pairs in a sentence pair that pair a target phrase with the smallest source phrase that contains all of the alignments in the target phrase; if ? k:ikj a(k) = ?, no phrase feature fires for t j i . 2.2 N -gram Language Model N -gram language models have become standard in machine translation systems.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice.</prevsent>
            <prevsent>
               220 (Koehn et al, 2003
               <papid>N03-1017</papid>
               ); they can overlap.
            </prevsent>
         </prevsection>
         <citsent citstr=" N07-1008 ">
            5Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005
            <papid>P12-2057</papid>
            ; Ittycheriah and Roukos, 2007
            <papid>N07-1008</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Lexical translation features factor as in Eq.</nextsent>
            <nextsent>3 (Tab.</nextsent>
            <nextsent>2).</nextsent>
            <nextsent>We score all phrase pairs in a sentence pair that pair a target phrase with the smallest source phrase that contains all of the alignments in the target phrase; if ? k:ikj a(k) = ?, no phrase feature fires for t j i . 2.2 N -gram Language Model N -gram language models have become standard in machine translation systems.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>There have been many features proposed that consider source- and target-language syntax during translation.</prevsent>
            <prevsent>Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities ina probabilistic grammar, but other syntactic features are possible.</prevsent>
         </prevsection>
         <citsent citstr=" P10-5002 ">
            For example, Quirk et al(2005) use features involving phrases and source side dependency trees and Mi et al (2008)
            <papid>P10-5002</papid>
            use features from a forest of parses of the source sen tence.
         </citsent>
         <aftsection>
            <nextsent>
               There is also substantial work in the use of target-side syntax (Galley et al, 2006
               <papid>P06-1121</papid>
               ; Marcuet al, 2006; Shen et al, 2008
               <papid>P08-1066</papid>
               ).
            </nextsent>
            <nextsent>
               In addition, researchers have recently added syntactic features tophrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008
               <papid>W08-0302</papid>
               ; Haque et al, 2009; Chiang et al, 2008
               <papid>D08-1024</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities ina probabilistic grammar, but other syntactic features are possible.</prevsent>
            <prevsent>
               For example, Quirk et al(2005) use features involving phrases and source side dependency trees and Mi et al (2008)
               <papid>P10-5002</papid>
               use features from a forest of parses of the source sen tence.
            </prevsent>
         </prevsection>
         <citsent citstr=" P06-1121 ">
            There is also substantial work in the use of target-side syntax (Galley et al, 2006
            <papid>P06-1121</papid>
            ; Marcuet al, 2006; Shen et al, 2008
            <papid>P08-1066</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               In addition, researchers have recently added syntactic features tophrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008
               <papid>W08-0302</papid>
               ; Haque et al, 2009; Chiang et al, 2008
               <papid>D08-1024</papid>
               ).
            </nextsent>
            <nextsent>In this work, we focus on syntactic features of target-side dependency trees, ? t , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities ina probabilistic grammar, but other syntactic features are possible.</prevsent>
            <prevsent>
               For example, Quirk et al(2005) use features involving phrases and source side dependency trees and Mi et al (2008)
               <papid>P10-5002</papid>
               use features from a forest of parses of the source sen tence.
            </prevsent>
         </prevsection>
         <citsent citstr=" P08-1066 ">
            There is also substantial work in the use of target-side syntax (Galley et al, 2006
            <papid>P06-1121</papid>
            ; Marcuet al, 2006; Shen et al, 2008
            <papid>P08-1066</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               In addition, researchers have recently added syntactic features tophrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008
               <papid>W08-0302</papid>
               ; Haque et al, 2009; Chiang et al, 2008
               <papid>D08-1024</papid>
               ).
            </nextsent>
            <nextsent>In this work, we focus on syntactic features of target-side dependency trees, ? t , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               For example, Quirk et al(2005) use features involving phrases and source side dependency trees and Mi et al (2008)
               <papid>P10-5002</papid>
               use features from a forest of parses of the source sen tence.
            </prevsent>
            <prevsent>
               There is also substantial work in the use of target-side syntax (Galley et al, 2006
               <papid>P06-1121</papid>
               ; Marcuet al, 2006; Shen et al, 2008
               <papid>P08-1066</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" W08-0302 ">
            In addition, researchers have recently added syntactic features tophrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008
            <papid>W08-0302</papid>
            ; Haque et al, 2009; Chiang et al, 2008
            <papid>D08-1024</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>In this work, we focus on syntactic features of target-side dependency trees, ? t , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features.</nextsent>
            <nextsent>They factor as in Eq.</nextsent>
            <nextsent>5 (Tab.</nextsent>
            <nextsent>2).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               For example, Quirk et al(2005) use features involving phrases and source side dependency trees and Mi et al (2008)
               <papid>P10-5002</papid>
               use features from a forest of parses of the source sen tence.
            </prevsent>
            <prevsent>
               There is also substantial work in the use of target-side syntax (Galley et al, 2006
               <papid>P06-1121</papid>
               ; Marcuet al, 2006; Shen et al, 2008
               <papid>P08-1066</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" D08-1024 ">
            In addition, researchers have recently added syntactic features tophrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008
            <papid>W08-0302</papid>
            ; Haque et al, 2009; Chiang et al, 2008
            <papid>D08-1024</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>In this work, we focus on syntactic features of target-side dependency trees, ? t , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features.</nextsent>
            <nextsent>They factor as in Eq.</nextsent>
            <nextsent>5 (Tab.</nextsent>
            <nextsent>2).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>2).</prevsent>
            <prevsent>Features that consider only target-side syntax and words without considering s can be seen as syntactic language model?</prevsent>
         </prevsection>
         <citsent citstr=" P08-1066 ">
            features (Shen et al, 2008
            <papid>P08-1066</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>5 Segmentation might be modeled as a hidden variable in future work.</nextsent>
            <nextsent>g trans (s,a, t) = P m j=1 P ia(j) f lex (s i , t j ) (3) + P i,j:1i&lt;jm f phr (s last(i,j) first(i,j) , t j i ) g lm (t) = P N?{2,3} P m+1 j=1 f N (t j jN+1 ) (4) g syn (t, ? t ) = P m j=1 f att (t j , j, t ? t (j) , ? t (j)) +f val (t j , j, ? 1 t (j)) (5) g reor (s, ? s ,a, t, ? t ) = P m j=1 P ia(j) f dist (i, j) (6) g tree 2 (?</nextsent>
            <nextsent>s ,a, ? t ) = m X j=1 f qg (a(j),a(?</nextsent>
            <nextsent>t (j)), j, ? t (j)) (7) Table 2: Factoring of global feature collections g into f . x j i denotes x i , . . .</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>first(i, j) = min k:ikj (min(a(k))) and last(i, j) = max k:ikj (max(a(k))).</prevsent>
            <prevsent>2.4 Reordering.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) aswell as through distance-based distortion models (Koehn et al, 2003
            <papid>N03-1017</papid>
            ) and lexicalized reordering models (Koehn et al, 2007
            <papid>P07-2045</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>In syntax-based systems, reordering is typically parameterized by grammar rules.</nextsent>
            <nextsent>For generality we permit these features to see?</nextsent>
            <nextsent>all structures and denote them g reor (s, ? s ,a, t, ? t).</nextsent>
            <nextsent>Eq.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Model.</section>
      <citcontext>
         <prevsection>
            <prevsent>first(i, j) = min k:ikj (min(a(k))) and last(i, j) = max k:ikj (max(a(k))).</prevsent>
            <prevsent>2.4 Reordering.</prevsent>
         </prevsection>
         <citsent citstr=" P07-2045 ">
            Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) aswell as through distance-based distortion models (Koehn et al, 2003
            <papid>N03-1017</papid>
            ) and lexicalized reordering models (Koehn et al, 2007
            <papid>P07-2045</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>In syntax-based systems, reordering is typically parameterized by grammar rules.</nextsent>
            <nextsent>For generality we permit these features to see?</nextsent>
            <nextsent>all structures and denote them g reor (s, ? s ,a, t, ? t).</nextsent>
            <nextsent>Eq.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Quasi-Synchronous Grammars.</section>
      <citcontext>
         <prevsection>
            <prevsent>t (j)) and a(j), the source-side words to which t ? t (j) and t j align.</prevsent>
            <prevsent>If, for example, we require that, for all j, a(?</prevsent>
         </prevsection>
         <citsent citstr=" J00-1004 ">
            t (j)) = ? s (a(j)) or a(j) = 0, and that the root of ? t must align to the root of ? s or to NULL, then strict isomorphism must hold between ? s and ? t , and we have implemented a synchronous CF dependency grammar (Alshawi et al, 2000
            <papid>J00-1004</papid>
            ; Ding and Palmer, 2005
            <papid>P05-1067</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment.</nextsent>
            <nextsent>(a(?</nextsent>
            <nextsent>t (j)) = ? s (a(j))?</nextsent>
            <nextsent>corresponds to their parent-child?</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Quasi-Synchronous Grammars.</section>
      <citcontext>
         <prevsection>
            <prevsent>t (j)) and a(j), the source-side words to which t ? t (j) and t j align.</prevsent>
            <prevsent>If, for example, we require that, for all j, a(?</prevsent>
         </prevsection>
         <citsent citstr=" P05-1067 ">
            t (j)) = ? s (a(j)) or a(j) = 0, and that the root of ? t must align to the root of ? s or to NULL, then strict isomorphism must hold between ? s and ? t , and we have implemented a synchronous CF dependency grammar (Alshawi et al, 2000
            <papid>J00-1004</papid>
            ; Ding and Palmer, 2005
            <papid>P05-1067</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment.</nextsent>
            <nextsent>(a(?</nextsent>
            <nextsent>t (j)) = ? s (a(j))?</nextsent>
            <nextsent>corresponds to their parent-child?</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Decoding.</section>
      <citcontext>
         <prevsection>
            <prevsent>, n} where 0 denotes alignment to NULL.</prevsent>
            <prevsent>7 Arguably, we seek argmax t p(t | s), marginalizing out everything else.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1023 ">
            Approximate solutions have been proposed for that problem in several settings (Blunsom and Osborne, 2008
            <papid>D08-1023</papid>
            ; Sun and Tsujii, 2009
            <papid>E09-1088</papid>
            ); we leave their combination with our approach to future work.
         </citsent>
         <aftsection>
            <nextsent>lem.)</nextsent>
            <nextsent>As usual, the normalization constant is not required for decoding; it suffices to solve: t ? , ? ?</nextsent>
            <nextsent>t ,a ? ?</nextsent>
            <nextsent>= argmax t,?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Decoding.</section>
      <citcontext>
         <prevsection>
            <prevsent>, n} where 0 denotes alignment to NULL.</prevsent>
            <prevsent>7 Arguably, we seek argmax t p(t | s), marginalizing out everything else.</prevsent>
         </prevsection>
         <citsent citstr=" E09-1088 ">
            Approximate solutions have been proposed for that problem in several settings (Blunsom and Osborne, 2008
            <papid>D08-1023</papid>
            ; Sun and Tsujii, 2009
            <papid>E09-1088</papid>
            ); we leave their combination with our approach to future work.
         </citsent>
         <aftsection>
            <nextsent>lem.)</nextsent>
            <nextsent>As usual, the normalization constant is not required for decoding; it suffices to solve: t ? , ? ?</nextsent>
            <nextsent>t ,a ? ?</nextsent>
            <nextsent>= argmax t,?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Decoding.</section>
      <citcontext>
         <prevsection>
            <prevsent>Most MT decoders enforce a notion of coverage?</prevsent>
            <prevsent>of the source sentence during translation: all parts of s should be aligned to some part of t (alignmentto NULL incurs an explicit cost).</prevsent>
         </prevsection>
         <citsent citstr=" P07-2045 ">
            Phrase-based systems such as Moses (Koehn et al, 2007
            <papid>P07-2045</papid>
            ) explicitly search for the highest-scoring string in which all source words are translated.
         </citsent>
         <aftsection>
            <nextsent>Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar,ensuring that every phrase and word has an analogue in ? t (or a deliberate choice is made by the decoder to translate it to NULL).</nextsent>
            <nextsent>In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder.Our QDG decoder has no way to enforce cov erage; it does not track any kind of state in ? s apart from a single recently aligned word.</nextsent>
            <nextsent>
               This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al, 1993
               <papid>J93-2003</papid>
               ).
            </nextsent>
            <nextsent>This sacrifice is the result of our choice to use a conditional model (2).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Decoding.</section>
      <citcontext>
         <prevsection>
            <prevsent>Systems based on synchronous grammars proceed by parsing the source sentence with the synchronous grammar,ensuring that every phrase and word has an analogue in ? t (or a deliberate choice is made by the decoder to translate it to NULL).</prevsent>
            <prevsent>In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder.Our QDG decoder has no way to enforce cov erage; it does not track any kind of state in ? s apart from a single recently aligned word.</prevsent>
         </prevsection>
         <citsent citstr=" J93-2003 ">
            This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al, 1993
            <papid>J93-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>This sacrifice is the result of our choice to use a conditional model (2).</nextsent>
            <nextsent>The solution is to introduce a set of coverage features g cov (a).</nextsent>
            <nextsent>Here, these include: ? A counter for the number of times each source word is covered: f scov (a) = ? n i=1 |a 1 (i)|.</nextsent>
            <nextsent>Features that fire once when a source word is covered the zth time (z ? {2, 3, 4}) and fire again all subsequent times it is covered; these are denoted f 2nd , f 3rd , and f 4th . ? A counter of uncovered source words: f sunc (a) = ? n i=1 ?(|a 1 (i)|, 0).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Training.</section>
      <citcontext>
         <prevsection>
            <prevsent>11 Computing the numerator in Eq.</prevsent>
            <prevsent>9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j)| for all j, a fast inside?</prevsent>
         </prevsection>
         <citsent citstr=" W06-3104 ">
            DP solution is known (Smith and Eisner, 2006
            <papid>W06-3104</papid>
            ; Wang et al, 2007
            <papid>D07-1003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>It runs in O(mn 2 ) time and O(mn) space.</nextsent>
            <nextsent>Computing the denominator in Eq.</nextsent>
            <nextsent>9 requires summing over all word sequences and dependency trees for the target language sentence and all word alignments between the sentences.</nextsent>
            <nextsent>Witha maximum length imposed, this is tractable using the inside?</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Training.</section>
      <citcontext>
         <prevsection>
            <prevsent>11 Computing the numerator in Eq.</prevsent>
            <prevsent>9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j)| for all j, a fast inside?</prevsent>
         </prevsection>
         <citsent citstr=" D07-1003 ">
            DP solution is known (Smith and Eisner, 2006
            <papid>W06-3104</papid>
            ; Wang et al, 2007
            <papid>D07-1003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>It runs in O(mn 2 ) time and O(mn) space.</nextsent>
            <nextsent>Computing the denominator in Eq.</nextsent>
            <nextsent>9 requires summing over all word sequences and dependency trees for the target language sentence and all word alignments between the sentences.</nextsent>
            <nextsent>Witha maximum length imposed, this is tractable using the inside?</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Training.</section>
      <citcontext>
         <prevsection>
            <prevsent>9, never requiring summation over more than two structures at a time.</prevsent>
            <prevsent>We must sum over target word sequences and word alignments (with fixed ? t ), and separately over target trees and word alignments (with fixed t).</prevsent>
         </prevsection>
         <citsent citstr=" H92-1024 ">
            5.1 Summing over t and a. The summation over target word sequences and alignments given fixed ? t bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992
            <papid>H92-1024</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Let S(j, i, t)denote the sum of all translations rooted at position j in ? t such that a(j) = i and t j = t. Tab.</nextsent>
            <nextsent>3 gives the equations for this DP: Eq.</nextsent>
            <nextsent>11 is the quantity of interest, Eq.</nextsent>
            <nextsent>12 is the recur sion, and Eq.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Training.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007
               <papid>J07-2003</papid>
               ) that maintains k-best lists of derivations for each DP chart item.
            </prevsent>
            <prevsent>Cube summing augments the k-best list with a residual term that sums over remaining structures not inthe k-best list, albeit without their non-local fea tures.</prevsent>
         </prevsection>
         <citsent citstr=" C08-1074 ">
            Using the machinery of cube summing, it is straightforward to include the desired non-localfeatures in the summations required for pseudo likelihood, as well as to compute their approximate gradients.Our approach permits an alternative to minimum error-rate training (MERT; Och, 2003
            <papid>C08-1074</papid>
            ); it is discriminative but handles latent structure and regularization in more principled ways.
         </citsent>
         <aftsection>
            <nextsent>The pseudo likelihood calculations for a sentence pair, taken together, are faster than (k-best) decoding, making SGAs inner loop faster than MERTs inner loop.</nextsent>
            <nextsent>Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based andsyntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output.</nextsent>
            <nextsent>6.1 Data and Evaluation.</nextsent>
            <nextsent>We use the German-English portion of the Basic Travel Expression Corpus (BTEC).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>We filter sentences of length more than 15 words, which only removes 6% of the data.</prevsent>
            <prevsent>We end upwith a training set of 82,299 sentences, a development set of 934 sentences, and a test set of 500 sentences.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1072 ">
            We evaluate translation output using case-insensitive BLEU (Papineni et al, 2001
            <papid>C04-1072</papid>
            ), as provided by NIST, and METEOR (Banerjee and Lavie, 2005
            <papid>W05-0909</papid>
            ), version 0.6, with Porter stemming and WordNet synonym matching.
         </citsent>
         <aftsection>
            <nextsent>6.2 Features.</nextsent>
            <nextsent>Our base system uses features as discussed in 2.</nextsent>
            <nextsent>To obtain lexical translation features g trans (s,a, t), we use the Moses pipeline (Koehnet al, 2007).</nextsent>
            <nextsent>
               We perform word alignment using GIZA++ (Och and Ney, 2003
               <papid>J03-1002</papid>
               ), symmetrize the alignments using the grow-diag-final-and?
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>We filter sentences of length more than 15 words, which only removes 6% of the data.</prevsent>
            <prevsent>We end upwith a training set of 82,299 sentences, a development set of 934 sentences, and a test set of 500 sentences.</prevsent>
         </prevsection>
         <citsent citstr=" W05-0909 ">
            We evaluate translation output using case-insensitive BLEU (Papineni et al, 2001
            <papid>C04-1072</papid>
            ), as provided by NIST, and METEOR (Banerjee and Lavie, 2005
            <papid>W05-0909</papid>
            ), version 0.6, with Porter stemming and WordNet synonym matching.
         </citsent>
         <aftsection>
            <nextsent>6.2 Features.</nextsent>
            <nextsent>Our base system uses features as discussed in 2.</nextsent>
            <nextsent>To obtain lexical translation features g trans (s,a, t), we use the Moses pipeline (Koehnet al, 2007).</nextsent>
            <nextsent>
               We perform word alignment using GIZA++ (Och and Ney, 2003
               <papid>J03-1002</papid>
               ), symmetrize the alignments using the grow-diag-final-and?
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>Our base system uses features as discussed in 2.</prevsent>
            <prevsent>To obtain lexical translation features g trans (s,a, t), we use the Moses pipeline (Koehnet al, 2007).</prevsent>
         </prevsection>
         <citsent citstr=" J03-1002 ">
            We perform word alignment using GIZA++ (Och and Ney, 2003
            <papid>J03-1002</papid>
            ), symmetrize the alignments using the grow-diag-final-and?
         </citsent>
         <aftsection>
            <nextsent>heuristic, and extract phrases up to length 3.</nextsent>
            <nextsent>We define f lex by the lexical probabilities p(t | s) andp(s | t) estimated from the symmetrized alignments.</nextsent>
            <nextsent>After discarding phrase pairs with onlyone target-side word (since we only allow a target word to align to at most one source word), we define f phr by 8 features: {2, 3} target words phrase conditional and lexical smoothing?</nextsent>
            <nextsent>probabilities ? two conditional directions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>After discarding phrase pairs with onlyone target-side word (since we only allow a target word to align to at most one source word), we define f phr by 8 features: {2, 3} target words phrase conditional and lexical smoothing?</prevsent>
            <prevsent>probabilities ? two conditional directions.</prevsent>
         </prevsection>
         <citsent citstr=" P96-1041 ">
            Bigram and trigam language model features, f 2 and f 3, are estimated using the SRI toolkit (Stol cke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998
            <papid>P96-1041</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               For our target-language syntactic features g syn , we use features similar to lexicalized CFG events(Collins, 1999
               <papid>J03-4003</papid>
               ), specifically following the dependency model of Klein and Manning (2004)
               <papid>P04-1061</papid>
               .These include probabilities associated with individual attachments (f att ) and child-generation valence probabilities (f val ).
            </nextsent>
            <nextsent>These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>probabilities ? two conditional directions.</prevsent>
            <prevsent>
               Bigram and trigam language model features, f 2 and f 3, are estimated using the SRI toolkit (Stol cke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998
               <papid>P96-1041</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" J03-4003 ">
            For our target-language syntactic features g syn , we use features similar to lexicalized CFG events(Collins, 1999
            <papid>J03-4003</papid>
            ), specifically following the dependency model of Klein and Manning (2004)
            <papid>P04-1061</papid>
            .These include probabilities associated with individual attachments (f att ) and child-generation valence probabilities (f val ).
         </citsent>
         <aftsection>
            <nextsent>These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003).</nextsent>
            <nextsent>
               The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA++ mkcls utility (Och and Ney, 2003
               <papid>J03-1002</papid>
               ).
            </nextsent>
            <nextsent>In total, there are 7 lexical and 7 word-class syntax features.For reordering, we use a single absolute distortion feature f dist (i, j) that returns |ij|whenever a(j) = i and i, j &gt; 0.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>probabilities ? two conditional directions.</prevsent>
            <prevsent>
               Bigram and trigam language model features, f 2 and f 3, are estimated using the SRI toolkit (Stol cke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998
               <papid>P96-1041</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P04-1061 ">
            For our target-language syntactic features g syn , we use features similar to lexicalized CFG events(Collins, 1999
            <papid>J03-4003</papid>
            ), specifically following the dependency model of Klein and Manning (2004)
            <papid>P04-1061</papid>
            .These include probabilities associated with individual attachments (f att ) and child-generation valence probabilities (f val ).
         </citsent>
         <aftsection>
            <nextsent>These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003).</nextsent>
            <nextsent>
               The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA++ mkcls utility (Och and Ney, 2003
               <papid>J03-1002</papid>
               ).
            </nextsent>
            <nextsent>In total, there are 7 lexical and 7 word-class syntax features.For reordering, we use a single absolute distortion feature f dist (i, j) that returns |ij|whenever a(j) = i and i, j &gt; 0.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               For our target-language syntactic features g syn , we use features similar to lexicalized CFG events(Collins, 1999
               <papid>J03-4003</papid>
               ), specifically following the dependency model of Klein and Manning (2004)
               <papid>P04-1061</papid>
               .These include probabilities associated with individual attachments (f att ) and child-generation valence probabilities (f val ).
            </prevsent>
            <prevsent>These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003).</prevsent>
         </prevsection>
         <citsent citstr=" J03-1002 ">
            The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA++ mkcls utility (Och and Ney, 2003
            <papid>J03-1002</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>In total, there are 7 lexical and 7 word-class syntax features.For reordering, we use a single absolute distortion feature f dist (i, j) that returns |ij|whenever a(j) = i and i, j &gt; 0.</nextsent>
            <nextsent>(Unlike the other feature functions, which returned probabilities, this feature function returns a nonnegative integer.)</nextsent>
            <nextsent>The tree-to-tree syntactic features g tree 2 in our model are binary features f qgthat fire for particular QG configurations.</nextsent>
            <nextsent>
               We use one feature for each of the configurations in (Smith and Eisner, 2006
               <papid>W06-3104</papid>
               ),adding 7 additional features that score configura 225 Phrase Syntactic Features: features: +f att ? f val +f qg (base) (target) (tree-to-tree) (base) 0.3727 0.4458 0.4424 +f phr 0.4682 0.4971 0.5142 Table 4: Feature set comparison (BLEU).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1023.xml">feature rich translation by quasi synchronous lattice parsing</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>(Unlike the other feature functions, which returned probabilities, this feature function returns a nonnegative integer.)</prevsent>
            <prevsent>The tree-to-tree syntactic features g tree 2 in our model are binary features f qgthat fire for particular QG configurations.</prevsent>
         </prevsection>
         <citsent citstr=" W06-3104 ">
            We use one feature for each of the configurations in (Smith and Eisner, 2006
            <papid>W06-3104</papid>
            ),adding 7 additional features that score configura 225 Phrase Syntactic Features: features: +f att ? f val +f qg (base) (target) (tree-to-tree) (base) 0.3727 0.4458 0.4424 +f phr 0.4682 0.4971 0.5142 Table 4: Feature set comparison (BLEU).
         </citsent>
         <aftsection>
            <nextsent>tions involving root words and NULL-alignments more finely.</nextsent>
            <nextsent>There are 14 features in this category.</nextsent>
            <nextsent>Coverage features g cov are as described in 4.2.</nextsent>
            <nextsent>In all, 46 feature weights are learned.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Most notably, our structured ramp loss minimization algorithm, RAMPION, is less sensitive to initialization and random seeds than standard approaches.</prevsent>
            <prevsent>Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text.</prevsent>
         </prevsection>
         <citsent citstr=" C08-1074 ">
            Well-known examples include MERT (Och, 2003
            <papid>C08-1074</papid>
            ), MIRA (Chiang et al, 2008
            <papid>D08-1024</papid>
            ), and PRO (Hopkins and May, 2011
            <papid>D11-1125</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>While such procedures can be analyzed as machine learning algorithmse.g., in the general framework of empirical risk minimization (Vapnik, 1998)their procedural specifications have made this difficult.From a practical perspective, such algorithms are of ten complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparam eters.</nextsent>
            <nextsent>In this paper, we consider training algorithms that are first specified declaratively, as loss functions tobe minimized.</nextsent>
            <nextsent>We relate well-known training algorithms for MT to particular loss functions.</nextsent>
            <nextsent>We show that a family of structured ramp loss functions (Do et al, 2008) is useful for this analysis.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Most notably, our structured ramp loss minimization algorithm, RAMPION, is less sensitive to initialization and random seeds than standard approaches.</prevsent>
            <prevsent>Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1024 ">
            Well-known examples include MERT (Och, 2003
            <papid>C08-1074</papid>
            ), MIRA (Chiang et al, 2008
            <papid>D08-1024</papid>
            ), and PRO (Hopkins and May, 2011
            <papid>D11-1125</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>While such procedures can be analyzed as machine learning algorithmse.g., in the general framework of empirical risk minimization (Vapnik, 1998)their procedural specifications have made this difficult.From a practical perspective, such algorithms are of ten complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparam eters.</nextsent>
            <nextsent>In this paper, we consider training algorithms that are first specified declaratively, as loss functions tobe minimized.</nextsent>
            <nextsent>We relate well-known training algorithms for MT to particular loss functions.</nextsent>
            <nextsent>We show that a family of structured ramp loss functions (Do et al, 2008) is useful for this analysis.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Most notably, our structured ramp loss minimization algorithm, RAMPION, is less sensitive to initialization and random seeds than standard approaches.</prevsent>
            <prevsent>Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text.</prevsent>
         </prevsection>
         <citsent citstr=" D11-1125 ">
            Well-known examples include MERT (Och, 2003
            <papid>C08-1074</papid>
            ), MIRA (Chiang et al, 2008
            <papid>D08-1024</papid>
            ), and PRO (Hopkins and May, 2011
            <papid>D11-1125</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>While such procedures can be analyzed as machine learning algorithmse.g., in the general framework of empirical risk minimization (Vapnik, 1998)their procedural specifications have made this difficult.From a practical perspective, such algorithms are of ten complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparam eters.</nextsent>
            <nextsent>In this paper, we consider training algorithms that are first specified declaratively, as loss functions tobe minimized.</nextsent>
            <nextsent>We relate well-known training algorithms for MT to particular loss functions.</nextsent>
            <nextsent>We show that a family of structured ramp loss functions (Do et al, 2008) is useful for this analysis.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>For example, McAllester and Keshet (2011) recently suggested that, while Chiang et al (2008, 2009) described their algorithm as MIRA?</prevsent>
            <prevsent>(Crammer et al, 2006), in factit targets a kind of ramp loss.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1096 ">
            We note here other ex amples: Liang et al (2006)
            <papid>P06-1096</papid>
            described their algorithm as a variant of the perceptron (Collins, 2002
            <papid>W02-1001</papid>
            ), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al).
         </citsent>
         <aftsection>
            <nextsent>
               Och and Ney (2002)
               <papid>P02-1038</papid>
               sought to optimize log loss (likelihood in a probabilistic model; Lafferty et al, 2001) but actually optimized a version of the soft ramp loss.
            </nextsent>
            <nextsent>Why isnt the application of ML to MT more straightforward?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>(Crammer et al, 2006), in factit targets a kind of ramp loss.</prevsent>
            <prevsent>
               We note here other ex amples: Liang et al (2006)
               <papid>P06-1096</papid>
               described their algorithm as a variant of the perceptron (Collins, 2002
               <papid>W02-1001</papid>
               ), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al).
            </prevsent>
         </prevsection>
         <citsent citstr=" P02-1038 ">
            Och and Ney (2002)
            <papid>P02-1038</papid>
            sought to optimize log loss (likelihood in a probabilistic model; Lafferty et al, 2001) but actually optimized a version of the soft ramp loss.
         </citsent>
         <aftsection>
            <nextsent>Why isnt the application of ML to MT more straightforward?</nextsent>
            <nextsent>We note two key reasons: (i) ML generally assumes that the correct output can alwaysbe scored by a model, but in MT the reference translation is often unreachable, due to a models limited expressive power or search error, requiring the useof surrogate?</nextsent>
            <nextsent>references; (ii) MT models nearly always include latent derivation variables, leading to non-convex losses that have generally received little attention in ML.</nextsent>
            <nextsent>In this paper, we discuss how these two have caused a disconnect between the loss function minimized by an algorithm in ML and the loss minimized when it is adapted for MT. From a practical perspective, our framework leads to a simple training algorithm for structured ramp loss based on general optimization techniques.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Notation and Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>Ep(X,Y ) [loss (X,Y ,?)] (1)where p(X,Y ) is the (unknown) true joint distribution over corpora.</prevsent>
            <prevsent>We note that the loss function depends on the entire corpus, while the decoder operates independently on one sentence at a time.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            Thisis done to fit the standard assumptions in MT sys tems: the evaluation metric (e.g., BLEU) depends on 1For phrase-based MT, a segmentation of the source and target sentences into phrases and an alignment between them (Koehn et al, 2003
            <papid>N03-1017</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               For hierarchical phrase-based MT, a derivation under a synchronous CFG (Chiang, 2005
               <papid>P12-2057</papid>
               ).
            </nextsent>
            <nextsent>the entire corpus and does not decompose linearly, while the model score does.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Notation and Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>We note that the loss function depends on the entire corpus, while the decoder operates independently on one sentence at a time.</prevsent>
            <prevsent>
               Thisis done to fit the standard assumptions in MT sys tems: the evaluation metric (e.g., BLEU) depends on 1For phrase-based MT, a segmentation of the source and target sentences into phrases and an alignment between them (Koehn et al, 2003
               <papid>N03-1017</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P12-2057 ">
            For hierarchical phrase-based MT, a derivation under a synchronous CFG (Chiang, 2005
            <papid>P12-2057</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>the entire corpus and does not decompose linearly, while the model score does.</nextsent>
            <nextsent>Since in practice we donot know p(X,Y ), but we do have access to an actual corpus pair X?, Y?</nextsent>
            <nextsent>?, where X?</nextsent>
            <nextsent>= {x(i)}Ni=1 and Y?</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Notation and Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>In this paper, we use `2.</prevsent>
            <prevsent>Models are evaluated using a task-specific notion of error, here encoded as a cost function, cost : YN ? YN ? R0, such that the worse a translation is, the higher its cost.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1072 ">
            The cost function will typically make use of an automatic evaluation metric for machine translation; e.g., cost might be 1 minus the BLEU score (Papineni et al, 2001
            <papid>C04-1072</papid>
            ).2We note that our analysis in this paper is applicable for understanding the loss function being optimized given a fixed set of k-best lists.3 However,most training procedures periodically invoke the decoder to generate new k-best lists, which are then typically merged with those from previous training iterations.
         </citsent>
         <aftsection>
            <nextsent>It is an open question how this practice affects the loss function being optimized by the procedure as a whole.</nextsent>
            <nextsent>Example 1: MERT.</nextsent>
            <nextsent>The most commonly-usedtraining algorithm for machine translation is minimum error rate training, which seeks to directly minimize the cost of the predictions on the trainingdata.</nextsent>
            <nextsent>This idea has been used in the pattern recognition and speech recognition communities (Duda and Hart, 1973; Juang et al, 1997); its first application to MT was by Och (2003).</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Notation and Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>= cost(y, y?,h??)</prevsent>
            <prevsent>= cost(y,y?).</prevsent>
         </prevsection>
         <citsent citstr=" N12-1047 ">
            3Cherry and Foster (2012)
            <papid>N12-1047</papid>
            have concurrently performed a similar analysis.
         </citsent>
         <aftsection>
            <nextsent>222 MERT directly minimizes the corpus-level cost function of the best outputs from the decoder with out any regularization (i.e., R(?)</nextsent>
            <nextsent>= 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search.MERT avoids the need to compute feature vectors for the references (1(i)) and allows corpus level metrics like BLEU to be easily incorporated.However, the complexity of the loss and the difficulty of the search lead to instabilities during learning.</nextsent>
            <nextsent>
               Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al, 2008
               <papid>W08-0304</papid>
               ; Moore and Quirk, 2008
               <papid>C08-1074</papid>
               ;Foster and Kuhn, 2009
               <papid>W09-0439</papid>
               ; Clark et al, 2011
               <papid>P11-2031</papid>
               ).
            </nextsent>
            <nextsent>But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows.Example 2: Probabilistic Models.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Notation and Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>222 MERT directly minimizes the corpus-level cost function of the best outputs from the decoder with out any regularization (i.e., R(?)</prevsent>
            <prevsent>= 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search.MERT avoids the need to compute feature vectors for the references (1(i)) and allows corpus level metrics like BLEU to be easily incorporated.However, the complexity of the loss and the difficulty of the search lead to instabilities during learning.</prevsent>
         </prevsection>
         <citsent citstr=" W08-0304 ">
            Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al, 2008
            <papid>W08-0304</papid>
            ; Moore and Quirk, 2008
            <papid>C08-1074</papid>
            ;Foster and Kuhn, 2009
            <papid>W09-0439</papid>
            ; Clark et al, 2011
            <papid>P11-2031</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows.Example 2: Probabilistic Models.</nextsent>
            <nextsent>By exponenti ating and normalizing score(x,y,h;?), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: p?(y,h|x) = 1Z(x,?)</nextsent>
            <nextsent>exp{score(x,y,h;?)} (4) The log loss then defines losslog(X?, Y?</nextsent>
            <nextsent>= ? N i=1 log p?(y (i) | x(i)).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Notation and Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>222 MERT directly minimizes the corpus-level cost function of the best outputs from the decoder with out any regularization (i.e., R(?)</prevsent>
            <prevsent>= 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search.MERT avoids the need to compute feature vectors for the references (1(i)) and allows corpus level metrics like BLEU to be easily incorporated.However, the complexity of the loss and the difficulty of the search lead to instabilities during learning.</prevsent>
         </prevsection>
         <citsent citstr=" C08-1074 ">
            Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al, 2008
            <papid>W08-0304</papid>
            ; Moore and Quirk, 2008
            <papid>C08-1074</papid>
            ;Foster and Kuhn, 2009
            <papid>W09-0439</papid>
            ; Clark et al, 2011
            <papid>P11-2031</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows.Example 2: Probabilistic Models.</nextsent>
            <nextsent>By exponenti ating and normalizing score(x,y,h;?), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: p?(y,h|x) = 1Z(x,?)</nextsent>
            <nextsent>exp{score(x,y,h;?)} (4) The log loss then defines losslog(X?, Y?</nextsent>
            <nextsent>= ? N i=1 log p?(y (i) | x(i)).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Notation and Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>222 MERT directly minimizes the corpus-level cost function of the best outputs from the decoder with out any regularization (i.e., R(?)</prevsent>
            <prevsent>= 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search.MERT avoids the need to compute feature vectors for the references (1(i)) and allows corpus level metrics like BLEU to be easily incorporated.However, the complexity of the loss and the difficulty of the search lead to instabilities during learning.</prevsent>
         </prevsection>
         <citsent citstr=" W09-0439 ">
            Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al, 2008
            <papid>W08-0304</papid>
            ; Moore and Quirk, 2008
            <papid>C08-1074</papid>
            ;Foster and Kuhn, 2009
            <papid>W09-0439</papid>
            ; Clark et al, 2011
            <papid>P11-2031</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows.Example 2: Probabilistic Models.</nextsent>
            <nextsent>By exponenti ating and normalizing score(x,y,h;?), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: p?(y,h|x) = 1Z(x,?)</nextsent>
            <nextsent>exp{score(x,y,h;?)} (4) The log loss then defines losslog(X?, Y?</nextsent>
            <nextsent>= ? N i=1 log p?(y (i) | x(i)).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Notation and Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>222 MERT directly minimizes the corpus-level cost function of the best outputs from the decoder with out any regularization (i.e., R(?)</prevsent>
            <prevsent>= 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search.MERT avoids the need to compute feature vectors for the references (1(i)) and allows corpus level metrics like BLEU to be easily incorporated.However, the complexity of the loss and the difficulty of the search lead to instabilities during learning.</prevsent>
         </prevsection>
         <citsent citstr=" P11-2031 ">
            Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al, 2008
            <papid>W08-0304</papid>
            ; Moore and Quirk, 2008
            <papid>C08-1074</papid>
            ;Foster and Kuhn, 2009
            <papid>W09-0439</papid>
            ; Clark et al, 2011
            <papid>P11-2031</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows.Example 2: Probabilistic Models.</nextsent>
            <nextsent>By exponenti ating and normalizing score(x,y,h;?), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: p?(y,h|x) = 1Z(x,?)</nextsent>
            <nextsent>exp{score(x,y,h;?)} (4) The log loss then defines losslog(X?, Y?</nextsent>
            <nextsent>= ? N i=1 log p?(y (i) | x(i)).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Notation and Background.</section>
      <citcontext>
         <prevsection>
            <prevsent>4) and a cost function to define a loss: lossB risk = N i=1 Ep?(y,h|x(i))[cost(y (i),y)] (5) The use of this loss is often simply called risk minimization?</prevsent>
            <prevsent>in the speech and MT communities.Bayes risk is non-convex, whether or not latent variables are present.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1076 ">
            Like MERT, it naturally avoids the need to compute features for y(i) and uses a cost function, making it appealing for MT. Bayes risk minimization first appeared in the speech recognition community (Kaiser et al, 2000; Povey and 4However, Cer et al (2008) and Macherey et al (2008)
            <papid>D08-1076</papid>
            achieved a sort of regularization by altering MERTs line search.Woodland, 2002) and more recently has been applied to MT (Smith and Eisner, 2006
            <papid>P06-2101</papid>
            ; Zens et al, 2007
            <papid>D07-1055</papid>
            ; Li and Eisner, 2009
            <papid>D09-1005</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>In this section we consider other ML-inspired approaches to MT training, situating each in the framework from 2: ramp, perceptron, hinge, and soft?</nextsent>
            <nextsent>losses.</nextsent>
            <nextsent>Each of the first three kinds of losses can be understood as a way of selecting, for each x(i), two candidate translation/derivation pairs: y?,h??</nextsent>
            <nextsent>and y?,h??.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>This involves first finding the translation to update towards for all sentences in the tuning set (lines 57), then making parameter updates in an online fashion with T ??</prevsent>
            <prevsent>epochs of stochastic sub gradient descent (lines 814).</prevsent>
         </prevsection>
         <citsent citstr=" D07-1080 ">
            The sub gradient update for the `2 regularization term is done in line 11 and then for the loss in line 12.6Unlike prior work that targeted similar loss functions (Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2008
            <papid>D08-1024</papid>
            ;Chiang et al, 2009
            <papid>N09-1025</papid>
            ), we do not use a fully online algorithm such as MIRA in an outer loop because we are not aware of an online learning algorithm with theoretical guarantees for non-differentiable, non convex loss functions like the ramp losses.
         </citsent>
         <aftsection>
            <nextsent>CCCP 6`2 regularization done here regularizes toward 0, not 0.</nextsent>
            <nextsent>224 loss ramp 1 = N?</nextsent>
            <nextsent>i=1 ? max y,hTi (scorei(y,h;?)) + max y,hTi (scorei(y,h;?)</nextsent>
            <nextsent>+ costi(y)) (6) loss ramp 2 = N?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>This involves first finding the translation to update towards for all sentences in the tuning set (lines 57), then making parameter updates in an online fashion with T ??</prevsent>
            <prevsent>epochs of stochastic sub gradient descent (lines 814).</prevsent>
         </prevsection>
         <citsent citstr=" D08-1024 ">
            The sub gradient update for the `2 regularization term is done in line 11 and then for the loss in line 12.6Unlike prior work that targeted similar loss functions (Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2008
            <papid>D08-1024</papid>
            ;Chiang et al, 2009
            <papid>N09-1025</papid>
            ), we do not use a fully online algorithm such as MIRA in an outer loop because we are not aware of an online learning algorithm with theoretical guarantees for non-differentiable, non convex loss functions like the ramp losses.
         </citsent>
         <aftsection>
            <nextsent>CCCP 6`2 regularization done here regularizes toward 0, not 0.</nextsent>
            <nextsent>224 loss ramp 1 = N?</nextsent>
            <nextsent>i=1 ? max y,hTi (scorei(y,h;?)) + max y,hTi (scorei(y,h;?)</nextsent>
            <nextsent>+ costi(y)) (6) loss ramp 2 = N?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>This involves first finding the translation to update towards for all sentences in the tuning set (lines 57), then making parameter updates in an online fashion with T ??</prevsent>
            <prevsent>epochs of stochastic sub gradient descent (lines 814).</prevsent>
         </prevsection>
         <citsent citstr=" N09-1025 ">
            The sub gradient update for the `2 regularization term is done in line 11 and then for the loss in line 12.6Unlike prior work that targeted similar loss functions (Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2008
            <papid>D08-1024</papid>
            ;Chiang et al, 2009
            <papid>N09-1025</papid>
            ), we do not use a fully online algorithm such as MIRA in an outer loop because we are not aware of an online learning algorithm with theoretical guarantees for non-differentiable, non convex loss functions like the ramp losses.
         </citsent>
         <aftsection>
            <nextsent>CCCP 6`2 regularization done here regularizes toward 0, not 0.</nextsent>
            <nextsent>224 loss ramp 1 = N?</nextsent>
            <nextsent>i=1 ? max y,hTi (scorei(y,h;?)) + max y,hTi (scorei(y,h;?)</nextsent>
            <nextsent>+ costi(y)) (6) loss ramp 2 = N?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>is fundamentally a batch optimization algorithm andhas been used for solving many non-convex learning problems, such as latent structured SVMs (Yu and Joachims, 2009).</prevsent>
            <prevsent>3.2 Structured Perceptron.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1096 ">
            The stuctured perceptron algorithm (Collins, 2002
            <papid>W02-1001</papid>
            )was considered by Liang et al (2006)
            <papid>P06-1096</papid>
            as an alternative to MERT.
         </citsent>
         <aftsection>
            <nextsent>It requires only a decoder and comes with some attractive guarantees, at least for models without latent variables.</nextsent>
            <nextsent>Liang et al modified the perceptron in several ways for use in MT. The first was to generalize it to handle latent variables.</nextsent>
            <nextsent>The second change relates to the need to compute the feature vector for the reference translation y(i), which may be unreachable (1(i)).</nextsent>
            <nextsent>To address this, researchers have proposed the use of surrogates that are both favored by the current model parameters and similar to the reference.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>The second change relates to the need to compute the feature vector for the reference translation y(i), which may be unreachable (1(i)).</prevsent>
            <prevsent>To address this, researchers have proposed the use of surrogates that are both favored by the current model parameters and similar to the reference.</prevsent>
         </prevsection>
         <citsent citstr=" P02-1038 ">
            Och and Ney (2002)
            <papid>P02-1038</papid>
            were the first to do so, using the translation on a k-best list with the highest evaluation metric score as y?.
         </citsent>
         <aftsection>
            <nextsent>
               This practice was followed by Liang et al (2006)
               <papid>P06-1096</papid>
               and others with success (Arun and Koehn, 2007; Watanabe et al, 2007
               <papid>D07-1080</papid>
               ).7 Perceptron Loss Though typically described and 7Liang et al (2006)
               <papid>P06-1096</papid>
               also tried a variant that updated directly to the reference when it is reachable (bold updating?), but they and others found that Och and Neys strategy worked better.
            </nextsent>
            <nextsent>analyzed procedurally, it is straightforward to show that Collins?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>To address this, researchers have proposed the use of surrogates that are both favored by the current model parameters and similar to the reference.</prevsent>
            <prevsent>
               Och and Ney (2002)
               <papid>P02-1038</papid>
               were the first to do so, using the translation on a k-best list with the highest evaluation metric score as y?.
            </prevsent>
         </prevsection>
         <citsent citstr=" P06-1096 ">
            This practice was followed by Liang et al (2006)
            <papid>P06-1096</papid>
            and others with success (Arun and Koehn, 2007; Watanabe et al, 2007
            <papid>D07-1080</papid>
            ).7 Perceptron Loss Though typically described and 7Liang et al (2006)
            <papid>P06-1096</papid>
            also tried a variant that updated directly to the reference when it is reachable (bold updating?), but they and others found that Och and Neys strategy worked better.
         </citsent>
         <aftsection>
            <nextsent>analyzed procedurally, it is straightforward to show that Collins?</nextsent>
            <nextsent>perceptron (without latent variables) equates to SSD with fixed step size 1 on loss: N?</nextsent>
            <nextsent>i=1 score(x(i),y(i);?)+ max yY(x(i)) score(x(i),y;?)</nextsent>
            <nextsent>(12) This loss is convex but ignores cost functions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>To address this, researchers have proposed the use of surrogates that are both favored by the current model parameters and similar to the reference.</prevsent>
            <prevsent>
               Och and Ney (2002)
               <papid>P02-1038</papid>
               were the first to do so, using the translation on a k-best list with the highest evaluation metric score as y?.
            </prevsent>
         </prevsection>
         <citsent citstr=" D07-1080 ">
            This practice was followed by Liang et al (2006)
            <papid>P06-1096</papid>
            and others with success (Arun and Koehn, 2007; Watanabe et al, 2007
            <papid>D07-1080</papid>
            ).7 Perceptron Loss Though typically described and 7Liang et al (2006)
            <papid>P06-1096</papid>
            also tried a variant that updated directly to the reference when it is reachable (bold updating?), but they and others found that Och and Neys strategy worked better.
         </citsent>
         <aftsection>
            <nextsent>analyzed procedurally, it is straightforward to show that Collins?</nextsent>
            <nextsent>perceptron (without latent variables) equates to SSD with fixed step size 1 on loss: N?</nextsent>
            <nextsent>i=1 score(x(i),y(i);?)+ max yY(x(i)) score(x(i),y;?)</nextsent>
            <nextsent>(12) This loss is convex but ignores cost functions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>To address this, researchers have proposed the use of surrogates that are both favored by the current model parameters and similar to the reference.</prevsent>
            <prevsent>
               Och and Ney (2002)
               <papid>P02-1038</papid>
               were the first to do so, using the translation on a k-best list with the highest evaluation metric score as y?.
            </prevsent>
         </prevsection>
         <citsent citstr=" P06-1096 ">
            This practice was followed by Liang et al (2006)
            <papid>P06-1096</papid>
            and others with success (Arun and Koehn, 2007; Watanabe et al, 2007
            <papid>D07-1080</papid>
            ).7 Perceptron Loss Though typically described and 7Liang et al (2006)
            <papid>P06-1096</papid>
            also tried a variant that updated directly to the reference when it is reachable (bold updating?), but they and others found that Och and Neys strategy worked better.
         </citsent>
         <aftsection>
            <nextsent>analyzed procedurally, it is straightforward to show that Collins?</nextsent>
            <nextsent>perceptron (without latent variables) equates to SSD with fixed step size 1 on loss: N?</nextsent>
            <nextsent>i=1 score(x(i),y(i);?)+ max yY(x(i)) score(x(i),y;?)</nextsent>
            <nextsent>(12) This loss is convex but ignores cost functions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>It appears that Eq.</prevsent>
            <prevsent>10 (Fig.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1096 ">
            2) is the loss that Liang et al (2006)
            <papid>P06-1096</papid>
            sought to optimize, using SSD.
         </citsent>
         <aftsection>
            <nextsent>In light of footnote 5 and the non-convexity of Eq.</nextsent>
            <nextsent>10 (Fig.</nextsent>
            <nextsent>2), we have no theoretical guarantee that such an algorithm will find a (local) optimum.</nextsent>
            <nextsent>225 Input: inputs {x(i)}Ni=1, references {y (i)}Ni=1, init.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>3.3 Large-Margin Methods.</prevsent>
            <prevsent>A related family of approaches for training MT models involves the margin-infused relaxed algorithm(MIRA; Crammer et al, 2006), an online large margin training algorithm.</prevsent>
         </prevsection>
         <citsent citstr=" D07-1080 ">
            It has recently shown success for MT, particularly when training models with large feature sets (Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2008; Chiang et al, 2009
            <papid>N09-1025</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>In order to apply it to MT, Watanabe et aland Chiang et al made modifications similar to those made by Liang et al for perceptron training, namely the extension to latent variables and the use of a surrogate reference with high model score and low cost.Hinge Loss It can be shown that 1-best MIRA corresponds to dual coordinate ascent for the structured hinge loss when using `2 regularization (Martins etal., 2010).</nextsent>
            <nextsent>The structured hinge is the loss underlying maximum-margin Markov networks (Taskar et al., 2003): setting y?</nextsent>
            <nextsent>= y(i) and: y?</nextsent>
            <nextsent>= argmax yY(x(i)) ( score(x(i),y;?)</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>3.3 Large-Margin Methods.</prevsent>
            <prevsent>A related family of approaches for training MT models involves the margin-infused relaxed algorithm(MIRA; Crammer et al, 2006), an online large margin training algorithm.</prevsent>
         </prevsection>
         <citsent citstr=" N09-1025 ">
            It has recently shown success for MT, particularly when training models with large feature sets (Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2008; Chiang et al, 2009
            <papid>N09-1025</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>In order to apply it to MT, Watanabe et aland Chiang et al made modifications similar to those made by Liang et al for perceptron training, namely the extension to latent variables and the use of a surrogate reference with high model score and low cost.Hinge Loss It can be shown that 1-best MIRA corresponds to dual coordinate ascent for the structured hinge loss when using `2 regularization (Martins etal., 2010).</nextsent>
            <nextsent>The structured hinge is the loss underlying maximum-margin Markov networks (Taskar et al., 2003): setting y?</nextsent>
            <nextsent>= y(i) and: y?</nextsent>
            <nextsent>= argmax yY(x(i)) ( score(x(i),y;?)</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>in Fig.</prevsent>
            <prevsent>1.While the method of Chiang et al showed impres 226 sive performance improvements, its implementation is non-trivial, involving a complex cost function anda parallel architecture, and it has not yet been embraced by the MT community.</prevsent>
         </prevsection>
         <citsent citstr=" D11-1125 ">
            Indeed, the complexity of Chiang et als algorithm was one of the reasons cited for the development of PRO (Hopkins and May, 2011
            <papid>D11-1125</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>In this paper, we have sought to isolate the loss functions used in prior work like thatby Chiang et aland identify simple, generic optimization procedures for optimizing them.</nextsent>
            <nextsent>We offer RAMPION as an alternative to Chiang et als MIRA that is simpler to implement and achieves empirical success in experiments (4).</nextsent>
            <nextsent>3.4 Likelihood and Softened Losses.</nextsent>
            <nextsent>We can derive new loss functions from the above by converting any max?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>(log ? exp, where the set of elements under the summation is the same as under the max).</prevsent>
            <prevsent>For example, the softmax version of the perceptron loss is the well-known log loss (2, Ex.</prevsent>
         </prevsection>
         <citsent citstr=" P02-1038 ">
            2), the loss underlying the conditional likelihood training criterion whichis frequently used when a probabilistic interpretation of the learned model is desired, as in conditional random fields (Lafferty et al, 2001).Och and Ney (2002)
            <papid>P02-1038</papid>
            popularized the use of loglinear models for MT and initially sought to optimize log loss, but by using the min-cost translation on a k-best list as their surrogate, we argue that their loss was closer to the soft ramp loss obtained by softening the second max in loss ramp 2 in Eq.
         </citsent>
         <aftsection>
            <nextsent>7 (Fig.</nextsent>
            <nextsent>2).</nextsent>
            <nextsent>
               The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006
               <papid>P06-2101</papid>
               ; Zens et al, 2007
               <papid>D07-1055</papid>
               ; Cer, 2011
               <papid>W09-3528</papid>
               ).The softmax version of the latent variable perceptron loss, Eq.
            </nextsent>
            <nextsent>9 (Fig.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>7 (Fig.</prevsent>
            <prevsent>2).</prevsent>
         </prevsection>
         <citsent citstr=" P06-2101 ">
            The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006
            <papid>P06-2101</papid>
            ; Zens et al, 2007
            <papid>D07-1055</papid>
            ; Cer, 2011
            <papid>W09-3528</papid>
            ).The softmax version of the latent variable perceptron loss, Eq.
         </citsent>
         <aftsection>
            <nextsent>9 (Fig.</nextsent>
            <nextsent>2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al, 2004).</nextsent>
            <nextsent>
               Blunsom et al (2008)
               <papid>P08-1024</papid>
               and Blunsom and Osborne (2008)
               <papid>D08-1023</papid>
               actually did optimize latent log loss for MT,discarding training examples for which y(i) was unreachable by the model.
            </nextsent>
            <nextsent>Finally, we note that softening?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>7 (Fig.</prevsent>
            <prevsent>2).</prevsent>
         </prevsection>
         <citsent citstr=" D07-1055 ">
            The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006
            <papid>P06-2101</papid>
            ; Zens et al, 2007
            <papid>D07-1055</papid>
            ; Cer, 2011
            <papid>W09-3528</papid>
            ).The softmax version of the latent variable perceptron loss, Eq.
         </citsent>
         <aftsection>
            <nextsent>9 (Fig.</nextsent>
            <nextsent>2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al, 2004).</nextsent>
            <nextsent>
               Blunsom et al (2008)
               <papid>P08-1024</papid>
               and Blunsom and Osborne (2008)
               <papid>D08-1023</papid>
               actually did optimize latent log loss for MT,discarding training examples for which y(i) was unreachable by the model.
            </nextsent>
            <nextsent>Finally, we note that softening?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>7 (Fig.</prevsent>
            <prevsent>2).</prevsent>
         </prevsection>
         <citsent citstr=" W09-3528 ">
            The same is true for others who aimed to optimize log loss for MT (Smith and Eisner, 2006
            <papid>P06-2101</papid>
            ; Zens et al, 2007
            <papid>D07-1055</papid>
            ; Cer, 2011
            <papid>W09-3528</papid>
            ).The softmax version of the latent variable perceptron loss, Eq.
         </citsent>
         <aftsection>
            <nextsent>9 (Fig.</nextsent>
            <nextsent>2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al, 2004).</nextsent>
            <nextsent>
               Blunsom et al (2008)
               <papid>P08-1024</papid>
               and Blunsom and Osborne (2008)
               <papid>D08-1023</papid>
               actually did optimize latent log loss for MT,discarding training examples for which y(i) was unreachable by the model.
            </nextsent>
            <nextsent>Finally, we note that softening?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>9 (Fig.</prevsent>
            <prevsent>2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al, 2004).</prevsent>
         </prevsection>
         <citsent citstr=" P08-1024 ">
            Blunsom et al (2008)
            <papid>P08-1024</papid>
            and Blunsom and Osborne (2008)
            <papid>D08-1023</papid>
            actually did optimize latent log loss for MT,discarding training examples for which y(i) was unreachable by the model.
         </citsent>
         <aftsection>
            <nextsent>Finally, we note that softening?</nextsent>
            <nextsent>the ramp loss in Eq.</nextsent>
            <nextsent>6 (Fig.</nextsent>
            <nextsent>
               2) results in the Jensen risk bound from Gimpel and Smith (2010)
               <papid>N10-1112</papid>
               , which is a computationally-attractive upper bound on the Bayes risk.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>9 (Fig.</prevsent>
            <prevsent>2), is the latent log loss inherent in latent-variable CRFs (Quattoni et al, 2004).</prevsent>
         </prevsection>
         <citsent citstr=" D08-1023 ">
            Blunsom et al (2008)
            <papid>P08-1024</papid>
            and Blunsom and Osborne (2008)
            <papid>D08-1023</papid>
            actually did optimize latent log loss for MT,discarding training examples for which y(i) was unreachable by the model.
         </citsent>
         <aftsection>
            <nextsent>Finally, we note that softening?</nextsent>
            <nextsent>the ramp loss in Eq.</nextsent>
            <nextsent>6 (Fig.</nextsent>
            <nextsent>
               2) results in the Jensen risk bound from Gimpel and Smith (2010)
               <papid>N10-1112</papid>
               , which is a computationally-attractive upper bound on the Bayes risk.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Training Methods for MT.</section>
      <citcontext>
         <prevsection>
            <prevsent>the ramp loss in Eq.</prevsent>
            <prevsent>6 (Fig.</prevsent>
         </prevsection>
         <citsent citstr=" N10-1112 ">
            2) results in the Jensen risk bound from Gimpel and Smith (2010)
            <papid>N10-1112</papid>
            , which is a computationally-attractive upper bound on the Bayes risk.
         </citsent>
         <aftsection>
            <nextsent>The goal of our experiments is to compare RAMPION (Alg.</nextsent>
            <nextsent>1) to state-of-the-art methods for training MT systems.</nextsent>
            <nextsent>
               RAMPION minimizes loss ramp 3, which we found in preliminary experiments to work better than other loss functions tested.8System and Datasets We use the Moses phrase based MT system (Koehn et al, 2007
               <papid>P07-2045</papid>
               ) and consider Urdu english (UREN), Chinese english (ZHEN) translation, and Arabic english (AREN) translation.9 We trained a Moses system using default settings and features, except for setting the distortion limit to 10.
            </nextsent>
            <nextsent>
               Word alignment was performed using GIZA++ (Och and Ney, 2003
               <papid>J03-1002</papid>
               ) in both directions, the grow-diag-final-and heuristic was used to symmetrize the alignments, and a max phrase length of 7 was used for phrase extraction.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>The goal of our experiments is to compare RAMPION (Alg.</prevsent>
            <prevsent>1) to state-of-the-art methods for training MT systems.</prevsent>
         </prevsection>
         <citsent citstr=" P07-2045 ">
            RAMPION minimizes loss ramp 3, which we found in preliminary experiments to work better than other loss functions tested.8System and Datasets We use the Moses phrase based MT system (Koehn et al, 2007
            <papid>P07-2045</papid>
            ) and consider Urdu english (UREN), Chinese english (ZHEN) translation, and Arabic english (AREN) translation.9 We trained a Moses system using default settings and features, except for setting the distortion limit to 10.
         </citsent>
         <aftsection>
            <nextsent>
               Word alignment was performed using GIZA++ (Och and Ney, 2003
               <papid>J03-1002</papid>
               ) in both directions, the grow-diag-final-and heuristic was used to symmetrize the alignments, and a max phrase length of 7 was used for phrase extraction.
            </nextsent>
            <nextsent>
               We estimated 5-gram language models using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998
               <papid>P96-1041</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>1) to state-of-the-art methods for training MT systems.</prevsent>
            <prevsent>
               RAMPION minimizes loss ramp 3, which we found in preliminary experiments to work better than other loss functions tested.8System and Datasets We use the Moses phrase based MT system (Koehn et al, 2007
               <papid>P07-2045</papid>
               ) and consider Urdu english (UREN), Chinese english (ZHEN) translation, and Arabic english (AREN) translation.9 We trained a Moses system using default settings and features, except for setting the distortion limit to 10.
            </prevsent>
         </prevsection>
         <citsent citstr=" J03-1002 ">
            Word alignment was performed using GIZA++ (Och and Ney, 2003
            <papid>J03-1002</papid>
            ) in both directions, the grow-diag-final-and heuristic was used to symmetrize the alignments, and a max phrase length of 7 was used for phrase extraction.
         </citsent>
         <aftsection>
            <nextsent>
               We estimated 5-gram language models using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998
               <papid>P96-1041</papid>
               ).
            </nextsent>
            <nextsent>For each language pair, we used the English sideof the parallel text and 600M words of randomly selected sentences from the Gigaword v4 corpus (excluding NYT and LAT).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               RAMPION minimizes loss ramp 3, which we found in preliminary experiments to work better than other loss functions tested.8System and Datasets We use the Moses phrase based MT system (Koehn et al, 2007
               <papid>P07-2045</papid>
               ) and consider Urdu english (UREN), Chinese english (ZHEN) translation, and Arabic english (AREN) translation.9 We trained a Moses system using default settings and features, except for setting the distortion limit to 10.
            </prevsent>
            <prevsent>
               Word alignment was performed using GIZA++ (Och and Ney, 2003
               <papid>J03-1002</papid>
               ) in both directions, the grow-diag-final-and heuristic was used to symmetrize the alignments, and a max phrase length of 7 was used for phrase extraction.
            </prevsent>
         </prevsection>
         <citsent citstr=" P96-1041 ">
            We estimated 5-gram language models using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998
            <papid>P96-1041</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>For each language pair, we used the English sideof the parallel text and 600M words of randomly selected sentences from the Gigaword v4 corpus (excluding NYT and LAT).</nextsent>
            <nextsent>For UREN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words.</nextsent>
            <nextsent>We used half of the documents (882 sentences) from the MT08 test set for tuning.</nextsent>
            <nextsent>We used the remaining half for one test set (MT08??)</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>and MT09 as our other test set.</prevsent>
            <prevsent>For ZHEN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14).</prevsent>
         </prevsection>
         <citsent citstr=" W08-0336 ">
            We segmented the Chinese data using the Stanford Chinese segmenter (Chang et al, 2008
            <papid>W08-0336</papid>
            ) in CTB?
         </citsent>
         <aftsection>
            <nextsent>mode, giving us 7.9M Chinese words and 9.4M English words.</nextsent>
            <nextsent>We used MT03 for tuning and used MT02 and MT05 for testing.</nextsent>
            <nextsent>For AREN, we used data provided by the LDC 8We only present full results using loss ramp 3.</nextsent>
            <nextsent>We found that minimizing loss ramp 1 did poorly, resulting in single-digit BLEU scores, and that loss ramp 2 reached high BLEU scores on the tuning data but failed to generalize well.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>A forthcoming report will present these results, as well as experiments with additional loss functions, in detail.</prevsent>
            <prevsent>227 for the NIST evaluations, including 3.29M sentence pairs of UN data and 982k sentence pairs of non UN data.</prevsent>
         </prevsection>
         <citsent citstr=" P03-1051 ">
            The Arabic data was preprocessed usingan HMM segmenter that splits off attached prepositional phrases, personal pronouns, and the future marker (Lee et al, 2003
            <papid>P03-1051</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The common stylistic sentence-initial wa# (and ...)</nextsent>
            <nextsent>was removed from the training and test data.</nextsent>
            <nextsent>The resulting corpus contained 130M Arabic tokens and 130M English tokens.</nextsent>
            <nextsent>We used MT06 for tuning and three test sets: MT05, the MT08 newswire test set (MT08 NW?), and the MT08 weblog test set (MT08 WB?).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>The resulting corpus contained 130M Arabic tokens and 130M English tokens.</prevsent>
            <prevsent>We used MT06 for tuning and three test sets: MT05, the MT08 newswire test set (MT08 NW?), and the MT08 weblog test set (MT08 WB?).</prevsent>
         </prevsection>
         <citsent citstr=" C04-1072 ">
            For all languages we evaluated translation output using case-insensitive IBM BLEU (Papineni et al, 2001
            <papid>C04-1072</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Training Algorithms Our baselines are MERT and PRO as implemented in the Moses toolkit.10 PRO uses the hyperparameter settings from Hopkins and May (2011), including k-best lists of size 1500 and 25 training iterations.11 MERT uses k-best lists of size 100 and was run to convergence.</nextsent>
            <nextsent>For both MERT and PRO, previous iterations?</nextsent>
            <nextsent>k-best lists were merged in.</nextsent>
            <nextsent>For RAMPION, we used T = 20, T ? = 10, T ??</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>For RAMPION, we used T = 20, T ? = 10, T ??</prevsent>
            <prevsent>= 5, k = 500, ? = 0.0001, and C = 1.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1072 ">
            Our cost function is ?(1 ? BLEU+1(y,y?)) where BLEU+1(y,y?) returns the BLEU+1 score (Lin and Och, 2004
            <papid>C04-1072</papid>
            ) for reference y and hypothesis y?.
         </citsent>
         <aftsection>
            <nextsent>We used ? = 10.</nextsent>
            <nextsent>We used these same hyperparameter values for all experiments reported here and found them to perform well across other language pairs and systems.12 4.1 Results.</nextsent>
            <nextsent>Table 1 shows our results.</nextsent>
            <nextsent>MERT and PRO were run 3 times with differing random seeds and averages10The PRO algorithm samples pairs of translations from k best lists on each iteration and trains a binary classifier to rank pairs according to the cost function.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" N12-1023.xml">structured ramp loss minimization for machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Finally, we compare RAMPION and PRO with an extended feature set; MERT is excluded as it fails in such settings (Hopkins and May, 2011
               <papid>D11-1125</papid>
               ).We added count features for common monolingual and bilingual lexical patterns from the parallel corpus: the 1k most common bilingual word pairs from phrase extraction, 200 top unigrams, 1k top bigrams, 1k top tri grams, and 4k top trigger pairs extracted with the method of Rosenfeld (1996), ranked by mutual information.
            </prevsent>
            <prevsent>We integrated the features with our training procedure by using Moses to generate lattices instead of k-best lists.</prevsent>
         </prevsection>
         <citsent citstr=" J07-2003 ">
            We used cube pruning (Chiang, 2007
            <papid>J07-2003</papid>
            ) to incorporate the additional(potentially non-local) features while extracting kbest lists from the lattices to pass to the training al gorithms.14 Results are shown in Table 2.
         </citsent>
         <aftsection>
            <nextsent>We find that PRO finds much higher BLEU scores on the tuning data but fails to generalize, leading to poor performance on the held-out test sets.</nextsent>
            <nextsent>We suspect that incorporating regularization into training the binary classifier within PRO may mitigate this overfitting.</nextsent>
            <nextsent>RAMPION is more stable by contrast.</nextsent>
            <nextsent>This is a challenging learning task, as lexical features are prone to over 14In cube pruning, each nodes local n-best list had n = 100.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We focus on prepositional arguments, since non-prepositional ones are generally cores.</prevsent>
            <prevsent>The algorithm uses three measures based on different characterizations of thecore-adjunct distinction, and combines them using an ensemble method followed by self-training.The measures used are based on selectional preference, predicate-slot collocation and argument-slot collocation.</prevsent>
         </prevsection>
         <citsent citstr=" J05-1004 ">
            We evaluate against PropBank (Palmer et al, 2005
            <papid>J05-1004</papid>
            ), obtaining roughly 70% accuracy when evaluated on the prepositional arguments and more than 80% for the entire argument set.
         </citsent>
         <aftsection>
            <nextsent>These results are substantially better than those obtained by a non-trivial baseline.</nextsent>
            <nextsent>226 Section 2 discusses the core-adjunct distinction.</nextsent>
            <nextsent>Section 3 describes the algorithm.</nextsent>
            <nextsent>Sections 4 and 5 present our experimental setup and results.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Core-Adjunct in Previous Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>However, induction of the mapping in an unsupervised manner must be based on inherent core-adjunct properties.</prevsent>
            <prevsent>In addition, supervised models utilize supervised parsers and POS taggers, while the current state-of-the-art in unsupervised parsing and POS tagging is considerably worse than their supervised counterparts.This challenge has some resemblance to un 227 supervised detection of multiword expressions (MWEs).</prevsent>
         </prevsection>
         <citsent citstr=" E09-1086 ">
            An important MWE sub-class is that of phrasal verbs, which are also characterized by verb-preposition pairs (Li et al, 2003; Sporleder and Li, 2009
            <papid>E09-1086</papid>
            ) (see also (Boukobza and Rappoport, 2009
            <papid>D09-1049</papid>
            )).
         </citsent>
         <aftsection>
            <nextsent>Both tasks aim to determine semantic compositionality, which is a highly challenging task.</nextsent>
            <nextsent>Few works addressed unsupervised SRL-related tasks.</nextsent>
            <nextsent>
               The setup of (Grenager and Manning, 2006
               <papid>W06-1601</papid>
               ), who presented a Bayesian Network model for argument classification, is perhaps closest to ours.
            </nextsent>
            <nextsent>Their work relied on a supervised parserand a rule-based argument identification (both during training and testing).</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Core-Adjunct in Previous Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>However, induction of the mapping in an unsupervised manner must be based on inherent core-adjunct properties.</prevsent>
            <prevsent>In addition, supervised models utilize supervised parsers and POS taggers, while the current state-of-the-art in unsupervised parsing and POS tagging is considerably worse than their supervised counterparts.This challenge has some resemblance to un 227 supervised detection of multiword expressions (MWEs).</prevsent>
         </prevsection>
         <citsent citstr=" D09-1049 ">
            An important MWE sub-class is that of phrasal verbs, which are also characterized by verb-preposition pairs (Li et al, 2003; Sporleder and Li, 2009
            <papid>E09-1086</papid>
            ) (see also (Boukobza and Rappoport, 2009
            <papid>D09-1049</papid>
            )).
         </citsent>
         <aftsection>
            <nextsent>Both tasks aim to determine semantic compositionality, which is a highly challenging task.</nextsent>
            <nextsent>Few works addressed unsupervised SRL-related tasks.</nextsent>
            <nextsent>
               The setup of (Grenager and Manning, 2006
               <papid>W06-1601</papid>
               ), who presented a Bayesian Network model for argument classification, is perhaps closest to ours.
            </nextsent>
            <nextsent>Their work relied on a supervised parserand a rule-based argument identification (both during training and testing).</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Core-Adjunct in Previous Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Both tasks aim to determine semantic compositionality, which is a highly challenging task.</prevsent>
            <prevsent>Few works addressed unsupervised SRL-related tasks.</prevsent>
         </prevsection>
         <citsent citstr=" W06-1601 ">
            The setup of (Grenager and Manning, 2006
            <papid>W06-1601</papid>
            ), who presented a Bayesian Network model for argument classification, is perhaps closest to ours.
         </citsent>
         <aftsection>
            <nextsent>Their work relied on a supervised parserand a rule-based argument identification (both during training and testing).</nextsent>
            <nextsent>
               Swier and Stevenson (2004
               <papid>W04-3213</papid>
               , 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al, 2000) verb lexicon, in addition to supervised parses.
            </nextsent>
            <nextsent>
               Finally,Abend et al (2009)
               <papid>P09-1004</papid>
               tackled the argument identification task alone and did not perform argument classification of any sort.PP attachment.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Core-Adjunct in Previous Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The setup of (Grenager and Manning, 2006
               <papid>W06-1601</papid>
               ), who presented a Bayesian Network model for argument classification, is perhaps closest to ours.
            </prevsent>
            <prevsent>Their work relied on a supervised parserand a rule-based argument identification (both during training and testing).</prevsent>
         </prevsection>
         <citsent citstr=" W04-3213 ">
            Swier and Stevenson (2004
            <papid>W04-3213</papid>
            , 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al, 2000) verb lexicon, in addition to supervised parses.
         </citsent>
         <aftsection>
            <nextsent>
               Finally,Abend et al (2009)
               <papid>P09-1004</papid>
               tackled the argument identification task alone and did not perform argument classification of any sort.PP attachment.
            </nextsent>
            <nextsent>PP attachment is the task of determining whether a prepositional phrase which immediately follows a noun phrase attaches to the latter or to the preceding verb.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Core-Adjunct in Previous Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Their work relied on a supervised parserand a rule-based argument identification (both during training and testing).</prevsent>
            <prevsent>
               Swier and Stevenson (2004
               <papid>W04-3213</papid>
               , 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al, 2000) verb lexicon, in addition to supervised parses.
            </prevsent>
         </prevsection>
         <citsent citstr=" P09-1004 ">
            Finally,Abend et al (2009)
            <papid>P09-1004</papid>
            tackled the argument identification task alone and did not perform argument classification of any sort.PP attachment.
         </citsent>
         <aftsection>
            <nextsent>PP attachment is the task of determining whether a prepositional phrase which immediately follows a noun phrase attaches to the latter or to the preceding verb.</nextsent>
            <nextsent>This tasks relation to the core-adjunct distinction was addressed in several works.</nextsent>
            <nextsent>
               For instance, the results of (Hindle and Rooth, 1993
               <papid>H90-1052</papid>
               ) indicate that their PP attachment system works better for cores than for adjuncts.Merlo and Esteve Ferrer (2006) suggest a system that jointly tackles the PP attachment and the core-adjunct distinction tasks.
            </nextsent>
            <nextsent>Unlike in this work,their classifier requires extensive supervision including WordNet, language-specific features and a supervised parser.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Core-Adjunct in Previous Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>PP attachment is the task of determining whether a prepositional phrase which immediately follows a noun phrase attaches to the latter or to the preceding verb.</prevsent>
            <prevsent>This tasks relation to the core-adjunct distinction was addressed in several works.</prevsent>
         </prevsection>
         <citsent citstr=" H90-1052 ">
            For instance, the results of (Hindle and Rooth, 1993
            <papid>H90-1052</papid>
            ) indicate that their PP attachment system works better for cores than for adjuncts.Merlo and Esteve Ferrer (2006) suggest a system that jointly tackles the PP attachment and the core-adjunct distinction tasks.
         </citsent>
         <aftsection>
            <nextsent>Unlike in this work,their classifier requires extensive supervision including WordNet, language-specific features and a supervised parser.</nextsent>
            <nextsent>Their features are generally motivated by common linguistic considerations.Features found adaptable to a completely unsupervised scenario are used in this work as well.</nextsent>
            <nextsent>Syntactic Parsing.</nextsent>
            <nextsent>The core-adjunct distinction is included in many syntactic annotation schemes.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Core-Adjunct in Previous Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>The core-adjunct distinction is included in many syntactic annotation schemes.</prevsent>
            <prevsent>Although the Penn Treebank does not explicitly annotate adjuncts and cores, a few works suggested mapping its annotation (including function tags) to core-adjunct labels.</prevsent>
         </prevsection>
         <citsent citstr=" J03-4003 ">
            Such a mapping was presented in (Collins, 1999
            <papid>J03-4003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>In his Model2, Collins modifies his parser to provide a core adjunct prediction, thereby improving its performance.</nextsent>
            <nextsent>The Combinatory Categorial Grammar (CCG) formulation models the core-adjunct distinction explicitly.</nextsent>
            <nextsent>
               Therefore, any CCG parser can be used as a core-adjunct classifier (Hockenmaier, 2003
               <papid>P02-1043</papid>
               ).Subcategorization Acquisition.
            </nextsent>
            <nextsent>This task specifies for each predicate the number, type and orderof obligatory arguments.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Core-Adjunct in Previous Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>In his Model2, Collins modifies his parser to provide a core adjunct prediction, thereby improving its performance.</prevsent>
            <prevsent>The Combinatory Categorial Grammar (CCG) formulation models the core-adjunct distinction explicitly.</prevsent>
         </prevsection>
         <citsent citstr=" P02-1043 ">
            Therefore, any CCG parser can be used as a core-adjunct classifier (Hockenmaier, 2003
            <papid>P02-1043</papid>
            ).Subcategorization Acquisition.
         </citsent>
         <aftsection>
            <nextsent>This task specifies for each predicate the number, type and orderof obligatory arguments.</nextsent>
            <nextsent>Determining the allowable subcategorization frames for a given predicate necessarily involves separating its cores fromits allowable adjuncts (which are not framed).</nextsent>
            <nextsent>
               Notable works in the field include (Briscoe and Car roll, 1997; Sarkar and Zeman, 2000
               <papid>C00-2100</papid>
               ; Korhonen, 2002).
            </nextsent>
            <nextsent>All these works used a parsed corpus inorder to collect, for each predicate, a set of hypothesized subcategorization frames, to be filtered by hypothesis testing methods.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Core-Adjunct in Previous Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>This task specifies for each predicate the number, type and orderof obligatory arguments.</prevsent>
            <prevsent>Determining the allowable subcategorization frames for a given predicate necessarily involves separating its cores fromits allowable adjuncts (which are not framed).</prevsent>
         </prevsection>
         <citsent citstr=" C00-2100 ">
            Notable works in the field include (Briscoe and Car roll, 1997; Sarkar and Zeman, 2000
            <papid>C00-2100</papid>
            ; Korhonen, 2002).
         </citsent>
         <aftsection>
            <nextsent>All these works used a parsed corpus inorder to collect, for each predicate, a set of hypothesized subcategorization frames, to be filtered by hypothesis testing methods.</nextsent>
            <nextsent>This line of work differs from ours in a fewaspects.</nextsent>
            <nextsent>First, all works use manual or supervised syntactic annotations, usually including a POS tagger.</nextsent>
            <nextsent>Second, the common approach to thetask focuses on syntax and tries to identify the entire frame, rather than to tag each argument separately.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Core-Adjunct in Previous Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Consequently, the common evaluation focuses on the quality of the allowable frames acquired for each verb type, and not on the classification of specific arguments in a given corpus.</prevsent>
            <prevsent>Such a token level evaluation was conducted in a few works (Briscoe and Carroll, 1997; Sarkarand Zeman, 2000), but often with a small number of verbs or a small number of frames.</prevsent>
         </prevsection>
         <citsent citstr=" W10-2911 ">
            A discussion of the differences between type and token level evaluation can be found in (Reichart et al, 2010
            <papid>W10-2911</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The core-adjunct distinction task was tackled inthe context of child language acquisition.</nextsent>
            <nextsent>Villavicencio (2002) developed a classifier based on preposition selection and frequency information for modeling the distinction for locative prepositional phrases.</nextsent>
            <nextsent>Her approach is not entirely corpus based, as it assumes the input sentences are given in a basic logical form.</nextsent>
            <nextsent>The study of prepositions is a vibrant research area in NLP.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Algorithm.</section>
      <citcontext>
         <prevsection>
            <prevsent>3.1 Overview.</prevsent>
            <prevsent>Our algorithm utilizes statistics based on the(predicate, slot, argument head) (PSH) joint distribution (a slot is represented by its preposition).</prevsent>
         </prevsection>
         <citsent citstr=" E03-1009 ">
            To estimate this joint distribution, PSH samples are extracted from the training corpus using unsupervised POS taggers (Clark, 2003
            <papid>E03-1009</papid>
            ; Abend et al, 2010) and an unsupervised parser (Seginer, 2007
            <papid>P07-1049</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>As current performance of unsupervised parsers for long sentences is low, we use only short sentences (up to 10 words, excluding punctuation).</nextsent>
            <nextsent>The length of test sentences is not bounded.</nextsent>
            <nextsent>Our results will show that the training data accounts well for the argument realization phenomena inthe test set, despite the length bound on its sentences.</nextsent>
            <nextsent>The sample extraction process is detailed in Section 3.2.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Algorithm.</section>
      <citcontext>
         <prevsection>
            <prevsent>3.1 Overview.</prevsent>
            <prevsent>Our algorithm utilizes statistics based on the(predicate, slot, argument head) (PSH) joint distribution (a slot is represented by its preposition).</prevsent>
         </prevsection>
         <citsent citstr=" P07-1049 ">
            To estimate this joint distribution, PSH samples are extracted from the training corpus using unsupervised POS taggers (Clark, 2003
            <papid>E03-1009</papid>
            ; Abend et al, 2010) and an unsupervised parser (Seginer, 2007
            <papid>P07-1049</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>As current performance of unsupervised parsers for long sentences is low, we use only short sentences (up to 10 words, excluding punctuation).</nextsent>
            <nextsent>The length of test sentences is not bounded.</nextsent>
            <nextsent>Our results will show that the training data accounts well for the argument realization phenomena inthe test set, despite the length bound on its sentences.</nextsent>
            <nextsent>The sample extraction process is detailed in Section 3.2.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Algorithm.</section>
      <citcontext>
         <prevsection>
            <prevsent>The statistical measures used by our classifier are based on the (predicate, slot, argument head) (PSH) joint distribution.</prevsent>
            <prevsent>This section details the process of extracting samples from this joint distribution given a raw text corpus.</prevsent>
         </prevsection>
         <citsent citstr=" P07-1049 ">
            We start by parsing the corpus using the Seginer parser (Seginer, 2007
            <papid>P07-1049</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>This parser is unique in its ability to induce a bracketing (unlabeled parsing) from raw text (without even using POS tags) with strong results.</nextsent>
            <nextsent>Its high speed (thousands of words per second) allows us to use millions of sentences, a prohibitive number for other parsers.</nextsent>
            <nextsent>
               We continue by tagging the corpus using Clarks unsupervised POS tagger (Clark, 2003
               <papid>E03-1009</papid>
               ) and the unsupervised Prototype Tagger (Abend etal., 2010)2.
            </nextsent>
            <nextsent>The classes corresponding to prepositions and to verbs are manually selected from the induced clusters3.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Algorithm.</section>
      <citcontext>
         <prevsection>
            <prevsent>This parser is unique in its ability to induce a bracketing (unlabeled parsing) from raw text (without even using POS tags) with strong results.</prevsent>
            <prevsent>Its high speed (thousands of words per second) allows us to use millions of sentences, a prohibitive number for other parsers.</prevsent>
         </prevsection>
         <citsent citstr=" E03-1009 ">
            We continue by tagging the corpus using Clarks unsupervised POS tagger (Clark, 2003
            <papid>E03-1009</papid>
            ) and the unsupervised Prototype Tagger (Abend etal., 2010)2.
         </citsent>
         <aftsection>
            <nextsent>The classes corresponding to prepositions and to verbs are manually selected from the induced clusters3.</nextsent>
            <nextsent>A preposition is defined to be any word which is the first word of an argument and belongs to a prepositions cluster.</nextsent>
            <nextsent>A verb is any word belonging to a verb cluster.</nextsent>
            <nextsent>This manual selection requires only a minute, since the number of classes is very small (34 in our experiments).In addition, knowing what is considered a preposition is part of the task definition itself.Argument identification is hard even for supervised models and is considerably more so for unsupervised ones (Abend et al, 2009).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Algorithm.</section>
      <citcontext>
         <prevsection>
            <prevsent>We therefore confine ourselves to sentences of length not greater than 10 (excluding punctuation) which contain a single verb.</prevsent>
            <prevsent>A sequence of words willbe marked as an argument of the verb if it is a constituent that does not contain the verb (according to the unsupervised parse tree), whose parent is an ancestor of the verb.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3212 ">
            This follows the pruning heuristic of (Xue and Palmer, 2004
            <papid>W04-3212</papid>
            ) often used by SRL algorithms.
         </citsent>
         <aftsection>
            <nextsent>The corpus is now tagged using an unsupervised POS tagger.</nextsent>
            <nextsent>Since the sentences in question are short, we consider every word which does not be long to a closed class cluster as a head word (an argument can have several head words).</nextsent>
            <nextsent>A closed class is a class of function words with relatively few word types, each of which is very frequent.Typical examples include deter miners, prepositions and conjunctions.</nextsent>
            <nextsent>A class which is not closed is open.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Algorithm.</section>
      <citcontext>
         <prevsection>
            <prevsent>Since the semantics of cores is more predicate dependent than the semantics of adjuncts, we expect arguments for which the predicate has a strong preference (in a specific slot) to be cores.Selectional preference induction is a well established task in NLP.</prevsent>
            <prevsent>It aims to quantify the likelihood that a certain argument appears in a certain slot of a predicate.</prevsent>
         </prevsection>
         <citsent citstr=" J98-2002 ">
            Several methods have been suggested (Resnik, 1996; Li and Abe, 1998
            <papid>J98-2002</papid>
            ; Schulte im Walde et al, 2008
            <papid>P08-1057</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               We use the paradigm of (Erk, 2007
               <papid>P07-1028</papid>
               ).
            </nextsent>
            <nextsent>For a given predicate slot pair (p, s), we define its preference to the argument head h to be: SP (p, s, h) = ? h?Heads Pr(h?|p, s) ? sim(h, h?)</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Algorithm.</section>
      <citcontext>
         <prevsection>
            <prevsent>Since the semantics of cores is more predicate dependent than the semantics of adjuncts, we expect arguments for which the predicate has a strong preference (in a specific slot) to be cores.Selectional preference induction is a well established task in NLP.</prevsent>
            <prevsent>It aims to quantify the likelihood that a certain argument appears in a certain slot of a predicate.</prevsent>
         </prevsection>
         <citsent citstr=" P08-1057 ">
            Several methods have been suggested (Resnik, 1996; Li and Abe, 1998
            <papid>J98-2002</papid>
            ; Schulte im Walde et al, 2008
            <papid>P08-1057</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               We use the paradigm of (Erk, 2007
               <papid>P07-1028</papid>
               ).
            </nextsent>
            <nextsent>For a given predicate slot pair (p, s), we define its preference to the argument head h to be: SP (p, s, h) = ? h?Heads Pr(h?|p, s) ? sim(h, h?)</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Algorithm.</section>
      <citcontext>
         <prevsection>
            <prevsent>It aims to quantify the likelihood that a certain argument appears in a certain slot of a predicate.</prevsent>
            <prevsent>
               Several methods have been suggested (Resnik, 1996; Li and Abe, 1998
               <papid>J98-2002</papid>
               ; Schulte im Walde et al, 2008
               <papid>P08-1057</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P07-1028 ">
            We use the paradigm of (Erk, 2007
            <papid>P07-1028</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>For a given predicate slot pair (p, s), we define its preference to the argument head h to be: SP (p, s, h) = ? h?Heads Pr(h?|p, s) ? sim(h, h?)</nextsent>
            <nextsent>Pr(h|p, s) = N(p, s, h)hN(p, s, h?)sim(h, h?)</nextsent>
            <nextsent>is a similarity measure between argument heads.</nextsent>
            <nextsent>Heads is the set of all head words.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Experimental Setup.</section>
      <citcontext>
         <prevsection>
            <prevsent>were considered predicates.</prevsent>
            <prevsent>This scenario decouples the accuracy of the algorithm from the quality of the unsupervised POS tagging.In the Fully Unsupervised?</prevsent>
         </prevsection>
         <citsent citstr=" E03-1009 ">
            scenario, prepositions and verbs were identified using Clarks tagger (Clark, 2003
            <papid>E03-1009</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>It was asked to produce a tagging into 34 classes.</nextsent>
            <nextsent>The classes corresponding to prepositions and to verbs were manually identified.</nextsent>
            <nextsent>Prepositions in the test set were detected with 84.2% precision and 91.6% recall.</nextsent>
            <nextsent>The prediction of whether a word belongs to an open class or a closed was based on the output of the Prototype tagger (Abend et al, 2010).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Experimental Setup.</section>
      <citcontext>
         <prevsection>
            <prevsent>3.</prevsent>
            <prevsent>Thesaurus SP ? a selectional preference mea-.</prevsent>
         </prevsection>
         <citsent citstr=" P07-1028 ">
            sure which follows the paradigm of (Erk, 2007
            <papid>P07-1028</papid>
            ) (Section 3.3) and defines the similarity between two heads to be the Jaccard affinity between their two entries in Lins automatically compiled thesaurus (Lin, 1998
            <papid>P98-2127</papid>
            )10.
         </citsent>
         <aftsection>
            <nextsent>4.</nextsent>
            <nextsent>Pr(slot|predicate) ? an alternative to the used.</nextsent>
            <nextsent>predicate-slot collocation measure.</nextsent>
            <nextsent>argument-slot collocation measure.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1024.xml">fully unsupervised core adjunct argument classification</title>
      <section>Experimental Setup.</section>
      <citcontext>
         <prevsection>
            <prevsent>3.</prevsent>
            <prevsent>Thesaurus SP ? a selectional preference mea-.</prevsent>
         </prevsection>
         <citsent citstr=" P98-2127 ">
            sure which follows the paradigm of (Erk, 2007
            <papid>P07-1028</papid>
            ) (Section 3.3) and defines the similarity between two heads to be the Jaccard affinity between their two entries in Lins automatically compiled thesaurus (Lin, 1998
            <papid>P98-2127</papid>
            )10.
         </citsent>
         <aftsection>
            <nextsent>4.</nextsent>
            <nextsent>Pr(slot|predicate) ? an alternative to the used.</nextsent>
            <nextsent>predicate-slot collocation measure.</nextsent>
            <nextsent>argument-slot collocation measure.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>Intuition and Motivating Examples.</section>
      <citcontext>
         <prevsection>
            <prevsent>(NP (ADJP (NP (JJ Libyan) (NN ruler)) (JJ Mu)) (??</prevsent>
            <prevsent>(NN ammar) (NNS al-Qaddafi))Above, a backward quote in an Arabic name confuses the Stanford parser.2 Yet mark-up lines up with the broken noun phrase, signals cohesion, and moreover sheds light on the internal structure of a compound.</prevsent>
         </prevsection>
         <citsent citstr=" P07-1031 ">
            As Vadas and Curran (2007)
            <papid>P07-1031</papid>
            point out, such details are frequently omitted even from manually compiled tree-banks that err on the side of flat annotations of base-NPs.
         </citsent>
         <aftsection>
            <nextsent>Admittedly, not all boundaries between HTML tags and syntactic constituents match up nicely: ..., but [S [NP the &lt;a&gt;&lt;i&gt;Toronto Star&lt;/i&gt;][VP reports [NP this][PP in the softest possible way]&lt;/a&gt;,[S stating only that ...]]] Combining parsing with mark-up may not be straight-forward, but there is hope: even above, 1Even when (American) grammar schools lived up to their name, they only taught dependencies.</nextsent>
            <nextsent>This was back in the days before constituent grammars were invented.</nextsent>
            <nextsent>2http://nlp.stanford.edu:8080/parser/ one of each nested tags boundaries aligns; and toronto Stars neglected determiner could be forgiven, certainly within a dependency formulation.</nextsent>
            <nextsent>Our idea is to implement the DMV (Klein and manning, 2004) ? a standard unsupervised grammar inducer.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>A High-Level Outline of Our Approach</section>
      <citcontext>
         <prevsection>
            <prevsent>Our idea is to implement the DMV (Klein and manning, 2004) ? a standard unsupervised grammar inducer.</prevsent>
            <prevsent>But instead of learning the unannotated test set, we train with text that contains web mark-up, using various ways of converting HTML into parsing constraints.</prevsent>
         </prevsection>
         <citsent citstr=" J93-2004 ">
            We still test on WSJ (Marcus et al, 1993
            <papid>J93-2004</papid>
            ), in the standard way, and also check generalization against a hidden dataset ? the Brown corpus (Francis and Kucera, 1979).
         </citsent>
         <aftsection>
            <nextsent>Our parsing constraints come from a blog ? a new corpus we created, the web and news (see Table 1 for corporas sentence and token counts).</nextsent>
            <nextsent>To facilitate future work, we make the final models and our manually-constructed blog data publicly available.3 Although we are unable to share larger-scale resources, our main results should be reproducible, as both linguistic analysis and our best model rely exclusively on the blog.</nextsent>
            <nextsent>Corpus Sentences POS Tokens WSJ?</nextsent>
            <nextsent>49,208 1,028,347 Section 23 2,353 48,201 WSJ45 48,418 986,830 WSJ15 15,922 163,715 Brown100 24,208 391,796 BLOGp 57,809 1,136,659 BLOGt45 56,191 1,048,404 BLOGt15 23,214 212,872 NEWS45 2,263,563,078 32,119,123,561 NEWS15 1,433,779,438 11,786,164,503 WEB45 8,903,458,234 87,269,385,640 WEB15 7,488,669,239 55,014,582,024 Table 1: Sizes of corpora derived from WSJ and Brown, as well as those we collected from the web.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>DataSets for Evaluation and Training.</section>
      <citcontext>
         <prevsection>
            <prevsent>49,208 1,028,347 Section 23 2,353 48,201 WSJ45 48,418 986,830 WSJ15 15,922 163,715 Brown100 24,208 391,796 BLOGp 57,809 1,136,659 BLOGt45 56,191 1,048,404 BLOGt15 23,214 212,872 NEWS45 2,263,563,078 32,119,123,561 NEWS15 1,433,779,438 11,786,164,503 WEB45 8,903,458,234 87,269,385,640 WEB15 7,488,669,239 55,014,582,024 Table 1: Sizes of corpora derived from WSJ and Brown, as well as those we collected from the web.</prevsent>
            <prevsent>The appeal of unsupervised parsing lies in its ability to learn from surface text alone; but (intrinsic)evaluation still requires parsed sentences.</prevsent>
         </prevsection>
         <citsent citstr=" P04-1061 ">
            Following Klein and Manning (2004)
            <papid>P04-1061</papid>
            , we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic head-percolation?
         </citsent>
         <aftsection>
            <nextsent>
               rules (Collins, 1999
               <papid>J03-4003</papid>
               ) to convert the rest, as is standard practice.
            </nextsent>
            <nextsent>3http://cs.stanford.edu/valentin/ 1279 Length Marked POS Bracketings Length Marked POS Bracketings Cutoff Sentences Tokens All Multi-Token Cutoff Sentences Tokens All Multi-Token 0 6,047 1,136,659 7,731 6,015 8 485 14,528 710 684 1 of 57,809 149,483 7,731 6,015 9 333 10,484 499 479 2 4,934 124,527 6,482 6,015 10 245 7,887 365 352 3 3,295 85,423 4,476 4,212 15 42 1,519 65 63 4 2,103 56,390 2,952 2,789 20 13 466 20 20 6 960 27,285 1,365 1,302 30 3 136 6 6 7 692 19,894 992 952 40 0 0 0 0 Table 2: Counts of sentences, tokens and (unique) bracketings for BLOGp, restricted to only those sentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence).</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>DataSets for Evaluation and Training.</section>
      <citcontext>
         <prevsection>
            <prevsent>The appeal of unsupervised parsing lies in its ability to learn from surface text alone; but (intrinsic)evaluation still requires parsed sentences.</prevsent>
            <prevsent>
               Following Klein and Manning (2004)
               <papid>P04-1061</papid>
               , we begin with reference constituent parses and compare against deterministically derived dependencies: after pruning out all empty sub trees, punctuation and terminals (tagged # and $) not pronounced where they appear, we drop all sentences with more than a prescribed number of tokens remaining and use automatic head-percolation?
            </prevsent>
         </prevsection>
         <citsent citstr=" J03-4003 ">
            rules (Collins, 1999
            <papid>J03-4003</papid>
            ) to convert the rest, as is standard practice.
         </citsent>
         <aftsection>
            <nextsent>3http://cs.stanford.edu/valentin/ 1279 Length Marked POS Bracketings Length Marked POS Bracketings Cutoff Sentences Tokens All Multi-Token Cutoff Sentences Tokens All Multi-Token 0 6,047 1,136,659 7,731 6,015 8 485 14,528 710 684 1 of 57,809 149,483 7,731 6,015 9 333 10,484 499 479 2 4,934 124,527 6,482 6,015 10 245 7,887 365 352 3 3,295 85,423 4,476 4,212 15 42 1,519 65 63 4 2,103 56,390 2,952 2,789 20 13 466 20 20 6 960 27,285 1,365 1,302 30 3 136 6 6 7 692 19,894 992 952 40 0 0 0 0 Table 2: Counts of sentences, tokens and (unique) bracketings for BLOGp, restricted to only those sentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence).</nextsent>
            <nextsent>
               Our primary reference sets are derived from thePenn English Treebanks Wall Street Journal portion (Marcus et al, 1993
               <papid>J93-2004</papid>
               ): WSJ45 (sentences with fewer than 46 tokens) and Section 23 of WSJ?
            </nextsent>
            <nextsent>(all sentence lengths).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>DataSets for Evaluation and Training.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               rules (Collins, 1999
               <papid>J03-4003</papid>
               ) to convert the rest, as is standard practice.
            </prevsent>
            <prevsent>3http://cs.stanford.edu/valentin/ 1279 Length Marked POS Bracketings Length Marked POS Bracketings Cutoff Sentences Tokens All Multi-Token Cutoff Sentences Tokens All Multi-Token 0 6,047 1,136,659 7,731 6,015 8 485 14,528 710 684 1 of 57,809 149,483 7,731 6,015 9 333 10,484 499 479 2 4,934 124,527 6,482 6,015 10 245 7,887 365 352 3 3,295 85,423 4,476 4,212 15 42 1,519 65 63 4 2,103 56,390 2,952 2,789 20 13 466 20 20 6 960 27,285 1,365 1,302 30 3 136 6 6 7 692 19,894 992 952 40 0 0 0 0 Table 2: Counts of sentences, tokens and (unique) bracketings for BLOGp, restricted to only those sentences having at least one bracketing no shorter than the length cutoff (but shorter than the sentence).</prevsent>
         </prevsection>
         <citsent citstr=" J93-2004 ">
            Our primary reference sets are derived from thePenn English Treebanks Wall Street Journal portion (Marcus et al, 1993
            <papid>J93-2004</papid>
            ): WSJ45 (sentences with fewer than 46 tokens) and Section 23 of WSJ?
         </citsent>
         <aftsection>
            <nextsent>(all sentence lengths).</nextsent>
            <nextsent>We also evaluate on Brown100, similarly derived from the parsed portion of the Brown corpus (Francis and Kucera, 1979).</nextsent>
            <nextsent>Whilewe use WSJ45 and WSJ15 to train baseline models, the bulk of our experiments is with web data.</nextsent>
            <nextsent>4.1 A News-Style Blog: Daniel Pipes Since there was no corpus overlaying syntactic structure with mark-up, we began constructing anew one by downloading articles4 from a news style blog.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>DataSets for Evaluation and Training.</section>
      <citcontext>
         <prevsection>
            <prevsent>id=.</prevsent>
            <prevsent>tags left covering entire sentences.</prevsent>
         </prevsection>
         <citsent citstr=" W00-1308 ">
            We finalized two versions of the data: BLOGt, tagged with the Stanford tagger (Toutanova and Manning, 2000
            <papid>W00-1308</papid>
            ; Toutanova et al, 2003
            <papid>N03-1033</papid>
            ),6 and BLOGp, parsed with Charniaks parser (Charniak,2001; Charniak and Johnson, 2005
            <papid>P05-1022</papid>
            ).7 The reason for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (andnon-standard AUX[G]) POS sequences from interfering with our (otherwise unsupervised) training.8 4.2 Scaled up Quantity: The (English) Web.
         </citsent>
         <aftsection>
            <nextsent>We built a large (see Table 1) but messy dataset, WEB ? English-looking web-pages, pre-crawled by a search engine.</nextsent>
            <nextsent>To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system.</nextsent>
            <nextsent>
               We kept only sentence-like runs of words (satisfying punctuation and capitalization constraints), POS-tagged with TnT (Brants, 2000
               <papid>A00-1031</papid>
               ).
            </nextsent>
            <nextsent>4.3 Scaled up Quality: (English) Web News.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>DataSets for Evaluation and Training.</section>
      <citcontext>
         <prevsection>
            <prevsent>id=.</prevsent>
            <prevsent>tags left covering entire sentences.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1033 ">
            We finalized two versions of the data: BLOGt, tagged with the Stanford tagger (Toutanova and Manning, 2000
            <papid>W00-1308</papid>
            ; Toutanova et al, 2003
            <papid>N03-1033</papid>
            ),6 and BLOGp, parsed with Charniaks parser (Charniak,2001; Charniak and Johnson, 2005
            <papid>P05-1022</papid>
            ).7 The reason for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (andnon-standard AUX[G]) POS sequences from interfering with our (otherwise unsupervised) training.8 4.2 Scaled up Quantity: The (English) Web.
         </citsent>
         <aftsection>
            <nextsent>We built a large (see Table 1) but messy dataset, WEB ? English-looking web-pages, pre-crawled by a search engine.</nextsent>
            <nextsent>To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system.</nextsent>
            <nextsent>
               We kept only sentence-like runs of words (satisfying punctuation and capitalization constraints), POS-tagged with TnT (Brants, 2000
               <papid>A00-1031</papid>
               ).
            </nextsent>
            <nextsent>4.3 Scaled up Quality: (English) Web News.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>DataSets for Evaluation and Training.</section>
      <citcontext>
         <prevsection>
            <prevsent>id=.</prevsent>
            <prevsent>tags left covering entire sentences.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1022 ">
            We finalized two versions of the data: BLOGt, tagged with the Stanford tagger (Toutanova and Manning, 2000
            <papid>W00-1308</papid>
            ; Toutanova et al, 2003
            <papid>N03-1033</papid>
            ),6 and BLOGp, parsed with Charniaks parser (Charniak,2001; Charniak and Johnson, 2005
            <papid>P05-1022</papid>
            ).7 The reason for this dichotomy was to use state-of-the-art parses to analyze the relationship between syntax and mark-up, yet to prevent jointly tagged (andnon-standard AUX[G]) POS sequences from interfering with our (otherwise unsupervised) training.8 4.2 Scaled up Quantity: The (English) Web.
         </citsent>
         <aftsection>
            <nextsent>We built a large (see Table 1) but messy dataset, WEB ? English-looking web-pages, pre-crawled by a search engine.</nextsent>
            <nextsent>To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system.</nextsent>
            <nextsent>
               We kept only sentence-like runs of words (satisfying punctuation and capitalization constraints), POS-tagged with TnT (Brants, 2000
               <papid>A00-1031</papid>
               ).
            </nextsent>
            <nextsent>4.3 Scaled up Quality: (English) Web News.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>DataSets for Evaluation and Training.</section>
      <citcontext>
         <prevsection>
            <prevsent>We built a large (see Table 1) but messy dataset, WEB ? English-looking web-pages, pre-crawled by a search engine.</prevsent>
            <prevsent>To avoid machine-generated spam, we excluded low quality sites flagged by the indexing system.</prevsent>
         </prevsection>
         <citsent citstr=" A00-1031 ">
            We kept only sentence-like runs of words (satisfying punctuation and capitalization constraints), POS-tagged with TnT (Brants, 2000
            <papid>A00-1031</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>4.3 Scaled up Quality: (English) Web News.</nextsent>
            <nextsent>In an effort to trade quantity for quality, we constructed a smaller, potentially cleaner dataset, NEWS.</nextsent>
            <nextsent>We reckoned editorialized content would lead to fewer extracted non-sentences.</nextsent>
            <nextsent>Perhaps surprisingly, NEWS is less than an order of magnitude smaller than WEB (see Table 1); in part, this is due to less aggressive filtering ? we trust sites approved by the human editors at Google News.9 In all other respects, our pre-processing of NEWS pages was identical to our handling of WEB data.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>Linguistic Analysis of Mark-Up.</section>
      <citcontext>
         <prevsection>
            <prevsent>6http://nlp.stanford.edu/software/ stanford-postagger-2008-09-28.tar.gz 7ftp://ftp.cs.brown.edu/pub/nlparser/ parser05Aug16.tar.gz 8However, since many taggers are themselves trained on manually parsed corpora, such as WSJ, no parser that relies on external POS tags could be considered truly unsupervised; for a fully unsupervised example, see Seginers (2007) CCL parser, available at http://www.seggu.net/ccl/ 9http://news.google.com/ 1280</prevsent>
            <prevsent>Is there a connection between mark-up and syntactic structure?</prevsent>
         </prevsection>
         <citsent citstr=" D08-1107 ">
            Previous work (Barr et al, 2008
            <papid>D08-1107</papid>
            )has only examined search engine queries, showing that they consist predominantly of short nounphrases.
         </citsent>
         <aftsection>
            <nextsent>If web mark-up shared a similar characteristic, it might not provide sufficiently dis ambiguating cues to syntactic structure: HTML tags could be too short (e.g., singletons like click &lt;a&gt;here&lt;/a&gt;?)</nextsent>
            <nextsent>or otherwise unhelpful in resolving truly difficult ambiguities (such as PPattachment).</nextsent>
            <nextsent>We began simply by counting various basic events in BLOGp.</nextsent>
            <nextsent>Count POS Sequence Frac Sum 1 1,242 NNP NNP 16.1% 2 643 NNP 8.3 24.4 3 419 NNP NNP NNP 5.4 29.8 4 414 NN 5.4 35.2 5 201 JJ NN 2.6 37.8 7 138 NNS 1.8 41.3 8 112 JJ 1.5 42.8 9 102 VBD 1.3 44.1 10 92 DT NNP NNP NNP 1.2 45.3 11 85 JJ NNS 1.1 46.4 12 79 NNP NN 1.0 47.4 13 76 NN NN 1.0 48.4 14 61 VBN 0.8 49.2 15 60 NNP NNP NNP NNP 0.8 50.0 BLOGp +3,869 more with Count ? 49 50.0% Table 3: Top 50% of marked POS tag sequences.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>Linguistic Analysis of Mark-Up.</section>
      <citcontext>
         <prevsection>
            <prevsent>Most instances contributing to this pattern are flat NPs that end with a noun, incorrectly assumed to be the head of all other words in the phrase, e.g., ...</prevsent>
            <prevsent>[NP a 1994 &lt;i&gt;New Yorker&lt;/i&gt; article] ...</prevsent>
         </prevsection>
         <citsent citstr=" P05-1022 ">
            As this example shows, disagreements (as wellas agreements) between mark-up and machine generated parse trees with automatically percolated heads should be taken with a grain of salt.11 11In a relatively recent study, Ravi et al (2008)
            <papid>D08-1093</papid>
            report that Charniaks re-ranking parser (Charniak and Johnson, 2005
            <papid>P05-1022</papid>
            ) ? reranking-parserAug06.tar.gz, also available from ftp://ftp.cs.brown.edu/pub/nlparser/ ? attains 86.3% accuracy when trained on WSJ and tested againstBrown; its nearly 5% performance loss out-of-domain is consistent with the numbers originally reported by Gildea (2001)
            <papid>W01-0521</papid>
            .
         </citsent>
         <aftsection>
            <nextsent>1281 Count Constituent Production Frac Sum 1 746 NP?</nextsent>
            <nextsent>NNP NNP 9.6% 2 357 NP?</nextsent>
            <nextsent>NNP 4.6 14.3 3 266 NP?</nextsent>
            <nextsent>NP PP 3.4 17.7 4 183 NP?</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>Linguistic Analysis of Mark-Up.</section>
      <citcontext>
         <prevsection>
            <prevsent>Most instances contributing to this pattern are flat NPs that end with a noun, incorrectly assumed to be the head of all other words in the phrase, e.g., ...</prevsent>
            <prevsent>[NP a 1994 &lt;i&gt;New Yorker&lt;/i&gt; article] ...</prevsent>
         </prevsection>
         <citsent citstr=" W01-0521 ">
            As this example shows, disagreements (as wellas agreements) between mark-up and machine generated parse trees with automatically percolated heads should be taken with a grain of salt.11 11In a relatively recent study, Ravi et al (2008)
            <papid>D08-1093</papid>
            report that Charniaks re-ranking parser (Charniak and Johnson, 2005
            <papid>P05-1022</papid>
            ) ? reranking-parserAug06.tar.gz, also available from ftp://ftp.cs.brown.edu/pub/nlparser/ ? attains 86.3% accuracy when trained on WSJ and tested againstBrown; its nearly 5% performance loss out-of-domain is consistent with the numbers originally reported by Gildea (2001)
            <papid>W01-0521</papid>
            .
         </citsent>
         <aftsection>
            <nextsent>1281 Count Constituent Production Frac Sum 1 746 NP?</nextsent>
            <nextsent>NNP NNP 9.6% 2 357 NP?</nextsent>
            <nextsent>NNP 4.6 14.3 3 266 NP?</nextsent>
            <nextsent>NP PP 3.4 17.7 4 183 NP?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>be helpful in constraining grammar induction.</prevsent>
            <prevsent>Following Pereira and Schabes?</prevsent>
         </prevsection>
         <citsent citstr=" P02-1035 ">
            (1992) success with partial annotations in training a model of (English) constituents generatively, their idea hasbeen extended to discriminative estimation (Riezler et al, 2002
            <papid>P02-1035</papid>
            ) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005
            <papid>I05-1008</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>There was demand for partially bracketed corpora.</nextsent>
            <nextsent>
               Chen and Lee (1995)
               <papid>W95-0113</papid>
               constructed one such corpus by learning to partition (English) POS sequences into chunks (Abney, 1991); Inui and Kotani (2001) used n-gram statistics to split (Japanese) clauses.
            </nextsent>
            <nextsent>We combine the two intuitions, using the web to build a partially parsed corpus.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>be helpful in constraining grammar induction.</prevsent>
            <prevsent>Following Pereira and Schabes?</prevsent>
         </prevsection>
         <citsent citstr=" I05-1008 ">
            (1992) success with partial annotations in training a model of (English) constituents generatively, their idea hasbeen extended to discriminative estimation (Riezler et al, 2002
            <papid>P02-1035</papid>
            ) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005
            <papid>I05-1008</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>There was demand for partially bracketed corpora.</nextsent>
            <nextsent>
               Chen and Lee (1995)
               <papid>W95-0113</papid>
               constructed one such corpus by learning to partition (English) POS sequences into chunks (Abney, 1991); Inui and Kotani (2001) used n-gram statistics to split (Japanese) clauses.
            </nextsent>
            <nextsent>We combine the two intuitions, using the web to build a partially parsed corpus.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               (1992) success with partial annotations in training a model of (English) constituents generatively, their idea hasbeen extended to discriminative estimation (Riezler et al, 2002
               <papid>P02-1035</papid>
               ) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005
               <papid>I05-1008</papid>
               ).
            </prevsent>
            <prevsent>There was demand for partially bracketed corpora.</prevsent>
         </prevsection>
         <citsent citstr=" W95-0113 ">
            Chen and Lee (1995)
            <papid>W95-0113</papid>
            constructed one such corpus by learning to partition (English) POS sequences into chunks (Abney, 1991); Inui and Kotani (2001) used n-gram statistics to split (Japanese) clauses.
         </citsent>
         <aftsection>
            <nextsent>We combine the two intuitions, using the web to build a partially parsed corpus.</nextsent>
            <nextsent>Our approach could be called lightly-supervised, since it does not require manual annotation of a single complete parse tree.</nextsent>
            <nextsent>In contrast, traditional semi-supervised methods relyon fully-annotated seed corpora.18</nextsent>
            <nextsent>We explored novel ways of training dependency parsing models, the best of which attains 50.4% accuracy on Section 23 (all sentences) of WSJ, beating all previous unsupervised state-of-the-artby more than 5%.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>Conclusion.</section>
      <citcontext>
         <prevsection>
            <prevsent>Our linguistic analysis of a blog reveals that web annotations can be converted into accurate parsing constraints (loose: 88%; sprawl: 95%; tear: 99%) that could be helpful to supervised methods, e.g.,by boosting an initial parser via self-training (Mc Closky et al, 2006) on sentences with mark-up.Similar techniques may apply to standard wordprocessing annotations, such as font changes, and to certain (balanced) punctuation (Briscoe, 1994).</prevsent>
            <prevsent>We make our blog dataset, overlaying mark-up and syntax, publicly available.</prevsent>
         </prevsection>
         <citsent citstr=" P09-1041 ">
            Its annotations are 18A significant effort expended in building a tree-bank comes with the first batch of sentences (Druck et al, 2009
            <papid>P09-1041</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>75% noun phrases, 13% verb phrases, 7% simple declarative clauses and 2% prepositional phrases,with traces of other phrases, clauses and fragments.</nextsent>
            <nextsent>The type of mark-up, combined with POStags, could make for valuable features in discriminative models of parsing (Ratnaparkhi, 1999).A logical next step would be to explore the connection between syntax and mark-up for genres other than a news-style blog and for languages other than English.</nextsent>
            <nextsent>
               We are excited by the possibilities, as unsupervised parsers are on the cuspof becoming useful in their own right ? recently, Davidov et al (2009)
               <papid>W09-1108</papid>
               successfully appliedSeginers (2007) fully unsupervised grammar in ducer to the problems of pattern-acquisition and extraction of semantic data.
            </nextsent>
            <nextsent>If the strength of the connection between web mark-up and syntactic structure is universal across languages and genres, this fact could have broad implications for NLP, with applications extending well beyond parsing.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P10-1130.xml">profiting from markup hypertext annotations for guided parsing</title>
      <section>Conclusion.</section>
      <citcontext>
         <prevsection>
            <prevsent>75% noun phrases, 13% verb phrases, 7% simple declarative clauses and 2% prepositional phrases,with traces of other phrases, clauses and fragments.</prevsent>
            <prevsent>The type of mark-up, combined with POStags, could make for valuable features in discriminative models of parsing (Ratnaparkhi, 1999).A logical next step would be to explore the connection between syntax and mark-up for genres other than a news-style blog and for languages other than English.</prevsent>
         </prevsection>
         <citsent citstr=" W09-1108 ">
            We are excited by the possibilities, as unsupervised parsers are on the cuspof becoming useful in their own right ? recently, Davidov et al (2009)
            <papid>W09-1108</papid>
            successfully appliedSeginers (2007) fully unsupervised grammar in ducer to the problems of pattern-acquisition and extraction of semantic data.
         </citsent>
         <aftsection>
            <nextsent>If the strength of the connection between web mark-up and syntactic structure is universal across languages and genres, this fact could have broad implications for NLP, with applications extending well beyond parsing.</nextsent>
            <nextsent>Acknowledgments Partially funded by NSF award IIS-0811974 and by the Air Force Research Laboratory (AFRL), under prime contract no.</nextsent>
            <nextsent>FA8750-09-C-0181; first author supported by the Fannie &amp; John Hertz Foundation Fellowship.</nextsent>
            <nextsent>We thank Angel X. Chang, Spence Green, Christopher D. Manning, Richard Socher, Mihai Surdeanu and the anonymous reviewers for many helpful suggestions, and we are especially grateful to Andy Golding, for pointing us to his sample Map-Reduceover the Google News crawl, and to Daniel Pipes, for allowing us to distribute the dataset derived from his blog entries.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality.</prevsent>
            <prevsent>It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1121 ">
            One of the most successful syntax-based models is the string-to-tree model (Galley et al, 2006
            <papid>P06-1121</papid>
            ; Marcu et al, 2006
            <papid>W06-1606</papid>
            ; Shen et al, 2008
            <papid>P08-1066</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al, 2010
               <papid>P10-1076</papid>
               ) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al, 2007) and the formal syntax-based system Hiero (Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </nextsent>
            <nextsent>However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality.</prevsent>
            <prevsent>It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering.</prevsent>
         </prevsection>
         <citsent citstr=" W06-1606 ">
            One of the most successful syntax-based models is the string-to-tree model (Galley et al, 2006
            <papid>P06-1121</papid>
            ; Marcu et al, 2006
            <papid>W06-1606</papid>
            ; Shen et al, 2008
            <papid>P08-1066</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al, 2010
               <papid>P10-1076</papid>
               ) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al, 2007) and the formal syntax-based system Hiero (Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </nextsent>
            <nextsent>However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality.</prevsent>
            <prevsent>It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering.</prevsent>
         </prevsection>
         <citsent citstr=" P08-1066 ">
            One of the most successful syntax-based models is the string-to-tree model (Galley et al, 2006
            <papid>P06-1121</papid>
            ; Marcu et al, 2006
            <papid>W06-1606</papid>
            ; Shen et al, 2008
            <papid>P08-1066</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al, 2010
               <papid>P10-1076</papid>
               ) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al, 2007) and the formal syntax-based system Hiero (Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </nextsent>
            <nextsent>However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>In recent years, statistical translation models based upon linguistic syntax have shown promising progress in improving translation quality.</prevsent>
            <prevsent>It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering.</prevsent>
         </prevsection>
         <citsent citstr=" N09-1025 ">
            One of the most successful syntax-based models is the string-to-tree model (Galley et al, 2006
            <papid>P06-1121</papid>
            ; Marcu et al, 2006
            <papid>W06-1606</papid>
            ; Shen et al, 2008
            <papid>P08-1066</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al, 2010
               <papid>P10-1076</papid>
               ) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al, 2007) and the formal syntax-based system Hiero (Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </nextsent>
            <nextsent>However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering.</prevsent>
            <prevsent>
               One of the most successful syntax-based models is the string-to-tree model (Galley et al, 2006
               <papid>P06-1121</papid>
               ; Marcu et al, 2006
               <papid>W06-1606</papid>
               ; Shen et al, 2008
               <papid>P08-1066</papid>
               ; Chiang et al, 2009
               <papid>N09-1025</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P10-1076 ">
            Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al, 2010
            <papid>P10-1076</papid>
            ) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al, 2007) and the formal syntax-based system Hiero (Chiang, 2007
            <papid>J07-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones.</nextsent>
            <nextsent>
               The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al, 2005; Liu et al, 2006
               <papid>P06-1077</papid>
               ; Huang et al, 2006
               <papid>W06-3601</papid>
               ; Mi et al, 2008; Zhang et al, 2009
               <papid>P09-1020</papid>
               ).
            </nextsent>
            <nextsent>The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>It appears that encoding syntactic annotations on either side or both sides in translation rules can increase the expressiveness of rules and can produce more accurate translations with improved reordering.</prevsent>
            <prevsent>
               One of the most successful syntax-based models is the string-to-tree model (Galley et al, 2006
               <papid>P06-1121</papid>
               ; Marcu et al, 2006
               <papid>W06-1606</papid>
               ; Shen et al, 2008
               <papid>P08-1066</papid>
               ; Chiang et al, 2009
               <papid>N09-1025</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" J07-2003 ">
            Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al, 2010
            <papid>P10-1076</papid>
            ) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al, 2007) and the formal syntax-based system Hiero (Chiang, 2007
            <papid>J07-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones.</nextsent>
            <nextsent>
               The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al, 2005; Liu et al, 2006
               <papid>P06-1077</papid>
               ; Huang et al, 2006
               <papid>W06-3601</papid>
               ; Mi et al, 2008; Zhang et al, 2009
               <papid>P09-1020</papid>
               ).
            </nextsent>
            <nextsent>The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al, 2010
               <papid>P10-1076</papid>
               ) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al, 2007) and the formal syntax-based system Hiero (Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </prevsent>
            <prevsent>However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1077 ">
            The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al, 2005; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Huang et al, 2006
            <papid>W06-3601</papid>
            ; Mi et al, 2008; Zhang et al, 2009
            <papid>P09-1020</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints.</nextsent>
            <nextsent>Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches.</nextsent>
            <nextsent>
               A natural idea is the tree-to-tree model (Ding and Palmer, 2005
               <papid>P05-1067</papid>
               ; Cowan et al, 2006; Liu et al., 2009).
            </nextsent>
            <nextsent>
               However, as discussed by Chiang (2010)
               <papid>P10-1146</papid>
               , while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al, 2010
               <papid>P10-1076</papid>
               ) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al, 2007) and the formal syntax-based system Hiero (Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </prevsent>
            <prevsent>However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones.</prevsent>
         </prevsection>
         <citsent citstr=" W06-3601 ">
            The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al, 2005; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Huang et al, 2006
            <papid>W06-3601</papid>
            ; Mi et al, 2008; Zhang et al, 2009
            <papid>P09-1020</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints.</nextsent>
            <nextsent>Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches.</nextsent>
            <nextsent>
               A natural idea is the tree-to-tree model (Ding and Palmer, 2005
               <papid>P05-1067</papid>
               ; Cowan et al, 2006; Liu et al., 2009).
            </nextsent>
            <nextsent>
               However, as discussed by Chiang (2010)
               <papid>P10-1146</papid>
               , while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Since it explicitly models the grammaticality of the output via target-side syntax, the string-to-tree model (Xiao et al, 2010
               <papid>P10-1076</papid>
               ) significantly outperforms both the state-of-the-art phrase-based system Moses (Koehn et al, 2007) and the formal syntax-based system Hiero (Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </prevsent>
            <prevsent>However, there is a major limitation in the string-to-tree model: it does not utilize any useful source-side syntactic information, and thus to some extent lacks the ability to distinguish good translation rules from bad ones.</prevsent>
         </prevsection>
         <citsent citstr=" P09-1020 ">
            The source syntax is well-known to be helpful in improving translation accuracy, as shown especially by tree-to-string systems (Quirk et al, 2005; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Huang et al, 2006
            <papid>W06-3601</papid>
            ; Mi et al, 2008; Zhang et al, 2009
            <papid>P09-1020</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints.</nextsent>
            <nextsent>Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches.</nextsent>
            <nextsent>
               A natural idea is the tree-to-tree model (Ding and Palmer, 2005
               <papid>P05-1067</papid>
               ; Cowan et al, 2006; Liu et al., 2009).
            </nextsent>
            <nextsent>
               However, as discussed by Chiang (2010)
               <papid>P10-1146</papid>
               , while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The tree-to-string systems are simple and efficient, but they also have a major limitation: they cannot guarantee the grammaticality of the translation output because they lack target-side syntactic constraints.</prevsent>
            <prevsent>Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1067 ">
            A natural idea is the tree-to-tree model (Ding and Palmer, 2005
            <papid>P05-1067</papid>
            ; Cowan et al, 2006; Liu et al., 2009).
         </citsent>
         <aftsection>
            <nextsent>
               However, as discussed by Chiang (2010)
               <papid>P10-1146</papid>
               , while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained.
            </nextsent>
            <nextsent>
               Alternatively, Mi and Liu (2010)
               <papid>P10-1145</papid>
               proposed to enhance the tree-to-string model with target dependency structures (as a language model).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Thus a promising solution is to combine the advantages of the tree-to-string and string-to-tree approaches.</prevsent>
            <prevsent>
               A natural idea is the tree-to-tree model (Ding and Palmer, 2005
               <papid>P05-1067</papid>
               ; Cowan et al, 2006; Liu et al., 2009).
            </prevsent>
         </prevsection>
         <citsent citstr=" P10-1146 ">
            However, as discussed by Chiang (2010)
            <papid>P10-1146</papid>
            , while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained.
         </citsent>
         <aftsection>
            <nextsent>
               Alternatively, Mi and Liu (2010)
               <papid>P10-1145</papid>
               proposed to enhance the tree-to-string model with target dependency structures (as a language model).
            </nextsent>
            <nextsent>In this paper, we explore in the other direction: based on the strong string-to-tree model which builds an explicit target syntactic tree during decoding rather than apply only a syntactic language model, we aim to find a useful way to incorporate the source-side syntax.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               A natural idea is the tree-to-tree model (Ding and Palmer, 2005
               <papid>P05-1067</papid>
               ; Cowan et al, 2006; Liu et al., 2009).
            </prevsent>
            <prevsent>
               However, as discussed by Chiang (2010)
               <papid>P10-1146</papid>
               , while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained.
            </prevsent>
         </prevsection>
         <citsent citstr=" P10-1145 ">
            Alternatively, Mi and Liu (2010)
            <papid>P10-1145</papid>
            proposed to enhance the tree-to-string model with target dependency structures (as a language model).
         </citsent>
         <aftsection>
            <nextsent>In this paper, we explore in the other direction: based on the strong string-to-tree model which builds an explicit target syntactic tree during decoding rather than apply only a syntactic language model, we aim to find a useful way to incorporate the source-side syntax.</nextsent>
            <nextsent>204 First, we give a motivating example to show the importance of the source syntax for a string-to-tree model.</nextsent>
            <nextsent>Then we discuss the difficulties of integrating the source syntax into the string-to-tree model.</nextsent>
            <nextsent>Finally, we propose our solutions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>For the first problem, one may require the source side of a string-to-tree rule to be a constituent.</prevsent>
            <prevsent>However, such excessive constraints will exclude many good string-to-tree rules whose source strings are not constituents.</prevsent>
         </prevsection>
         <citsent citstr=" W06-3119 ">
            Inspired by Chiang (2010)
            <papid>P10-1146</papid>
            , we adopt a fuzzy way to label every source string with the complex syntactic categories of SAMT (Zollmann and Venugopal, 2006
            <papid>W06-3119</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>This method leads to a one-to-one correspondence between the new rules and the string-to-tree rules.</nextsent>
            <nextsent>We will detail our fuzzy labeling method in Section 2.</nextsent>
            <nextsent>For the second problem, it appears simple and intuitive to match rules by requiring a rules source syntactic category to be the same as the category of the test string.</nextsent>
            <nextsent>However, this hard constraint will greatly narrow the search space during decoding.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Rule Extraction.</section>
      <citcontext>
         <prevsection>
            <prevsent>in Figure 2 are part of the total of 13 minimal rules.</prevsent>
            <prevsent>To improve the rule coverage, SPMT models can be employed to obtain phrasal rules (Marcu et at., 2006).</prevsent>
         </prevsection>
         <citsent citstr=" P06-1121 ">
            In addition, the minimal rules which share the adjacent tree fragments can be connected together to form composed rules (Galley et al, 2006
            <papid>P06-1121</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>In Figure 2, jr is a rule composed by combining cr and gr . 2.2 Fuzzy-tree to Exact-tree Rule Extraction.</nextsent>
            <nextsent>Our fuzzy-tree to exact-tree rule extraction works on word-aligned tree-to-tree data (Figure 2 illustrates a Chinese-English tree pair).</nextsent>
            <nextsent>Basically, the extraction algorithm includes two parts: (1) String-to-tree rule extraction (without considering the source parse tree); (2) Decoration of the source side of the string-to tree rules with syntactic annotations.</nextsent>
            <nextsent>We use the same algorithm introduced in the previous section for extracting the base string-to tree rules.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Fuzzy Rule Matching Algorithms.</section>
      <citcontext>
         <prevsection>
            <prevsent>In this section, three fuzzy matching algorithms, from simple to complex, are investigated in order.</prevsent>
            <prevsent>3.1 0-1 Matching 0-1 matching is a straightforward approach that rewards rules whose source syntactic category exactly matches the syntactic category of the test string and punishes mismatches.</prevsent>
         </prevsection>
         <citsent citstr=" D10-1014 ">
            It has mainly been employed in hierarchical phrase-based models for integrating source or both-side syntax (Marton and Resnik, 2008
            <papid>D10-1014</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ; Chiang, 2010).
         </citsent>
         <aftsection>
            <nextsent>Since it is verified to be very effective in hierarchical models, we borrow this idea in our source-syntax-augmented string-to-tree translation.</nextsent>
            <nextsent>In 0-1 matching, the rules source side must contain only one syntactic category, but a rule may have been decorated with more than one syntactic category on the source side.</nextsent>
            <nextsent>Thus we have to choose the most reliable category and discard the others.</nextsent>
            <nextsent>Here, we select the one with the highest frequency.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Fuzzy Rule Matching Algorithms.</section>
      <citcontext>
         <prevsection>
            <prevsent>In this section, three fuzzy matching algorithms, from simple to complex, are investigated in order.</prevsent>
            <prevsent>3.1 0-1 Matching 0-1 matching is a straightforward approach that rewards rules whose source syntactic category exactly matches the syntactic category of the test string and punishes mismatches.</prevsent>
         </prevsection>
         <citsent citstr=" N09-1025 ">
            It has mainly been employed in hierarchical phrase-based models for integrating source or both-side syntax (Marton and Resnik, 2008
            <papid>D10-1014</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ; Chiang, 2010).
         </citsent>
         <aftsection>
            <nextsent>Since it is verified to be very effective in hierarchical models, we borrow this idea in our source-syntax-augmented string-to-tree translation.</nextsent>
            <nextsent>In 0-1 matching, the rules source side must contain only one syntactic category, but a rule may have been decorated with more than one syntactic category on the source side.</nextsent>
            <nextsent>Thus we have to choose the most reliable category and discard the others.</nextsent>
            <nextsent>Here, we select the one with the highest frequency.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Fuzzy Rule Matching Algorithms.</section>
      <citcontext>
         <prevsection>
            <prevsent>0-1 matching assigns similarity 1 for exact matches and 0 for mismatch, while likelihood matching directly utilizes the likelihood to measure the similarity.</prevsent>
            <prevsent>Going one step further, we adopt a measure of deep similarity, computed using latent distributions of syntactic categories.</prevsent>
         </prevsection>
         <citsent citstr=" D10-1014 ">
            Huang et al (2010)
            <papid>D10-1014</papid>
            proposed this method to compute the similarity between two syntactic tag sequences, used to impose soft syntactic constraints in hierarchical phrase-based models.
         </citsent>
         <aftsection>
            <nextsent>Analogously, we borrow this idea to calculate the similarity between two SAMT-style syntactic categories, and then apply it to calculate the degree of matching between a translation rule and the syntactic category of a test source string for purposes of fuzzy matching.</nextsent>
            <nextsent>We call this procedure deep similarity matching.</nextsent>
            <nextsent>Instead of directly using the SAMT-style syntactic categories, we represent each category by a real-valued feature vector.</nextsent>
            <nextsent>Suppose there is a set of n latent syntactic categories ? 1, , nV v v?</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Rule Binarization.</section>
      <citcontext>
         <prevsection>
            <prevsent>There are several ways to ensure cubic-time decoding.</prevsent>
            <prevsent>One way is to prune the extracted rules using a scope-3 grammar and do SCFG decoding without binarization (Hopkins and Leng mead, 2010).</prevsent>
         </prevsection>
         <citsent citstr=" N06-1033 ">
            The other, and most popular way is to binarize the translation rules (Zhang et al, 2006
            <papid>N06-1033</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We adopt the latter approach for efficient decoding with integrated n gram language models since this binarization technique has been well studied in string-to-tree 209 translation.</nextsent>
            <nextsent>However, when the rules?</nextsent>
            <nextsent>source string is decorated with syntax (fuzzy-tree to exact-tree rules), how should we binarize these rules?</nextsent>
            <nextsent>We use the rule rn in Figure 2 for illustration: ? ?</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>LDC2003E07, LDC2004T07 and LDC2005T06.</prevsent>
            <prevsent>210 symmetric word alignment.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1055 ">
            We parsed both sides of the parallel text with the Berkeley parser (Petrov et al, 2006
            <papid>P06-1055</papid>
            ) and trained a 5-gram language model with the target part of the bilingual data and the Xinhua portion of the English Gigaword corpus.
         </citsent>
         <aftsection>
            <nextsent>For tuning and testing, we use NIST MT evaluation data for Chinese-to-English from 2003 to 2006 (MT03 to MT06).</nextsent>
            <nextsent>
               The development dataset comes from MT06 in which sentences with more than 20 words are removed to speed up MERT6 (Och, 2003
               <papid>P03-1021</papid>
               ).
            </nextsent>
            <nextsent>The test set includes MT03 to MT05.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               We parsed both sides of the parallel text with the Berkeley parser (Petrov et al, 2006
               <papid>P06-1055</papid>
               ) and trained a 5-gram language model with the target part of the bilingual data and the Xinhua portion of the English Gigaword corpus.
            </prevsent>
            <prevsent>For tuning and testing, we use NIST MT evaluation data for Chinese-to-English from 2003 to 2006 (MT03 to MT06).</prevsent>
         </prevsection>
         <citsent citstr=" P03-1021 ">
            The development dataset comes from MT06 in which sentences with more than 20 words are removed to speed up MERT6 (Och, 2003
            <papid>P03-1021</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The test set includes MT03 to MT05.</nextsent>
            <nextsent>
               We implemented the baseline string-to-tree system ourselves according to (Galley et al, 2006
               <papid>P06-1121</papid>
               ; Marcu et al, 2006
               <papid>W06-1606</papid>
               ).
            </nextsent>
            <nextsent>We extracted minimal GHKM rules and the rules of SPMT Model 1 with source language phrases up to length L=4.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The development dataset comes from MT06 in which sentences with more than 20 words are removed to speed up MERT6 (Och, 2003
               <papid>P03-1021</papid>
               ).
            </prevsent>
            <prevsent>The test set includes MT03 to MT05.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1121 ">
            We implemented the baseline string-to-tree system ourselves according to (Galley et al, 2006
            <papid>P06-1121</papid>
            ; Marcu et al, 2006
            <papid>W06-1606</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We extracted minimal GHKM rules and the rules of SPMT Model 1 with source language phrases up to length L=4.</nextsent>
            <nextsent>We further extracted composed rules by composing two or three minimal GHKM rules.</nextsent>
            <nextsent>We also ran the state of-the-art hierarchical phrase-based system Joshua (Li et al, 2009) for comparison.</nextsent>
            <nextsent>In all systems, we set the beam size to 200.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>In all systems, we set the beam size to 200.</prevsent>
            <prevsent>The final translation quality is evaluated in terms of case-insensitive BLEU-4 with shortest length penalty.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3250 ">
            The statistical significance test is performed using the re-sampling approach (Koehn, 2004
            <papid>W04-3250</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>6.2 Results.</nextsent>
            <nextsent>Table 1 shows the translation results on development and test sets.</nextsent>
            <nextsent>First, we investigate the performance of the strong baseline string-to-tree model (s2t for short).</nextsent>
            <nextsent>As the table shows, s2t outperforms the hierarchical phrase-based system Joshua by more than 1.0 BLEU point in all translation tasks.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>Specifically, the FT2ET system with deep similarity matching obtains the best translation quality in all tasks and surpasses the baseline s2t system by 0.93 BLEU points in development data and by more than 1.0 BLEU point in test sets.</prevsent>
            <prevsent>The 0-1 matching algorithm is simple but effective, and it yields quite good performance (line 3).</prevsent>
         </prevsection>
         <citsent citstr=" D10-1014 ">
            The contribution of 0-1 matching as reflected in our experiments is consistent with the conclusions of (Marton and Resnik, 2008
            <papid>D10-1014</papid>
            ; Chiang, 2010).
         </citsent>
         <aftsection>
            <nextsent>By contrast, the system with likelihood matching does not perform as well as the other two algorithms, although it also significantly improves the baseline s2t in all tasks.</nextsent>
            <nextsent>6.3 Analysis and Discussion.</nextsent>
            <nextsent>We are a bit surprised at the large improvement gained by the 0-1 matching algorithm.</nextsent>
            <nextsent>This algorithm has several advantages: it is simple and easy to implement, and enhances the translation model by enabling its rules to take account of the source-side syntax to some degree.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>and short verb phrase ?, and obtains the correct phrase reordering at last.</prevsent>
            <prevsent>Several studies have tried to incorporate source or target syntax into translation models in a fuzzy manner.</prevsent>
         </prevsection>
         <citsent citstr=" P12-2057 ">
            Zollmann and Venugopal (2006) augment the hierarchical string-to-string rules (Chiang, 2005
            <papid>P12-2057</papid>
            ) with target-side syntax.
         </citsent>
         <aftsection>
            <nextsent>They annotate the target side of each string-to-string rule using SAMT-style syntactic categories and aim to generate the output more syntactically.</nextsent>
            <nextsent>Zhang et al (2010) base their approach on tree-to-string models, and generate grammatical output more reliably with the help of tree-to-tree sequence rules.</nextsent>
            <nextsent>Neither of them builds target syntactic trees using target syntax, however.</nextsent>
            <nextsent>Thus they can be viewed as integrating target syntax in a fuzzy manner.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Thus they can be viewed as integrating target syntax in a fuzzy manner.</prevsent>
            <prevsent>By contrast, we base our approach on a string-to-tree model which does construct target syntactic trees during decoding.</prevsent>
         </prevsection>
         <citsent citstr=" D10-1014 ">
            (Marton and Resnik, 2008
            <papid>D10-1014</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            and Huang et al, 2010) apply fuzzy techniques for integrating source syntax into hierarchical phrase based systems (Chiang, 2005
            <papid>P12-2057</papid>
            , 2007).
         </citsent>
         <aftsection>
            <nextsent>The first two studies employ 0-1 matching and the last tries deep similarity matching between two tag sequences.</nextsent>
            <nextsent>By contrast, we incorporate source syntax into a string-to-tree model.</nextsent>
            <nextsent>Furthermore, we apply fuzzy syntactic annotation on each rules source string and design three fuzzy rule matching algorithms.</nextsent>
            <nextsent>
               Chiang (2010)
               <papid>P10-1146</papid>
               proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical phrase-based system.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Thus they can be viewed as integrating target syntax in a fuzzy manner.</prevsent>
            <prevsent>By contrast, we base our approach on a string-to-tree model which does construct target syntactic trees during decoding.</prevsent>
         </prevsection>
         <citsent citstr=" N09-1025 ">
            (Marton and Resnik, 2008
            <papid>D10-1014</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            and Huang et al, 2010) apply fuzzy techniques for integrating source syntax into hierarchical phrase based systems (Chiang, 2005
            <papid>P12-2057</papid>
            , 2007).
         </citsent>
         <aftsection>
            <nextsent>The first two studies employ 0-1 matching and the last tries deep similarity matching between two tag sequences.</nextsent>
            <nextsent>By contrast, we incorporate source syntax into a string-to-tree model.</nextsent>
            <nextsent>Furthermore, we apply fuzzy syntactic annotation on each rules source string and design three fuzzy rule matching algorithms.</nextsent>
            <nextsent>
               Chiang (2010)
               <papid>P10-1146</papid>
               proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical phrase-based system.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Thus they can be viewed as integrating target syntax in a fuzzy manner.</prevsent>
            <prevsent>By contrast, we base our approach on a string-to-tree model which does construct target syntactic trees during decoding.</prevsent>
         </prevsection>
         <citsent citstr=" P12-2057 ">
            (Marton and Resnik, 2008
            <papid>D10-1014</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            and Huang et al, 2010) apply fuzzy techniques for integrating source syntax into hierarchical phrase based systems (Chiang, 2005
            <papid>P12-2057</papid>
            , 2007).
         </citsent>
         <aftsection>
            <nextsent>The first two studies employ 0-1 matching and the last tries deep similarity matching between two tag sequences.</nextsent>
            <nextsent>By contrast, we incorporate source syntax into a string-to-tree model.</nextsent>
            <nextsent>Furthermore, we apply fuzzy syntactic annotation on each rules source string and design three fuzzy rule matching algorithms.</nextsent>
            <nextsent>
               Chiang (2010)
               <papid>P10-1146</papid>
               proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical phrase-based system.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1019.xml">augmenting stringtotree translation models with fuzzy use of source side syntax</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>By contrast, we incorporate source syntax into a string-to-tree model.</prevsent>
            <prevsent>Furthermore, we apply fuzzy syntactic annotation on each rules source string and design three fuzzy rule matching algorithms.</prevsent>
         </prevsection>
         <citsent citstr=" P10-1146 ">
            Chiang (2010)
            <papid>P10-1146</papid>
            proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical phrase-based system.
         </citsent>
         <aftsection>
            <nextsent>He not only executes 0-1 matching on both sides of rules, but also designs numerous features such as . 'X Xroot which counts the number of rules whose source-side root label is X and target-side root label is 'X . This fuzzy use of source and target syntax enables the translation system to learn which tree labels are similar enough to be compatible, which ones are harmful to combine, and which ones can be ignored.</nextsent>
            <nextsent>The differences between us are twofold: 1) his work applies fuzzy syntax in both sides, while ours bases on the string- 212 Source sentence ??</nextsent>
            <nextsent>Reference hussein also established ties with terrorist networks Joshua hussein also and terrorist networks established relations s2t hussein also and terrorist networks established relations 1 FT2ET- DeepSim hussein also established relations with terrorist networks Source sentence ? [?</nextsent>
            <nextsent>Reference ..</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D10-1069.xml">uptraining for accurate deterministic question parsing</title>
      <section>Experimental Setup.</section>
      <citcontext>
         <prevsection>
            <prevsent>2.1 Data.</prevsent>
            <prevsent>Our main training set consists of Sections 02-21 of the Wall Street Journal portion of the Penn Treebank(Marcus et al, 1993), with Section 22 serving as development set for source domain comparisons.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1063 ">
            For our target domain experiments, we evaluate on the Question bank (Judge et al, 2006
            <papid>P06-1063</papid>
            ), which includes a set of manually annotated questions from a Trec question answering task.
         </citsent>
         <aftsection>
            <nextsent>The questions in the Ques tion bank are very different from our training data in terms of grammatical constructions and vocabulary usage, making this a rather extreme case of domain adaptation.</nextsent>
            <nextsent>We split the 4,000 questions containedin this corpus in three parts: the first 2,000 questions are reserved as a small target-domain training set; the remaining 2,000 questions are split in two equal parts, the first serving as development set and the second as our final test set.</nextsent>
            <nextsent>We report accuracies on the developments sets throughout this paper, and test only at the very end on the final test set.We convert the trees in both treebanks from constituencies to labeled dependencies (see Figure 1) using the Stanford converter, which produces 46 types of labeled dependencies1 (de Marneffe et al, 2006).</nextsent>
            <nextsent>We evaluate on both unlabeled (UAS) and labeled dependency accuracy (LAS).2Additionally, we use a set of 2 million questions collected from Internet search queries as unlabeled target domain data.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D10-1069.xml">uptraining for accurate deterministic question parsing</title>
      <section>Experimental Setup.</section>
      <citcontext>
         <prevsection>
            <prevsent>2Because the Question bank does not contain function tags,we decided to strip off the function tags from the WSJ before conversion.</prevsent>
            <prevsent>The Stanford conversion only uses the -ADV and -TMP tags, and removing all function tags from the WSJ changed less than 0.2% of the labels (primarily tmod labels).</prevsent>
         </prevsection>
         <citsent citstr=" A00-2018 ">
            706 Training on Evaluating on WSJ Section 22 Evaluating on Question bank WSJ Sections 02-21 F1 UAS LAS POS F1 UAS LAS POS Nivre et al (2007) ? 88.42 84.89 95.00 ? 74.14 62.81 88.48 McDonald et al (2006) ? 89.47 86.43 95.00 ? 80.01 67.00 88.48 Charniak (2000)
            <papid>A00-2018</papid>
            90.27 92.33 89.86 96.71 83.01 85.61 73.59 90.49 Charniak and Johnson (2005) 91.92 93.56 91.24 96.69 84.47 87.13 75.94 90.59 Petrov et al (2006) 90.70 92.91 90.48 96.27 85.52 88.17 79.10 90.57 Petrov (2010)
            <papid>N10-1003</papid>
            92.10 93.85 91.60 96.44 86.62 88.77 79.92 91.08 Our shift-reduce parser ? 88.24 84.69 95.00 ? 72.23 60.06 88.48 Our shift-reduce parser (gold POS) ? 90.51 88.53 100.00 ? 78.30 68.92 100.00 Table 1: Parsing accuracies for parsers trained on newswire data and evaluated on newswire and question test sets.
         </citsent>
         <aftsection>
            <nextsent>similar in style to the questions in the QuestionBank: (i) the queries must start with an English function word that can be used to start a question (what, who when, how, why, can, does, etc.), and (ii) the queries have a maximum length of 160 characters.</nextsent>
            <nextsent>2.2 Parsers.</nextsent>
            <nextsent>We use multiple publicly available parsers, as wellas our own implementation of a deterministic shift reduce parser in our experiments.</nextsent>
            <nextsent>
               The dependency parsers that we compare are the deterministic shift-reduce Malt parser (Nivre et al, 2007) and the second-order minimum spanning tree algorithm based Mst parser (McDonald et al, 2006
               <papid>W06-2932</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" D10-1069.xml">uptraining for accurate deterministic question parsing</title>
      <section>Experimental Setup.</section>
      <citcontext>
         <prevsection>
            <prevsent>2Because the Question bank does not contain function tags,we decided to strip off the function tags from the WSJ before conversion.</prevsent>
            <prevsent>The Stanford conversion only uses the -ADV and -TMP tags, and removing all function tags from the WSJ changed less than 0.2% of the labels (primarily tmod labels).</prevsent>
         </prevsection>
         <citsent citstr=" N10-1003 ">
            706 Training on Evaluating on WSJ Section 22 Evaluating on Question bank WSJ Sections 02-21 F1 UAS LAS POS F1 UAS LAS POS Nivre et al (2007) ? 88.42 84.89 95.00 ? 74.14 62.81 88.48 McDonald et al (2006) ? 89.47 86.43 95.00 ? 80.01 67.00 88.48 Charniak (2000)
            <papid>A00-2018</papid>
            90.27 92.33 89.86 96.71 83.01 85.61 73.59 90.49 Charniak and Johnson (2005) 91.92 93.56 91.24 96.69 84.47 87.13 75.94 90.59 Petrov et al (2006) 90.70 92.91 90.48 96.27 85.52 88.17 79.10 90.57 Petrov (2010)
            <papid>N10-1003</papid>
            92.10 93.85 91.60 96.44 86.62 88.77 79.92 91.08 Our shift-reduce parser ? 88.24 84.69 95.00 ? 72.23 60.06 88.48 Our shift-reduce parser (gold POS) ? 90.51 88.53 100.00 ? 78.30 68.92 100.00 Table 1: Parsing accuracies for parsers trained on newswire data and evaluated on newswire and question test sets.
         </citsent>
         <aftsection>
            <nextsent>similar in style to the questions in the QuestionBank: (i) the queries must start with an English function word that can be used to start a question (what, who when, how, why, can, does, etc.), and (ii) the queries have a maximum length of 160 characters.</nextsent>
            <nextsent>2.2 Parsers.</nextsent>
            <nextsent>We use multiple publicly available parsers, as wellas our own implementation of a deterministic shift reduce parser in our experiments.</nextsent>
            <nextsent>
               The dependency parsers that we compare are the deterministic shift-reduce Malt parser (Nivre et al, 2007) and the second-order minimum spanning tree algorithm based Mst parser (McDonald et al, 2006
               <papid>W06-2932</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" D10-1069.xml">uptraining for accurate deterministic question parsing</title>
      <section>Experimental Setup.</section>
      <citcontext>
         <prevsection>
            <prevsent>Finally, we run the latent variable parser (a.k.a. BerkeleyParser) of Petrov et al (2006),as well as the recent product of latent variable grammars version (Petrov, 2010).</prevsent>
            <prevsent>To facilitate comparisons between constituency and dependency parsers, we convert the output of the constituency parsers to labeled dependencies using the same procedure that is applied to the treebanks.</prevsent>
         </prevsection>
         <citsent citstr=" A00-1031 ">
            We also report their F1 scores for completeness.While the constituency parsers used in our experiments view part-of-speech (POS) tagging as an integral part of parsing, the dependency parsers require the input to be tagged with a separate POS tagger.We use the TnT tagger (Brants, 2000
            <papid>A00-1031</papid>
            ) in our experiments, because of its efficiency and ease of use.
         </citsent>
         <aftsection>
            <nextsent>Tagger and parser are always trained on the same data.</nextsent>
            <nextsent>We consider two domain adaptation scenarios in this paper.</nextsent>
            <nextsent>In the first scenario (sometimes abbreviated as WSJ), we assume that we do not have any labeled training data from the target domain.</nextsent>
            <nextsent>In practice, this will always be the case when the target domain is unknown or very diverse.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D10-1069.xml">uptraining for accurate deterministic question parsing</title>
      <section>Parsing Questions.</section>
      <citcontext>
         <prevsection>
            <prevsent>As can be seen in the left columns of Table 1, all parsers perform very well on the WSJ development set.</prevsent>
            <prevsent>While there are differences in the accuracies, all scores fall within a close range.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1012 ">
            The table also confirms the commonly known fact (Yamada and matsumoto, 2003; McDonald et al, 2005
            <papid>P05-1012</papid>
            ) that constituency parsers are more accurate at producing dependencies than dependency parsers (at least when the dependencies were produced by a deterministic transformation of a constituency treebank, as is the case here).This picture changes drastically when the performance is measured on the Question bank development set (right columns in Table 1).
         </citsent>
         <aftsection>
            <nextsent>
               As one 707 Evaluating on Training on WSJ + QB Training on Question bank Question bank F1 UAS LAS POS F1 UAS LAS POS Nivre et al (2007) ? 83.54 78.85 91.32 ? 79.72 73.44 88.80 McDonald et al (2006) ? 84.95 80.17 91.32 ? 82.52 77.20 88.80 Charniak (2000)
               <papid>A00-2018</papid>
               89.40 90.30 85.01 94.17 79.70 76.69 69.69 87.84 Petrov et al (2006) 90.96 90.98 86.90 94.01 86.62 84.09 78.92 87.56 Petrov (2010)
               <papid>N10-1003</papid>
               92.81 92.23 88.84 94.48 87.72 85.07 80.08 87.79 Our shift-reduce parser ? 83.70 78.27 91.32 ? 80.44 74.29 88.80 Our shift-reduce parser (gold POS) ? 89.39 86.60 100.00 ? 87.31 84.15 100.00 Table 2: Parsing accuracies for parsers trained on newswire and question data and evaluated on a question test set.
            </nextsent>
            <nextsent>might have expected, the accuracies are significantly lower, however, the drop for some of the parsersis shocking.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D10-1069.xml">uptraining for accurate deterministic question parsing</title>
      <section>Parsing Questions.</section>
      <citcontext>
         <prevsection>
            <prevsent>While there are differences in the accuracies, all scores fall within a close range.</prevsent>
            <prevsent>
               The table also confirms the commonly known fact (Yamada and matsumoto, 2003; McDonald et al, 2005
               <papid>P05-1012</papid>
               ) that constituency parsers are more accurate at producing dependencies than dependency parsers (at least when the dependencies were produced by a deterministic transformation of a constituency treebank, as is the case here).This picture changes drastically when the performance is measured on the Question bank development set (right columns in Table 1).
            </prevsent>
         </prevsection>
         <citsent citstr=" A00-2018 ">
            As one 707 Evaluating on Training on WSJ + QB Training on Question bank Question bank F1 UAS LAS POS F1 UAS LAS POS Nivre et al (2007) ? 83.54 78.85 91.32 ? 79.72 73.44 88.80 McDonald et al (2006) ? 84.95 80.17 91.32 ? 82.52 77.20 88.80 Charniak (2000)
            <papid>A00-2018</papid>
            89.40 90.30 85.01 94.17 79.70 76.69 69.69 87.84 Petrov et al (2006) 90.96 90.98 86.90 94.01 86.62 84.09 78.92 87.56 Petrov (2010)
            <papid>N10-1003</papid>
            92.81 92.23 88.84 94.48 87.72 85.07 80.08 87.79 Our shift-reduce parser ? 83.70 78.27 91.32 ? 80.44 74.29 88.80 Our shift-reduce parser (gold POS) ? 89.39 86.60 100.00 ? 87.31 84.15 100.00 Table 2: Parsing accuracies for parsers trained on newswire and question data and evaluated on a question test set.
         </citsent>
         <aftsection>
            <nextsent>might have expected, the accuracies are significantly lower, however, the drop for some of the parsersis shocking.</nextsent>
            <nextsent>Most notably, the deterministic shift reduce parsers lose almost 25% (absolute) on labeled accuracies, while the latent variable parsers lose around 12%.3 Note also that even with goldPOS tags, LAS is below 70% for our deterministic shift-reduce parser, suggesting that the drop inaccuracy is primarily due to a syntactic shift rather than a lexical shift.</nextsent>
            <nextsent>These low accuracies are especially disturbing when one considers that the average question in the evaluation set is only nine words long and therefore potentially much less ambiguous than WSJ sentences.</nextsent>
            <nextsent>We will examine the main error types more carefully in Section 5.Overall, the dependency parsers seem to suffer more from the domain change than the constituency parsers.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D10-1069.xml">uptraining for accurate deterministic question parsing</title>
      <section>Parsing Questions.</section>
      <citcontext>
         <prevsection>
            <prevsent>While there are differences in the accuracies, all scores fall within a close range.</prevsent>
            <prevsent>
               The table also confirms the commonly known fact (Yamada and matsumoto, 2003; McDonald et al, 2005
               <papid>P05-1012</papid>
               ) that constituency parsers are more accurate at producing dependencies than dependency parsers (at least when the dependencies were produced by a deterministic transformation of a constituency treebank, as is the case here).This picture changes drastically when the performance is measured on the Question bank development set (right columns in Table 1).
            </prevsent>
         </prevsection>
         <citsent citstr=" N10-1003 ">
            As one 707 Evaluating on Training on WSJ + QB Training on Question bank Question bank F1 UAS LAS POS F1 UAS LAS POS Nivre et al (2007) ? 83.54 78.85 91.32 ? 79.72 73.44 88.80 McDonald et al (2006) ? 84.95 80.17 91.32 ? 82.52 77.20 88.80 Charniak (2000)
            <papid>A00-2018</papid>
            89.40 90.30 85.01 94.17 79.70 76.69 69.69 87.84 Petrov et al (2006) 90.96 90.98 86.90 94.01 86.62 84.09 78.92 87.56 Petrov (2010)
            <papid>N10-1003</papid>
            92.81 92.23 88.84 94.48 87.72 85.07 80.08 87.79 Our shift-reduce parser ? 83.70 78.27 91.32 ? 80.44 74.29 88.80 Our shift-reduce parser (gold POS) ? 89.39 86.60 100.00 ? 87.31 84.15 100.00 Table 2: Parsing accuracies for parsers trained on newswire and question data and evaluated on a question test set.
         </citsent>
         <aftsection>
            <nextsent>might have expected, the accuracies are significantly lower, however, the drop for some of the parsersis shocking.</nextsent>
            <nextsent>Most notably, the deterministic shift reduce parsers lose almost 25% (absolute) on labeled accuracies, while the latent variable parsers lose around 12%.3 Note also that even with goldPOS tags, LAS is below 70% for our deterministic shift-reduce parser, suggesting that the drop inaccuracy is primarily due to a syntactic shift rather than a lexical shift.</nextsent>
            <nextsent>These low accuracies are especially disturbing when one considers that the average question in the evaluation set is only nine words long and therefore potentially much less ambiguous than WSJ sentences.</nextsent>
            <nextsent>We will examine the main error types more carefully in Section 5.Overall, the dependency parsers seem to suffer more from the domain change than the constituency parsers.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D10-1069.xml">uptraining for accurate deterministic question parsing</title>
      <section>Parsing Questions.</section>
      <citcontext>
         <prevsection>
            <prevsent>This means for example, that it is not possible to require the final parse to contain a verb (something that can be easily expressed by a top-level production of the form S ? NP VP in acontext free grammar).</prevsent>
            <prevsent>This is not a limitation of dependency parsers in general.</prevsent>
         </prevsection>
         <citsent citstr=" W06-1616 ">
            For example, it would be easy to enforce such constraints in the Eisner(1996) algorithm or using Integer Linear Programming approaches (Riedel and Clarke, 2006
            <papid>W06-1616</papid>
            ; Mart inset al, 2009).
         </citsent>
         <aftsection>
            <nextsent>However, such richer modeling capacity comes with a much higher computational cost.</nextsent>
            <nextsent>Looking at the constituency parsers, we observe 3The difference between our shift-reduce parser and the Malt parser are due to small differences in the feature sets.</nextsent>
            <nextsent>that the lexicalized (reranking) parser of Charniakand Johnson (2005) loses more than the latent variable approach of Petrov et al (2006).</nextsent>
            <nextsent>This difference doesnt seem to be a difference of generative vs. discriminative estimation.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D10-1069.xml">uptraining for accurate deterministic question parsing</title>
      <section>Uptraining for Domain-Adaptation.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Finally, Suzuki et al (2009)
               <papid>D09-1058</papid>
               present a very effective semi-supervised approach in which features from multiple generative models estimated on unlabeled data are combined in a discriminative system for structured prediction.
            </prevsent>
            <prevsent>All of these approaches have in common that their ultimate goal is to improve the final performance.</prevsent>
         </prevsection>
         <citsent citstr=" N10-1003 ">
            Our work differs in that instead of improving the 709 Uptraining with Using only WSJ data Using WSJ + QB data different base parsers UAS LAS POS UAS LAS POS Baseline 72.23 60.06 88.48 83.70 78.27 91.32 Self-training 73.62 61.63 89.60 84.26 79.15 92.09 Uptraining on Petrov et al (2006) 86.02 76.94 90.75 88.38 84.02 93.63 Uptraining on Petrov (2010)
            <papid>N10-1003</papid>
            85.21 76.19 90.74 88.63 84.14 93.53 Table 3: Uptraining substantially improves parsing accuracies, while self-training gives only minor improvements.
         </citsent>
         <aftsection>
            <nextsent>performance of the best parser, we want to builda more efficient parser that comes close to the accuracy of the best parser.</nextsent>
            <nextsent>To do this, we parse the unlabeled data with our most accurate parser and generate noisy, but fairly accurate labels (parse trees) for the unlabeled data.</nextsent>
            <nextsent>We refer to the parser used for producing the automatic labels as the base parser (unless otherwise noted, we used the latent variable parser of Petrov et al (2006) as our base parser).</nextsent>
            <nextsent>Because the most accurate base parsers are constituency parsers, we need to convert the parse trees to dependencies using the Stanford converter (see Section 2).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D10-1069.xml">uptraining for accurate deterministic question parsing</title>
      <section>Uptraining for Domain-Adaptation.</section>
      <citcontext>
         <prevsection>
            <prevsent>However, looking closer at Table 3, one can see that the POS accuracy is still relatively low (93.53%), potentially limiting the final accuracy.To remove this limitation (and also the dependence on a separate POS tagger), we experimented with word cluster features.</prevsent>
            <prevsent>As shown in Koo et al(2008), word cluster features can be used in conjunction with POS tags to improve parsing accuracy.</prevsent>
         </prevsection>
         <citsent citstr=" J92-4003 ">
            Here, we use them instead of POS tags in order to further reduce the domain-dependence of our model.Similar to Koo et al (2008)
            <papid>P08-1068</papid>
            , we use the Brown clustering algorithm (Brown et al, 1992
            <papid>J92-4003</papid>
            ) to produce adeterministic hierarchical clustering of our input vocabulary.
         </citsent>
         <aftsection>
            <nextsent>We then extract features based on vary 710 UAS LAS POS Part-of-Speech Tags 88.35 84.05 93.53 Word Cluster Features 87.92 83.73 ? Table 4: Parsing accuracies of uptrained parsers with and without part-of-speech tags and word cluster features.ing cluster granularities (6 and 10 bits in our experi ments).</nextsent>
            <nextsent>Table 4 shows that roughly the same level of accuracy can be achieved with cluster based features instead of POS tag features.</nextsent>
            <nextsent>This change makes our parser completely deterministic and enables us to process sentences in a single left-to-right pass.</nextsent>
            <nextsent>To provide a better understanding of the challenges involved in parsing questions, we analyzed the errors made by our WSJ-trained shift-reduce parser and also compared them to the errors that are left after uptraining.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>and taking advantage of the lexico-semantic knowledge provided in the different languages to improve text understanding.</prevsent>
            <prevsent>These two aspects are strongly intertwined: on the one hand, enablinglanguage-independent text understanding would allow for the harvesting of more knowledge in arbitrary languages, while, on the other hand, bringing together the lexical and semantic information available in different languages would improve the quality of text understanding in arbitrary languages.</prevsent>
         </prevsection>
         <citsent citstr=" N06-2036 ">
            However, these two goals have hitherto never been achieved, as is attested to by the fact that research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009
            <papid>N06-2036</papid>
            , WSD)has always been focused mostly on English.
         </citsent>
         <aftsection>
            <nextsent>
               Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sense tagged corpora like SemCor (Miller et al 1993
               <papid>H93-1061</papid>
               ).
            </nextsent>
            <nextsent>
               As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al 2004
               <papid>W04-0805</papid>
               ; Ma`rquez et al 2004; Orhan et al 2007
               <papid>W07-2011</papid>
               ; Okumura et al 2010).Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al 1991
               <papid>P91-1017</papid>
               ; Gale et al 1992
               <papid>W07-2037</papid>
               ; Resnik and Yarowsky, 1999
               <papid>W07-2037</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>These two aspects are strongly intertwined: on the one hand, enablinglanguage-independent text understanding would allow for the harvesting of more knowledge in arbitrary languages, while, on the other hand, bringing together the lexical and semantic information available in different languages would improve the quality of text understanding in arbitrary languages.</prevsent>
            <prevsent>
               However, these two goals have hitherto never been achieved, as is attested to by the fact that research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009
               <papid>N06-2036</papid>
               , WSD)has always been focused mostly on English.
            </prevsent>
         </prevsection>
         <citsent citstr=" H93-1061 ">
            Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sense tagged corpora like SemCor (Miller et al 1993
            <papid>H93-1061</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al 2004
               <papid>W04-0805</papid>
               ; Ma`rquez et al 2004; Orhan et al 2007
               <papid>W07-2011</papid>
               ; Okumura et al 2010).Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al 1991
               <papid>P91-1017</papid>
               ; Gale et al 1992
               <papid>W07-2037</papid>
               ; Resnik and Yarowsky, 1999
               <papid>W07-2037</papid>
               ).
            </nextsent>
            <nextsent>
               While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010
               <papid>W09-2413</papid>
               ) and cross-lingual lexical substitution(Mihalcea et al 2010).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               However, these two goals have hitherto never been achieved, as is attested to by the fact that research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009
               <papid>N06-2036</papid>
               , WSD)has always been focused mostly on English.
            </prevsent>
            <prevsent>
               Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sense tagged corpora like SemCor (Miller et al 1993
               <papid>H93-1061</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" W04-0805 ">
            As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al 2004
            <papid>W04-0805</papid>
            ; Ma`rquez et al 2004; Orhan et al 2007
            <papid>W07-2011</papid>
            ; Okumura et al 2010).Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al 1991
            <papid>P91-1017</papid>
            ; Gale et al 1992
            <papid>W07-2037</papid>
            ; Resnik and Yarowsky, 1999
            <papid>W07-2037</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010
               <papid>W09-2413</papid>
               ) and cross-lingual lexical substitution(Mihalcea et al 2010).
            </nextsent>
            <nextsent>
               At the same time, new re 1399 search on the topic has been done, including the useof statistical translations of sentences into many languages as features for supervised models (Banea andMihalcea, 2011; Lefever et al 2011), and the projection of monolingual knowledge onto another language (Khapra et al 2011
               <papid>P11-1057</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               However, these two goals have hitherto never been achieved, as is attested to by the fact that research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009
               <papid>N06-2036</papid>
               , WSD)has always been focused mostly on English.
            </prevsent>
            <prevsent>
               Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sense tagged corpora like SemCor (Miller et al 1993
               <papid>H93-1061</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" W07-2011 ">
            As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al 2004
            <papid>W04-0805</papid>
            ; Ma`rquez et al 2004; Orhan et al 2007
            <papid>W07-2011</papid>
            ; Okumura et al 2010).Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al 1991
            <papid>P91-1017</papid>
            ; Gale et al 1992
            <papid>W07-2037</papid>
            ; Resnik and Yarowsky, 1999
            <papid>W07-2037</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010
               <papid>W09-2413</papid>
               ) and cross-lingual lexical substitution(Mihalcea et al 2010).
            </nextsent>
            <nextsent>
               At the same time, new re 1399 search on the topic has been done, including the useof statistical translations of sentences into many languages as features for supervised models (Banea andMihalcea, 2011; Lefever et al 2011), and the projection of monolingual knowledge onto another language (Khapra et al 2011
               <papid>P11-1057</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               However, these two goals have hitherto never been achieved, as is attested to by the fact that research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009
               <papid>N06-2036</papid>
               , WSD)has always been focused mostly on English.
            </prevsent>
            <prevsent>
               Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sense tagged corpora like SemCor (Miller et al 1993
               <papid>H93-1061</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P91-1017 ">
            As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al 2004
            <papid>W04-0805</papid>
            ; Ma`rquez et al 2004; Orhan et al 2007
            <papid>W07-2011</papid>
            ; Okumura et al 2010).Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al 1991
            <papid>P91-1017</papid>
            ; Gale et al 1992
            <papid>W07-2037</papid>
            ; Resnik and Yarowsky, 1999
            <papid>W07-2037</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010
               <papid>W09-2413</papid>
               ) and cross-lingual lexical substitution(Mihalcea et al 2010).
            </nextsent>
            <nextsent>
               At the same time, new re 1399 search on the topic has been done, including the useof statistical translations of sentences into many languages as features for supervised models (Banea andMihalcea, 2011; Lefever et al 2011), and the projection of monolingual knowledge onto another language (Khapra et al 2011
               <papid>P11-1057</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               However, these two goals have hitherto never been achieved, as is attested to by the fact that research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009
               <papid>N06-2036</papid>
               , WSD)has always been focused mostly on English.
            </prevsent>
            <prevsent>
               Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sense tagged corpora like SemCor (Miller et al 1993
               <papid>H93-1061</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" W07-2037 ">
            As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al 2004
            <papid>W04-0805</papid>
            ; Ma`rquez et al 2004; Orhan et al 2007
            <papid>W07-2011</papid>
            ; Okumura et al 2010).Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al 1991
            <papid>P91-1017</papid>
            ; Gale et al 1992
            <papid>W07-2037</papid>
            ; Resnik and Yarowsky, 1999
            <papid>W07-2037</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010
               <papid>W09-2413</papid>
               ) and cross-lingual lexical substitution(Mihalcea et al 2010).
            </nextsent>
            <nextsent>
               At the same time, new re 1399 search on the topic has been done, including the useof statistical translations of sentences into many languages as features for supervised models (Banea andMihalcea, 2011; Lefever et al 2011), and the projection of monolingual knowledge onto another language (Khapra et al 2011
               <papid>P11-1057</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               However, these two goals have hitherto never been achieved, as is attested to by the fact that research in a core language understanding task such as Word Sense Disambiguation (Navigli, 2009
               <papid>N06-2036</papid>
               , WSD)has always been focused mostly on English.
            </prevsent>
            <prevsent>
               Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sense tagged corpora like SemCor (Miller et al 1993
               <papid>H93-1061</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" W07-2037 ">
            As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al 2004
            <papid>W04-0805</papid>
            ; Ma`rquez et al 2004; Orhan et al 2007
            <papid>W07-2011</papid>
            ; Okumura et al 2010).Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al 1991
            <papid>P91-1017</papid>
            ; Gale et al 1992
            <papid>W07-2037</papid>
            ; Resnik and Yarowsky, 1999
            <papid>W07-2037</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010
               <papid>W09-2413</papid>
               ) and cross-lingual lexical substitution(Mihalcea et al 2010).
            </nextsent>
            <nextsent>
               At the same time, new re 1399 search on the topic has been done, including the useof statistical translations of sentences into many languages as features for supervised models (Banea andMihalcea, 2011; Lefever et al 2011), and the projection of monolingual knowledge onto another language (Khapra et al 2011
               <papid>P11-1057</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Historically, English became established as the language used and understood by the scientific community and, consequently, most resources were developed for it, including large-scale computational lexicons like WordNet (Fellbaum, 1998) and sense tagged corpora like SemCor (Miller et al 1993
               <papid>H93-1061</papid>
               ).
            </prevsent>
            <prevsent>
               As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al 2004
               <papid>W04-0805</papid>
               ; Ma`rquez et al 2004; Orhan et al 2007
               <papid>W07-2011</papid>
               ; Okumura et al 2010).Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al 1991
               <papid>P91-1017</papid>
               ; Gale et al 1992
               <papid>W07-2037</papid>
               ; Resnik and Yarowsky, 1999
               <papid>W07-2037</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" W09-2413 ">
            While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010
            <papid>W09-2413</papid>
            ) and cross-lingual lexical substitution(Mihalcea et al 2010).
         </citsent>
         <aftsection>
            <nextsent>
               At the same time, new re 1399 search on the topic has been done, including the useof statistical translations of sentences into many languages as features for supervised models (Banea andMihalcea, 2011; Lefever et al 2011), and the projection of monolingual knowledge onto another language (Khapra et al 2011
               <papid>P11-1057</papid>
               ).
            </nextsent>
            <nextsent>Yet the above two goals, i.e., disambiguating inan arbitrary language and using lexical and semantic knowledge from many languages in a joint way to improve the WSD task, have not hitherto been attained.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               As a result WSD in other languages was hindered by a lack of resources, which in turn led to poor results or low involvement on the part of the research community (Magnini et al 2004
               <papid>W04-0805</papid>
               ; Ma`rquez et al 2004; Orhan et al 2007
               <papid>W07-2011</papid>
               ; Okumura et al 2010).Nonetheless, already in the 1990s it had been remarked that WSD could be improved by means of multilingual information: a recurring idea proposed by several researchers was that plausible translations of a word in context would restrict its possible senses to a manageable subset of meanings (Dagan et al 1991
               <papid>P91-1017</papid>
               ; Gale et al 1992
               <papid>W07-2037</papid>
               ; Resnik and Yarowsky, 1999
               <papid>W07-2037</papid>
               ).
            </prevsent>
            <prevsent>
               While the lack of resources at that time hampered the development of effective multilingual approaches to WSD, recently this idea has been revamped with the organization of SemEval tasks dealing with cross-lingual WSD (Lefever and Hoste, 2010
               <papid>W09-2413</papid>
               ) and cross-lingual lexical substitution(Mihalcea et al 2010).
            </prevsent>
         </prevsection>
         <citsent citstr=" P11-1057 ">
            At the same time, new re 1399 search on the topic has been done, including the useof statistical translations of sentences into many languages as features for supervised models (Banea andMihalcea, 2011; Lefever et al 2011), and the projection of monolingual knowledge onto another language (Khapra et al 2011
            <papid>P11-1057</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Yet the above two goals, i.e., disambiguating inan arbitrary language and using lexical and semantic knowledge from many languages in a joint way to improve the WSD task, have not hitherto been attained.</nextsent>
            <nextsent>In this paper, we address both objectives and propose a graph-based approach to multilingual joint Word Sense Disambiguation.</nextsent>
            <nextsent>Our proposal brings together the lexical knowledge from different languages by exploiting empirical evidence for disambiguation from each of them, and then combining this information in a synergistic way: each language provides a piece of sense evidence for the meaning of a target word in context, and subsequent integration of these various pieces enables them to (soft) constrain each other.</nextsent>
            <nextsent>The results show that this way we are able to improve over previous, high performing graph-based methods in both a monolingual and multilingual setting, thus showing for the first time the beneficial effects of exploiting multilingual knowledge in a joint fashion.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Our proposal brings together the lexical knowledge from different languages by exploiting empirical evidence for disambiguation from each of them, and then combining this information in a synergistic way: each language provides a piece of sense evidence for the meaning of a target word in context, and subsequent integration of these various pieces enables them to (soft) constrain each other.</prevsent>
            <prevsent>The results show that this way we are able to improve over previous, high performing graph-based methods in both a monolingual and multilingual setting, thus showing for the first time the beneficial effects of exploiting multilingual knowledge in a joint fashion.</prevsent>
         </prevsection>
         <citsent citstr=" W07-2037 ">
            Parallel corpora have been used in the literature for the automatic creation of a sense-tagged dataset for supervised WSD in different languages (Gale et al 1992
            <papid>W07-2037</papid>
            ; Chan and Ng, 2005
            <papid>W07-2037</papid>
            ; Zhong and Ng,2009).
         </citsent>
         <aftsection>
            <nextsent>
               Other approaches include the use of a coherence index for identifying the tendency to lexicalize senses differently across languages (Ide, 2000) and the clustering of source words which translate into the same target word, then used to perform Wsd using a similarity measure (Diab, 2003
               <papid>W07-2037</papid>
               ).
            </nextsent>
            <nextsent>
               A historical approach (Brown et al 1991
               <papid>P91-1034</papid>
               ) uses bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.All the above approaches to multilingual or cross lingual WSD relyon bilingual corpora, including those which exploit existing multilingual WordNet like resources (Ide et al 2002
               <papid>W02-0808</papid>
               ), or use automatically induced multilingual co-occurrence graphs (Silberer and Ponzetto, 2010
               <papid>S10-1027</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>The results show that this way we are able to improve over previous, high performing graph-based methods in both a monolingual and multilingual setting, thus showing for the first time the beneficial effects of exploiting multilingual knowledge in a joint fashion.</prevsent>
            <prevsent>
               Parallel corpora have been used in the literature for the automatic creation of a sense-tagged dataset for supervised WSD in different languages (Gale et al 1992
               <papid>W07-2037</papid>
               ; Chan and Ng, 2005
               <papid>W07-2037</papid>
               ; Zhong and Ng,2009).
            </prevsent>
         </prevsection>
         <citsent citstr=" W07-2037 ">
            Other approaches include the use of a coherence index for identifying the tendency to lexicalize senses differently across languages (Ide, 2000) and the clustering of source words which translate into the same target word, then used to perform Wsd using a similarity measure (Diab, 2003
            <papid>W07-2037</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               A historical approach (Brown et al 1991
               <papid>P91-1034</papid>
               ) uses bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.All the above approaches to multilingual or cross lingual WSD relyon bilingual corpora, including those which exploit existing multilingual WordNet like resources (Ide et al 2002
               <papid>W02-0808</papid>
               ), or use automatically induced multilingual co-occurrence graphs (Silberer and Ponzetto, 2010
               <papid>S10-1027</papid>
               ).
            </nextsent>
            <nextsent>However, this requirement is often very hard to satisfy, especially if we need wide coverage.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Parallel corpora have been used in the literature for the automatic creation of a sense-tagged dataset for supervised WSD in different languages (Gale et al 1992
               <papid>W07-2037</papid>
               ; Chan and Ng, 2005
               <papid>W07-2037</papid>
               ; Zhong and Ng,2009).
            </prevsent>
            <prevsent>
               Other approaches include the use of a coherence index for identifying the tendency to lexicalize senses differently across languages (Ide, 2000) and the clustering of source words which translate into the same target word, then used to perform Wsd using a similarity measure (Diab, 2003
               <papid>W07-2037</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P91-1034 ">
            A historical approach (Brown et al 1991
            <papid>P91-1034</papid>
            ) uses bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.All the above approaches to multilingual or cross lingual WSD relyon bilingual corpora, including those which exploit existing multilingual WordNet like resources (Ide et al 2002
            <papid>W02-0808</papid>
            ), or use automatically induced multilingual co-occurrence graphs (Silberer and Ponzetto, 2010
            <papid>S10-1027</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, this requirement is often very hard to satisfy, especially if we need wide coverage.</nextsent>
            <nextsent>
               To overcome this limitation, in this work we make use of BabelNet (Navigli and Ponzetto, 2010
               <papid>P10-1023</papid>
               ), a very large multilingual lexical knowledge base.
            </nextsent>
            <nextsent>This resource ? complementary in nature to other recent efforts presented by de Melo and Weikum (2010), Nastase et al(2010) and Meyer andGurevych (2012), inter alia ? provides a truly multilingual semantic network by combining Wikipedias multilinguality with the output of a state-of-the-artmachine translation system to achieve high cover age for all languages.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Parallel corpora have been used in the literature for the automatic creation of a sense-tagged dataset for supervised WSD in different languages (Gale et al 1992
               <papid>W07-2037</papid>
               ; Chan and Ng, 2005
               <papid>W07-2037</papid>
               ; Zhong and Ng,2009).
            </prevsent>
            <prevsent>
               Other approaches include the use of a coherence index for identifying the tendency to lexicalize senses differently across languages (Ide, 2000) and the clustering of source words which translate into the same target word, then used to perform Wsd using a similarity measure (Diab, 2003
               <papid>W07-2037</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" W02-0808 ">
            A historical approach (Brown et al 1991
            <papid>P91-1034</papid>
            ) uses bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.All the above approaches to multilingual or cross lingual WSD relyon bilingual corpora, including those which exploit existing multilingual WordNet like resources (Ide et al 2002
            <papid>W02-0808</papid>
            ), or use automatically induced multilingual co-occurrence graphs (Silberer and Ponzetto, 2010
            <papid>S10-1027</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, this requirement is often very hard to satisfy, especially if we need wide coverage.</nextsent>
            <nextsent>
               To overcome this limitation, in this work we make use of BabelNet (Navigli and Ponzetto, 2010
               <papid>P10-1023</papid>
               ), a very large multilingual lexical knowledge base.
            </nextsent>
            <nextsent>This resource ? complementary in nature to other recent efforts presented by de Melo and Weikum (2010), Nastase et al(2010) and Meyer andGurevych (2012), inter alia ? provides a truly multilingual semantic network by combining Wikipedias multilinguality with the output of a state-of-the-artmachine translation system to achieve high cover age for all languages.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Parallel corpora have been used in the literature for the automatic creation of a sense-tagged dataset for supervised WSD in different languages (Gale et al 1992
               <papid>W07-2037</papid>
               ; Chan and Ng, 2005
               <papid>W07-2037</papid>
               ; Zhong and Ng,2009).
            </prevsent>
            <prevsent>
               Other approaches include the use of a coherence index for identifying the tendency to lexicalize senses differently across languages (Ide, 2000) and the clustering of source words which translate into the same target word, then used to perform Wsd using a similarity measure (Diab, 2003
               <papid>W07-2037</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" S10-1027 ">
            A historical approach (Brown et al 1991
            <papid>P91-1034</papid>
            ) uses bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.All the above approaches to multilingual or cross lingual WSD relyon bilingual corpora, including those which exploit existing multilingual WordNet like resources (Ide et al 2002
            <papid>W02-0808</papid>
            ), or use automatically induced multilingual co-occurrence graphs (Silberer and Ponzetto, 2010
            <papid>S10-1027</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, this requirement is often very hard to satisfy, especially if we need wide coverage.</nextsent>
            <nextsent>
               To overcome this limitation, in this work we make use of BabelNet (Navigli and Ponzetto, 2010
               <papid>P10-1023</papid>
               ), a very large multilingual lexical knowledge base.
            </nextsent>
            <nextsent>This resource ? complementary in nature to other recent efforts presented by de Melo and Weikum (2010), Nastase et al(2010) and Meyer andGurevych (2012), inter alia ? provides a truly multilingual semantic network by combining Wikipedias multilinguality with the output of a state-of-the-artmachine translation system to achieve high cover age for all languages.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               A historical approach (Brown et al 1991
               <papid>P91-1034</papid>
               ) uses bilingual corpora to perform unsupervised word alignment and determine the most appropriate translation for a target word from a set of contextual features.All the above approaches to multilingual or cross lingual WSD relyon bilingual corpora, including those which exploit existing multilingual WordNet like resources (Ide et al 2002
               <papid>W02-0808</papid>
               ), or use automatically induced multilingual co-occurrence graphs (Silberer and Ponzetto, 2010
               <papid>S10-1027</papid>
               ).
            </prevsent>
            <prevsent>However, this requirement is often very hard to satisfy, especially if we need wide coverage.</prevsent>
         </prevsection>
         <citsent citstr=" P10-1023 ">
            To overcome this limitation, in this work we make use of BabelNet (Navigli and Ponzetto, 2010
            <papid>P10-1023</papid>
            ), a very large multilingual lexical knowledge base.
         </citsent>
         <aftsection>
            <nextsent>This resource ? complementary in nature to other recent efforts presented by de Melo and Weikum (2010), Nastase et al(2010) and Meyer andGurevych (2012), inter alia ? provides a truly multilingual semantic network by combining Wikipedias multilinguality with the output of a state-of-the-artmachine translation system to achieve high cover age for all languages.</nextsent>
            <nextsent>
               The key insight here is that word Sense Disambiguation and Machine Translation (MT) are highly intertwined tasks, as previously shown by Carpuat and Wu (2007)
               <papid>D07-1007</papid>
               and Chan et al (2007), who successfully used sense information to boost state-of-the-art statistical MT. In this work we focus instead on the benefits of using multilingual information for WSD by exploiting the structure of a multilingual semantic network.
            </nextsent>
            <nextsent>We present our methodology for multilingual WSD: we first introduce BabelNet, the resource used in our work (Section 3.1) and then present our algorithm for multilingual joint WSD (Section 3.2), including its main components, namely graph-based WSD, ensemble methods and translation weighting (sections 3.3, 3.4 and 3.5).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               To overcome this limitation, in this work we make use of BabelNet (Navigli and Ponzetto, 2010
               <papid>P10-1023</papid>
               ), a very large multilingual lexical knowledge base.
            </prevsent>
            <prevsent>This resource ? complementary in nature to other recent efforts presented by de Melo and Weikum (2010), Nastase et al(2010) and Meyer andGurevych (2012), inter alia ? provides a truly multilingual semantic network by combining Wikipedias multilinguality with the output of a state-of-the-artmachine translation system to achieve high cover age for all languages.</prevsent>
         </prevsection>
         <citsent citstr=" D07-1007 ">
            The key insight here is that word Sense Disambiguation and Machine Translation (MT) are highly intertwined tasks, as previously shown by Carpuat and Wu (2007)
            <papid>D07-1007</papid>
            and Chan et al (2007), who successfully used sense information to boost state-of-the-art statistical MT. In this work we focus instead on the benefits of using multilingual information for WSD by exploiting the structure of a multilingual semantic network.
         </citsent>
         <aftsection>
            <nextsent>We present our methodology for multilingual WSD: we first introduce BabelNet, the resource used in our work (Section 3.1) and then present our algorithm for multilingual joint WSD (Section 3.2), including its main components, namely graph-based WSD, ensemble methods and translation weighting (sections 3.3, 3.4 and 3.5).</nextsent>
            <nextsent>3.1 BabelNet.</nextsent>
            <nextsent>
               BabelNet (Navigli and Ponzetto, 2010
               <papid>P10-1023</papid>
               ) follows the structure of a traditional lexical knowledge base and, accordingly, consists of a labeled directed graph whose nodes represent concepts and named entities, and whose edges express semantic relations between them.
            </nextsent>
            <nextsent>Concepts and relations are harvested from the largest available semantic lexicon of English,i.e., WordNet, and a wide-coverage collaboratively edited encyclopedia, i.e., Wikipedia1, thus making BabelNet a multilingual encyclopedic dictionary which combines lexicographic information with encyclopedic knowledge on the basis of an unsupervised mapping framework.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Multilingual Joint WSD.</section>
      <citcontext>
         <prevsection>
            <prevsent>We present our methodology for multilingual WSD: we first introduce BabelNet, the resource used in our work (Section 3.1) and then present our algorithm for multilingual joint WSD (Section 3.2), including its main components, namely graph-based WSD, ensemble methods and translation weighting (sections 3.3, 3.4 and 3.5).</prevsent>
            <prevsent>3.1 BabelNet.</prevsent>
         </prevsection>
         <citsent citstr=" P10-1023 ">
            BabelNet (Navigli and Ponzetto, 2010
            <papid>P10-1023</papid>
            ) follows the structure of a traditional lexical knowledge base and, accordingly, consists of a labeled directed graph whose nodes represent concepts and named entities, and whose edges express semantic relations between them.
         </citsent>
         <aftsection>
            <nextsent>Concepts and relations are harvested from the largest available semantic lexicon of English,i.e., WordNet, and a wide-coverage collaboratively edited encyclopedia, i.e., Wikipedia1, thus making BabelNet a multilingual encyclopedic dictionary which combines lexicographic information with encyclopedic knowledge on the basis of an unsupervised mapping framework.</nextsent>
            <nextsent>In addition to a core 1http://www.wikipedia.org.</nextsent>
            <nextsent>In the following, we refer to Wikipedia pages and senses using SMALL CAPS.</nextsent>
            <nextsent>1400semantic network, BabelNet provides a multilingual lexical dimension.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Multilingual Joint WSD.</section>
      <citcontext>
         <prevsection>
            <prevsent>3.3 Graph-based WSD.</prevsent>
            <prevsent>We use graph-based algorithms to exploit multilingual knowledge from BabelNet for WSD.</prevsent>
         </prevsection>
         <citsent citstr=" P10-1154 ">
            These are a natural choice for our approach, since BabelNet is a semantic network, and such algorithms have been shown to achieve high performance across domains (Agirre et al 2009; Navigli et al 2011), as wellas to compete with supervised methods on a variety of lexical disambiguation tasks (Ponzetto and Navigli, 2010
            <papid>P10-1154</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>To this end, we use the method of Navigli and Lapata (2010) and construct a directed graphG = (V,E) for an input word sequence ? = (w1, . . .</nextsent>
            <nextsent>, wn)5 using the lexical and semantic relations found in BabelNet.</nextsent>
            <nextsent>The result of this procedure is a subgraph of BabelNet containing (1) the senses of the words in context, (2) all edges and intermediate senses found in BabelNet alg all paths that connect them.</nextsent>
            <nextsent>Given G, a target word w ? and its set of senses in BabelNet S ? V , we compute a score distribution (score1, . . .</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Multilingual Joint WSD.</section>
      <citcontext>
         <prevsection>
            <prevsent>, score|S|) over S, where scorej refers to the confidence score forthe j-th sense of w, e.g. bank2n, based on some connectivity measure applied to G. In this paper, we specifically focus on two such measures.5In our experiments we always take ? to be a single sentence, thus disambiguating on a sentence-by-sentence basis.</prevsent>
            <prevsent>1403 Degree Centrality (Degree): The first measure ranks the senses of a given word in the graph based on the number of their incident edges, namely: scorej = |{{sj , v} ? E : v ? V }| . This standard connectivity measure weights a sense as more appropriate if it has a higher degree.</prevsent>
         </prevsection>
         <citsent citstr=" P10-1154 ">
            We chose context-based Degree since, albeit simple, ithad previously been shown to yield a highly competitive performance on various WSD tasks (Navigli and Lapata, 2010; Ponzetto and Navigli, 2010
            <papid>P10-1154</papid>
            ).Inverse path length sum (PLength): We then developed a graph connectivity measure which scores each sense by summing over the inverse length of all paths which connect it to other senses in the graph: scorej = ? p? paths(sj) 1 elength(p)1 ,where paths(sj) is the set of simple paths connecting sj to the senses of other context words, length(p) is the number of edges in the path p andeach path is scored with the exponential inverse decay of the path length.
         </citsent>
         <aftsection>
            <nextsent>This measure overcomes the locality of Degree by aggregating over all paths between a sense of the target word and those of the context words, thus being able to capture the richness of the BabelNet subgraph and the semantic density of the underlying knowledge base.</nextsent>
            <nextsent>3.4 Ensemble methods for multilingual WSD.</nextsent>
            <nextsent>
               At the core of our algorithm lies the combination of the scores generated using the different translationsof the target word w. For this purpose, we apply so called ensemble methods, which have been shownto improve the performance of both supervised (Flo rian et al 2002) and unsupervised WSD systems (Brody et al 2006
               <papid>P06-1013</papid>
               ).
            </nextsent>
            <nextsent>Given |T | lexicalizations and|S| senses for w, the input to the combination component consists of a |T |?</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Multilingual Joint WSD.</section>
      <citcontext>
         <prevsection>
            <prevsent>This measure overcomes the locality of Degree by aggregating over all paths between a sense of the target word and those of the context words, thus being able to capture the richness of the BabelNet subgraph and the semantic density of the underlying knowledge base.</prevsent>
            <prevsent>3.4 Ensemble methods for multilingual WSD.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1013 ">
            At the core of our algorithm lies the combination of the scores generated using the different translationsof the target word w. For this purpose, we apply so called ensemble methods, which have been shownto improve the performance of both supervised (Flo rian et al 2002) and unsupervised WSD systems (Brody et al 2006
            <papid>P06-1013</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Given |T | lexicalizations and|S| senses for w, the input to the combination component consists of a |T |?</nextsent>
            <nextsent>|S|matrix LScore, where each cell lScorei,j quantifies the empirical support for sense sj from a term ti ? T (see Section 3.2 for an example).</nextsent>
            <nextsent>The ensemble method computes from this translation-sense matrix a combined scoring, expressing the joint confidence across terms in different languages over the set of senses S. In this work, we use the Probability Mixture?</nextsent>
            <nextsent>(PMixture) method proposed by Brody et al(2006), which they show to be the best performing for WSD.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>Among the different graph algorithms, PLength consistently outperforms Degree: however, the differences are not statistically significant.In order to better understand the impact of our approach we follow previous work (e.g., Navigli andLapata (2010)) and explore a weakly-supervised setting where the system attempts no sense assignment if the highest score among those assigned to the senses of a target word is below a certain threshold.</prevsent>
            <prevsent>If this is the case, in order to provide an answer for all items, we output the most frequent sense assigned by the system to other instances of the target word, and fall back to SemCors MFS if no assignment has been attempted.</prevsent>
         </prevsection>
         <citsent citstr=" W04-0811 ">
            We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004
            <papid>W04-0811</papid>
            )English all-words datasets.
         </citsent>
         <aftsection>
            <nextsent>
               The results for this setting are shown in Table 2, where we also compare with the top-performing systems from the SemEval competition, namely CFILT (Kulkarni et al 2010
               <papid>S10-1094</papid>
               ) and IIITH (Reddy et al 2010
               <papid>N06-2036</papid>
               ).
            </nextsent>
            <nextsent>By complementing our multilingual method withthe MFS heuristic we achieve a performance comparable with the state of the art on this task.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>If this is the case, in order to provide an answer for all items, we output the most frequent sense assigned by the system to other instances of the target word, and fall back to SemCors MFS if no assignment has been attempted.</prevsent>
            <prevsent>
               We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004
               <papid>W04-0811</papid>
               )English all-words datasets.
            </prevsent>
         </prevsection>
         <citsent citstr=" S10-1094 ">
            The results for this setting are shown in Table 2, where we also compare with the top-performing systems from the SemEval competition, namely CFILT (Kulkarni et al 2010
            <papid>S10-1094</papid>
            ) and IIITH (Reddy et al 2010
            <papid>N06-2036</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>By complementing our multilingual method withthe MFS heuristic we achieve a performance comparable with the state of the art on this task.</nextsent>
            <nextsent>Again, the multilingual ensemble approach consistently outperforms the monolingual one and enables us to achieve the best overall results for this dataset: without multilingual information, in fact, we achieve only average performance above the MFS level, whereas by effectively combining sense evidence from multilingual translations we are able to boost the F1 measure by a 6-8 point margin, and thus outperform the top ranking SemEval systems.</nextsent>
            <nextsent>While differences with CFILT are not statistically significant, we still take this to be good news, since our system is general purpose in nature and, accordingly, does not use any domain information such as manually-labeled examples for the most frequent domain words (CFILT) or a domain-specific sense ranking (IIITH).</nextsent>
            <nextsent>4.2 Cross-lingual lexical disambiguation.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>If this is the case, in order to provide an answer for all items, we output the most frequent sense assigned by the system to other instances of the target word, and fall back to SemCors MFS if no assignment has been attempted.</prevsent>
            <prevsent>
               We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004
               <papid>W04-0811</papid>
               )English all-words datasets.
            </prevsent>
         </prevsection>
         <citsent citstr=" N06-2036 ">
            The results for this setting are shown in Table 2, where we also compare with the top-performing systems from the SemEval competition, namely CFILT (Kulkarni et al 2010
            <papid>S10-1094</papid>
            ) and IIITH (Reddy et al 2010
            <papid>N06-2036</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>By complementing our multilingual method withthe MFS heuristic we achieve a performance comparable with the state of the art on this task.</nextsent>
            <nextsent>Again, the multilingual ensemble approach consistently outperforms the monolingual one and enables us to achieve the best overall results for this dataset: without multilingual information, in fact, we achieve only average performance above the MFS level, whereas by effectively combining sense evidence from multilingual translations we are able to boost the F1 measure by a 6-8 point margin, and thus outperform the top ranking SemEval systems.</nextsent>
            <nextsent>While differences with CFILT are not statistically significant, we still take this to be good news, since our system is general purpose in nature and, accordingly, does not use any domain information such as manually-labeled examples for the most frequent domain words (CFILT) or a domain-specific sense ranking (IIITH).</nextsent>
            <nextsent>4.2 Cross-lingual lexical disambiguation.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>To this end, we return for each test instance only the 1406 Algorithm P/R/F1 Baseline 23.80 Monolingual Degree 30.52 graph PLength 30.64 Multilingual Degree 32.21 ensemble PLength 32.47 UBA-T 32.17Table 3: Performance on SemEval-2010 lexical substitution (best results are bolded).</prevsent>
            <prevsent>most frequent translation found in the Babel synset.</prevsent>
         </prevsection>
         <citsent citstr=" J03-1002 ">
            Given that the two tasks make different assumptionson the sense inventory (no fixed inventory for CL LS vs. Europarl-based for CL-WSD), the frequency of a translation is calculated as either the number of Babel synsets in which it occurs (CL-LS), or its frequency of alignment with the target word, as obtained by applying GIZA++ (Och and Ney, 2003
            <papid>J03-1002</papid>
            ) to Europarl (CL-WSD).
         </citsent>
         <aftsection>
            <nextsent>To provide an answer for all instances, we return this most frequent translation even when no sense assignment is attempted ? i.e., no sense of the target word is connected to any other sense of the context words ? or a tie occurs.</nextsent>
            <nextsent>Results and discussion.</nextsent>
            <nextsent>We report our results forCL-LS and CL-WSD in Tables 3 and 4.</nextsent>
            <nextsent>We evaluate using the nouns-only subset of the CL-LS dataset and the full CL-WSD dataset, consisting of 300 and 1,000 instances of nouns in context, respectively.The evaluation scheme is based on the SemEval 2007 English lexical substitution task (McCarthy.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>We report our results forCL-LS and CL-WSD in Tables 3 and 4.</prevsent>
            <prevsent>We evaluate using the nouns-only subset of the CL-LS dataset and the full CL-WSD dataset, consisting of 300 and 1,000 instances of nouns in context, respectively.The evaluation scheme is based on the SemEval 2007 English lexical substitution task (McCarthy.</prevsent>
         </prevsection>
         <citsent citstr=" N06-2036 ">
            and Navigli, 2009
            <papid>N06-2036</papid>
            ), and consists of an adaptation of the metrics of precision and recall for the translation setting.
         </citsent>
         <aftsection>
            <nextsent>
               For each task, we compare our monolingual and multilingual approaches against the best performing SemEval systems for these tasks, namely UBA-T (Basile and Semeraro, 2010
               <papid>S10-1054</papid>
               ) and UVT-v(van Gompel, 2010) for CL-LS and CL-WSD, respectively, as well as a recent supervised proposal that exploits automatically generated multilingual features from parallel text and translated contexts (Lefever et al 2011, Parasense).
            </nextsent>
            <nextsent>For each task we also report its official baseline, namely the first translation from an online-dictionary6 for CL-LS, and the most frequent word alignment obtained by 6www.spanishdict.com applying GIZA++ to the Europarl data for CL-WSD.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>We evaluate using the nouns-only subset of the CL-LS dataset and the full CL-WSD dataset, consisting of 300 and 1,000 instances of nouns in context, respectively.The evaluation scheme is based on the SemEval 2007 English lexical substitution task (McCarthy.</prevsent>
            <prevsent>
               and Navigli, 2009
               <papid>N06-2036</papid>
               ), and consists of an adaptation of the metrics of precision and recall for the translation setting.
            </prevsent>
         </prevsection>
         <citsent citstr=" S10-1054 ">
            For each task, we compare our monolingual and multilingual approaches against the best performing SemEval systems for these tasks, namely UBA-T (Basile and Semeraro, 2010
            <papid>S10-1054</papid>
            ) and UVT-v(van Gompel, 2010) for CL-LS and CL-WSD, respectively, as well as a recent supervised proposal that exploits automatically generated multilingual features from parallel text and translated contexts (Lefever et al 2011, Parasense).
         </citsent>
         <aftsection>
            <nextsent>For each task we also report its official baseline, namely the first translation from an online-dictionary6 for CL-LS, and the most frequent word alignment obtained by 6www.spanishdict.com applying GIZA++ to the Europarl data for CL-WSD.</nextsent>
            <nextsent>Our cross-lingual results confirm all trends of the English monolingual evaluation, namely that: a) our joint multilingual approach substantially improves over the simple monolingual graph-based approach;b) it enables us to achieve state-of-the-art performance for these tasks.</nextsent>
            <nextsent>In the case of both CLLS and CL-WSD, using a rich multilingual knowledge base like BabelNet makes it possible to achieve a respectable performance already with the simple monolingual approach, thus indicating the viability of a knowledge-rich approach to sense-driven wordtranslation.</nextsent>
            <nextsent>The use of multilingual ensembles always improves the monolingual setting for all languages, and allows us to achieve the best overall results for both CL-LS and CL-WSD.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Conclusions.</section>
      <citcontext>
         <prevsection>
            <prevsent>Key to our methodology is the effective use of a wide-coverage multilingual knowledge base, BabelNet, which we exploit to perform graph-based WSD across languages and combine complementary sense evidence from translations in different languages using an ensemble method.</prevsent>
            <prevsent>This is the first proposal to exploit structured multilingual information within a joint, knowledge-rich framework for WSD.</prevsent>
         </prevsection>
         <citsent citstr=" P12-3012 ">
            The APIs to perform multilingual wsd using BabelNet are freely available for research purposes (Navigli and Ponzetto, 2012
            <papid>P12-3012</papid>
            b).
         </citsent>
         <aftsection>
            <nextsent>Thanks to multilingual joint WSD we achieve state-of-the-art performance on three different gold standards.</nextsent>
            <nextsent>The good news about these results is that not only can further advances be achieved by using multilingual lexical knowledge, but, more importantly, that combining multilingual sense evidence from different languages at the same time yields consistent improvements over a monolingual ap 1407 French German Italian Spanish P/R/F1 P/R/F1 P/R/F1 P/R/F1 Baseline 21.25 13.16 15.18 19.74 UvT-v N/A N/A N/A 23.39 Para sense 24.54 16.88 18.03 22.80 Monolingual Degree 22.94 17.15 18.03 22.48 graph PLength 23.42 17.72 18.19 22.76 Multilingual Degree 24.02 18.07 18.93 23.51 ensemble PLength 24.61 18.26 19.05 23.65 Table 4: Results on the SemEval-2010 cross-lingual WSD dataset (best results are bolded).</nextsent>
            <nextsent>proach in both monolingual and cross-lingual lexical disambiguation tasks ? that is, joining forces pays off?.</nextsent>
            <nextsent>Effectively leveraging multilingual knowledge for WSD helps overcome the shortcomings of the underlying resource (noise, coverage, etc.), thus indicating that further performance boosts can come in the future from even better multilingual lexicalresources.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1128.xml">joining forces pays off multilingual joint word sense disambiguation</title>
      <section>Conclusions.</section>
      <citcontext>
         <prevsection>
            <prevsent>proach in both monolingual and cross-lingual lexical disambiguation tasks ? that is, joining forces pays off?.</prevsent>
            <prevsent>Effectively leveraging multilingual knowledge for WSD helps overcome the shortcomings of the underlying resource (noise, coverage, etc.), thus indicating that further performance boosts can come in the future from even better multilingual lexicalresources.</prevsent>
         </prevsection>
         <citsent citstr=" P12-3012 ">
            Moreover, our methodology is general purpose and can be adapted to tasks other than WSD: in fact, we have already taken the first steps in this direction by showing the beneficial effects of a joint multilingual approach to computing semantic relatedness (Navigli and Ponzetto, 2012
            <papid>P12-3012</papid>
            a).
         </citsent>
         <aftsection>
            <nextsent>In addition, we plan in the very near future to generalize our multilingual joint approach and apply it tohigh-end tasks such as multilingual textual entailment (Mehdad et al 2011) and sentiment analysis(Lu et al 2011) ? so as to provide a general framework for knowledge-rich multilingual NLP.</nextsent>
            <nextsent>Acknowledgments the authors gratefully acknowledge the support of the ERC Starting Grant Multi jedi No. 259234.</nextsent>
            <nextsent>BabelNet and its API are available for download at http://lcl.uniroma1.it/babelnet.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Traditionally, this was tackled as a concept-to-text realization problem.</prevsent>
            <prevsent>However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-textgeneration.</prevsent>
         </prevsection>
         <citsent citstr=" P09-1094 ">
            The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009
            <papid>P09-1094</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               For example, one may want a text to be shorter (Cohnand Lapata, 2008), tailored to some reader profile (Zhu et al, 2010
               <papid>C10-1152</papid>
               ), compliant with some specific norms (Max, 2004
               <papid>C04-1166</papid>
               ), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996
               <papid>C96-2183</papid>
               ).
            </nextsent>
            <nextsent>The generation process must produce a text having a meaning which is compatible withthe definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraphrasing for text simplification), while ensuring that it remains grammatically correct.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-textgeneration.</prevsent>
            <prevsent>
               The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009
               <papid>P09-1094</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" C10-1152 ">
            For example, one may want a text to be shorter (Cohnand Lapata, 2008), tailored to some reader profile (Zhu et al, 2010
            <papid>C10-1152</papid>
            ), compliant with some specific norms (Max, 2004
            <papid>C04-1166</papid>
            ), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996
            <papid>C96-2183</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The generation process must produce a text having a meaning which is compatible withthe definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraphrasing for text simplification), while ensuring that it remains grammatically correct.</nextsent>
            <nextsent>Its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from one text to another is very dependent on the rewriting context.</nextsent>
            <nextsent>
               The wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (Madnani and Dorr, 2010
               <papid>J10-3003</papid>
               ), the inherent polysemy of such linguistic units andthe pragmatic constraints on their uses make it im possible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at a lexical level (Zhao et al., 2007).
            </nextsent>
            <nextsent>Hence, automatic contextual validation of candidate rewritings is a fundamental issue for text paraphrasing with phrasal units.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-textgeneration.</prevsent>
            <prevsent>
               The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009
               <papid>P09-1094</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" C04-1166 ">
            For example, one may want a text to be shorter (Cohnand Lapata, 2008), tailored to some reader profile (Zhu et al, 2010
            <papid>C10-1152</papid>
            ), compliant with some specific norms (Max, 2004
            <papid>C04-1166</papid>
            ), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996
            <papid>C96-2183</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The generation process must produce a text having a meaning which is compatible withthe definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraphrasing for text simplification), while ensuring that it remains grammatically correct.</nextsent>
            <nextsent>Its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from one text to another is very dependent on the rewriting context.</nextsent>
            <nextsent>
               The wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (Madnani and Dorr, 2010
               <papid>J10-3003</papid>
               ), the inherent polysemy of such linguistic units andthe pragmatic constraints on their uses make it im possible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at a lexical level (Zhao et al., 2007).
            </nextsent>
            <nextsent>Hence, automatic contextual validation of candidate rewritings is a fundamental issue for text paraphrasing with phrasal units.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>However, such needs apply sometimes to cases where a new text should be derived from some existing texts, an instance of text-to-textgeneration.</prevsent>
            <prevsent>
               The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009
               <papid>P09-1094</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" C96-2183 ">
            For example, one may want a text to be shorter (Cohnand Lapata, 2008), tailored to some reader profile (Zhu et al, 2010
            <papid>C10-1152</papid>
            ), compliant with some specific norms (Max, 2004
            <papid>C04-1166</papid>
            ), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996
            <papid>C96-2183</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The generation process must produce a text having a meaning which is compatible withthe definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraphrasing for text simplification), while ensuring that it remains grammatically correct.</nextsent>
            <nextsent>Its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from one text to another is very dependent on the rewriting context.</nextsent>
            <nextsent>
               The wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (Madnani and Dorr, 2010
               <papid>J10-3003</papid>
               ), the inherent polysemy of such linguistic units andthe pragmatic constraints on their uses make it im possible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at a lexical level (Zhao et al., 2007).
            </nextsent>
            <nextsent>Hence, automatic contextual validation of candidate rewritings is a fundamental issue for text paraphrasing with phrasal units.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The generation process must produce a text having a meaning which is compatible withthe definition of the task at hand (e.g. strict paraphrasing for document normalization, relaxed paraphrasing for text simplification), while ensuring that it remains grammatically correct.</prevsent>
            <prevsent>Its complexity, compared with concept-to-text generation, mostly stems from the fact that the semantic relationship between the original text and the new one is more difficult to control, as the mapping from one text to another is very dependent on the rewriting context.</prevsent>
         </prevsection>
         <citsent citstr=" J10-3003 ">
            The wide variety of techniques for acquiring phrasal paraphrases, which can subsequently be used by text paraphrasing techniques (Madnani and Dorr, 2010
            <papid>J10-3003</papid>
            ), the inherent polysemy of such linguistic units andthe pragmatic constraints on their uses make it im possible to ensure that potential paraphrase pairs will be substitutable in any context, an observation which was already made at a lexical level (Zhao et al., 2007).
         </citsent>
         <aftsection>
            <nextsent>Hence, automatic contextual validation of candidate rewritings is a fundamental issue for text paraphrasing with phrasal units.</nextsent>
            <nextsent>In this article, we tackle the problem of what we call targeted paraphrasing, defined as the rewriting of a subpart of a sentence, as in e.g.</nextsent>
            <nextsent>(Resnik et al,2010) where it is applied to making parts of sentences easier to translate automatically.</nextsent>
            <nextsent>While this problem is simpler than full sentence rewriting, its study is justified as it should be handled correctly for the more complex task to be successful.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Targeted paraphrasing for text revision.</section>
      <citcontext>
         <prevsection>
            <prevsent>This study also reports that there is an important variety of rephrasing phenomena, as illustrated by the difficulty of reaching a good identification coverage using a rule-based term variant identification engine.</prevsent>
            <prevsent>1Note that using the web may not always be appropriate, or that at least it should be used in a different way than what we propose in this article, in particular in cases where the desired properties of the rewritten text are better described in controlled corpora.</prevsent>
         </prevsection>
         <citsent citstr=" W08-1911 ">
            The use of automatic targeted paraphrasing as an authoring aid has been illustrated by the work ofMax and Zock (2008)
            <papid>W08-1911</papid>
            , in which writers are presented with potential paraphrases of sub-sentential fragments that they wish to reword.
         </citsent>
         <aftsection>
            <nextsent>
               The automatic paraphrasing technique used is a contextual variant of bilingual translation pivoting (Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ).
            </nextsent>
            <nextsent>It has also been proposed to externalize various text editing tasks, including proofreading, by having crowdsourcing functions on text directly from word processors (Bernstein et al, 2010).Text improvements may also be more specifically targeted for automatic applications.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Targeted paraphrasing for text revision.</section>
      <citcontext>
         <prevsection>
            <prevsent>1Note that using the web may not always be appropriate, or that at least it should be used in a different way than what we propose in this article, in particular in cases where the desired properties of the rewritten text are better described in controlled corpora.</prevsent>
            <prevsent>
               The use of automatic targeted paraphrasing as an authoring aid has been illustrated by the work ofMax and Zock (2008)
               <papid>W08-1911</papid>
               , in which writers are presented with potential paraphrases of sub-sentential fragments that they wish to reword.
            </prevsent>
         </prevsection>
         <citsent citstr=" P05-1074 ">
            The automatic paraphrasing technique used is a contextual variant of bilingual translation pivoting (Bannard and Callison-Burch, 2005
            <papid>P05-1074</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>It has also been proposed to externalize various text editing tasks, including proofreading, by having crowdsourcing functions on text directly from word processors (Bernstein et al, 2010).Text improvements may also be more specifically targeted for automatic applications.</nextsent>
            <nextsent>In thework by Resnik et al (2010), rephrasings for specific phrases are acquired through crowdsourcing.</nextsent>
            <nextsent>Difficult-to-translate phrases in the source text are first identified, and monolingual contributors are asked to provide rephrasings in context.</nextsent>
            <nextsent>
               Collected rephrasings can then be used as input for a Machine Translation system, which can positively exploit the increased variety in expression to produce more confident translations for better estimated source units (Schroeder et al, 2009
               <papid>E09-1082</papid>
               ).2 For instance, the phrase in bold in the sentence The number of people known to have died has now reached 358 can be rewritten as 1) who died, 2) identified to have died and 3) known to have passed away.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Targeted paraphrasing for text revision.</section>
      <citcontext>
         <prevsection>
            <prevsent>In thework by Resnik et al (2010), rephrasings for specific phrases are acquired through crowdsourcing.</prevsent>
            <prevsent>Difficult-to-translate phrases in the source text are first identified, and monolingual contributors are asked to provide rephrasings in context.</prevsent>
         </prevsection>
         <citsent citstr=" E09-1082 ">
            Collected rephrasings can then be used as input for a Machine Translation system, which can positively exploit the increased variety in expression to produce more confident translations for better estimated source units (Schroeder et al, 2009
            <papid>E09-1082</papid>
            ).2 For instance, the phrase in bold in the sentence The number of people known to have died has now reached 358 can be rewritten as 1) who died, 2) identified to have died and 3) known to have passed away.
         </citsent>
         <aftsection>
            <nextsent>All such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey a meaning which is reasonably close to the original wording.</nextsent>
            <nextsent>The task of rewriting complete sentences has also been addressed in various works (e.g.</nextsent>
            <nextsent>
               (Barzilay and Lee, 2003
               <papid>N03-1003</papid>
               ; Quirk et al, 2004
               <papid>W04-3219</papid>
               ; Zhao et al, 2010
               <papid>C10-1149</papid>
               )).
            </nextsent>
            <nextsent>Itposes, however, numerous other challenges, in particular regarding how it could be correctly evaluated.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Targeted paraphrasing for text revision.</section>
      <citcontext>
         <prevsection>
            <prevsent>All such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey a meaning which is reasonably close to the original wording.</prevsent>
            <prevsent>The task of rewriting complete sentences has also been addressed in various works (e.g.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1003 ">
            (Barzilay and Lee, 2003
            <papid>N03-1003</papid>
            ; Quirk et al, 2004
            <papid>W04-3219</papid>
            ; Zhao et al, 2010
            <papid>C10-1149</papid>
            )).
         </citsent>
         <aftsection>
            <nextsent>Itposes, however, numerous other challenges, in particular regarding how it could be correctly evaluated.</nextsent>
            <nextsent>Human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality.</nextsent>
            <nextsent>Using sentential paraphrases to support a given task (e.g. providing alternative reference translations for optimizing Statistical Machine Translation systems (Madnani et al, 2008)) 2It is to be noted that, in the scenario presented in (Resnik et al., 2010), monolingual contributors cannot predict how useful their rewritings will be to the underlying Machine Translation engine used.</nextsent>
            <nextsent>11 can be seen as a proxy for extrinsic evaluation of the quality of paraphrases, but it is not clear from published results that improvements on the task are clearly correlated with the quality of the produced paraphrases.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Targeted paraphrasing for text revision.</section>
      <citcontext>
         <prevsection>
            <prevsent>All such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey a meaning which is reasonably close to the original wording.</prevsent>
            <prevsent>The task of rewriting complete sentences has also been addressed in various works (e.g.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3219 ">
            (Barzilay and Lee, 2003
            <papid>N03-1003</papid>
            ; Quirk et al, 2004
            <papid>W04-3219</papid>
            ; Zhao et al, 2010
            <papid>C10-1149</papid>
            )).
         </citsent>
         <aftsection>
            <nextsent>Itposes, however, numerous other challenges, in particular regarding how it could be correctly evaluated.</nextsent>
            <nextsent>Human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality.</nextsent>
            <nextsent>Using sentential paraphrases to support a given task (e.g. providing alternative reference translations for optimizing Statistical Machine Translation systems (Madnani et al, 2008)) 2It is to be noted that, in the scenario presented in (Resnik et al., 2010), monolingual contributors cannot predict how useful their rewritings will be to the underlying Machine Translation engine used.</nextsent>
            <nextsent>11 can be seen as a proxy for extrinsic evaluation of the quality of paraphrases, but it is not clear from published results that improvements on the task are clearly correlated with the quality of the produced paraphrases.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Targeted paraphrasing for text revision.</section>
      <citcontext>
         <prevsection>
            <prevsent>All such rephrasings are grammatically correct, the first one being significantly shorter, and they all convey a meaning which is reasonably close to the original wording.</prevsent>
            <prevsent>The task of rewriting complete sentences has also been addressed in various works (e.g.</prevsent>
         </prevsection>
         <citsent citstr=" C10-1149 ">
            (Barzilay and Lee, 2003
            <papid>N03-1003</papid>
            ; Quirk et al, 2004
            <papid>W04-3219</papid>
            ; Zhao et al, 2010
            <papid>C10-1149</papid>
            )).
         </citsent>
         <aftsection>
            <nextsent>Itposes, however, numerous other challenges, in particular regarding how it could be correctly evaluated.</nextsent>
            <nextsent>Human judgments of whole sentence transformations are complex and intra- and inter-judge coherence is difficult to attain with hypotheses of comparable quality.</nextsent>
            <nextsent>Using sentential paraphrases to support a given task (e.g. providing alternative reference translations for optimizing Statistical Machine Translation systems (Madnani et al, 2008)) 2It is to be noted that, in the scenario presented in (Resnik et al., 2010), monolingual contributors cannot predict how useful their rewritings will be to the underlying Machine Translation engine used.</nextsent>
            <nextsent>11 can be seen as a proxy for extrinsic evaluation of the quality of paraphrases, but it is not clear from published results that improvements on the task are clearly correlated with the quality of the produced paraphrases.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Automatic sub-sentential paraphrase.</section>
      <citcontext>
         <prevsection>
            <prevsent>The techniques proposed have a strong relationship to the type of text corpus used 3This verse from Apollinaires Nuit Rhenane [which seems almost without rhythmic structure ? whose cesura is as if hidden].</prevsent>
            <prevsent>for acquisition, mainly: ? pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g.</prevsent>
         </prevsection>
         <citsent citstr=" P01-1008 ">
            (Barzilay and McKeown, 2001
            <papid>P01-1008</papid>
            ; Pang et al, 2003
            <papid>N03-1024</papid>
            ; Cohn et al., 2008
            <papid>J08-4005</papid>
            ; Bouamor et al, 2011
            <papid>P11-2069</papid>
            )) ? pairs of bilingual sentences (bilingual parallelcorpora) allow for a comparatively better recall (e.g.
         </citsent>
         <aftsection>
            <nextsent>
               (Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ; Kok and Brockett, 2010
               <papid>N10-1017</papid>
               ))?
            </nextsent>
            <nextsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Automatic sub-sentential paraphrase.</section>
      <citcontext>
         <prevsection>
            <prevsent>The techniques proposed have a strong relationship to the type of text corpus used 3This verse from Apollinaires Nuit Rhenane [which seems almost without rhythmic structure ? whose cesura is as if hidden].</prevsent>
            <prevsent>for acquisition, mainly: ? pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1024 ">
            (Barzilay and McKeown, 2001
            <papid>P01-1008</papid>
            ; Pang et al, 2003
            <papid>N03-1024</papid>
            ; Cohn et al., 2008
            <papid>J08-4005</papid>
            ; Bouamor et al, 2011
            <papid>P11-2069</papid>
            )) ? pairs of bilingual sentences (bilingual parallelcorpora) allow for a comparatively better recall (e.g.
         </citsent>
         <aftsection>
            <nextsent>
               (Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ; Kok and Brockett, 2010
               <papid>N10-1017</papid>
               ))?
            </nextsent>
            <nextsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Automatic sub-sentential paraphrase.</section>
      <citcontext>
         <prevsection>
            <prevsent>The techniques proposed have a strong relationship to the type of text corpus used 3This verse from Apollinaires Nuit Rhenane [which seems almost without rhythmic structure ? whose cesura is as if hidden].</prevsent>
            <prevsent>for acquisition, mainly: ? pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g.</prevsent>
         </prevsection>
         <citsent citstr=" J08-4005 ">
            (Barzilay and McKeown, 2001
            <papid>P01-1008</papid>
            ; Pang et al, 2003
            <papid>N03-1024</papid>
            ; Cohn et al., 2008
            <papid>J08-4005</papid>
            ; Bouamor et al, 2011
            <papid>P11-2069</papid>
            )) ? pairs of bilingual sentences (bilingual parallelcorpora) allow for a comparatively better recall (e.g.
         </citsent>
         <aftsection>
            <nextsent>
               (Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ; Kok and Brockett, 2010
               <papid>N10-1017</papid>
               ))?
            </nextsent>
            <nextsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Automatic sub-sentential paraphrase.</section>
      <citcontext>
         <prevsection>
            <prevsent>The techniques proposed have a strong relationship to the type of text corpus used 3This verse from Apollinaires Nuit Rhenane [which seems almost without rhythmic structure ? whose cesura is as if hidden].</prevsent>
            <prevsent>for acquisition, mainly: ? pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g.</prevsent>
         </prevsection>
         <citsent citstr=" P11-2069 ">
            (Barzilay and McKeown, 2001
            <papid>P01-1008</papid>
            ; Pang et al, 2003
            <papid>N03-1024</papid>
            ; Cohn et al., 2008
            <papid>J08-4005</papid>
            ; Bouamor et al, 2011
            <papid>P11-2069</papid>
            )) ? pairs of bilingual sentences (bilingual parallelcorpora) allow for a comparatively better recall (e.g.
         </citsent>
         <aftsection>
            <nextsent>
               (Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ; Kok and Brockett, 2010
               <papid>N10-1017</papid>
               ))?
            </nextsent>
            <nextsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Automatic sub-sentential paraphrase.</section>
      <citcontext>
         <prevsection>
            <prevsent>for acquisition, mainly: ? pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g.</prevsent>
            <prevsent>
               (Barzilay and McKeown, 2001
               <papid>P01-1008</papid>
               ; Pang et al, 2003
               <papid>N03-1024</papid>
               ; Cohn et al., 2008
               <papid>J08-4005</papid>
               ; Bouamor et al, 2011
               <papid>P11-2069</papid>
               )) ? pairs of bilingual sentences (bilingual parallelcorpora) allow for a comparatively better recall (e.g.
            </prevsent>
         </prevsection>
         <citsent citstr=" P05-1074 ">
            (Bannard and Callison-Burch, 2005
            <papid>P05-1074</papid>
            ; Kok and Brockett, 2010
            <papid>N10-1017</papid>
            ))?
         </citsent>
         <aftsection>
            <nextsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</nextsent>
            <nextsent>
               (Barzilay and Lee, 2003
               <papid>N03-1003</papid>
               ; Li et al, 2005
               <papid>I05-5007</papid>
               ; Bhagat andRavichandran, 2008; Deleger and Zweigen baum, 2009) Although the precision of such techniques can insome cases be formulated with regards to a predefined reference set (Cohn et al, 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair.
            </nextsent>
            <nextsent>This refers tothe problem of substituability in context (e.g.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Automatic sub-sentential paraphrase.</section>
      <citcontext>
         <prevsection>
            <prevsent>for acquisition, mainly: ? pairs of sentential paraphrases (monolingual parallel corpora) allow for a good precision but evidently a low recall (e.g.</prevsent>
            <prevsent>
               (Barzilay and McKeown, 2001
               <papid>P01-1008</papid>
               ; Pang et al, 2003
               <papid>N03-1024</papid>
               ; Cohn et al., 2008
               <papid>J08-4005</papid>
               ; Bouamor et al, 2011
               <papid>P11-2069</papid>
               )) ? pairs of bilingual sentences (bilingual parallelcorpora) allow for a comparatively better recall (e.g.
            </prevsent>
         </prevsection>
         <citsent citstr=" N10-1017 ">
            (Bannard and Callison-Burch, 2005
            <papid>P05-1074</papid>
            ; Kok and Brockett, 2010
            <papid>N10-1017</papid>
            ))?
         </citsent>
         <aftsection>
            <nextsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</nextsent>
            <nextsent>
               (Barzilay and Lee, 2003
               <papid>N03-1003</papid>
               ; Li et al, 2005
               <papid>I05-5007</papid>
               ; Bhagat andRavichandran, 2008; Deleger and Zweigen baum, 2009) Although the precision of such techniques can insome cases be formulated with regards to a predefined reference set (Cohn et al, 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair.
            </nextsent>
            <nextsent>This refers tothe problem of substituability in context (e.g.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Automatic sub-sentential paraphrase.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               (Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ; Kok and Brockett, 2010
               <papid>N10-1017</papid>
               ))?
            </prevsent>
            <prevsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1003 ">
            (Barzilay and Lee, 2003
            <papid>N03-1003</papid>
            ; Li et al, 2005
            <papid>I05-5007</papid>
            ; Bhagat andRavichandran, 2008; Deleger and Zweigen baum, 2009) Although the precision of such techniques can insome cases be formulated with regards to a predefined reference set (Cohn et al, 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair.
         </citsent>
         <aftsection>
            <nextsent>This refers tothe problem of substituability in context (e.g.</nextsent>
            <nextsent>(Con nor and Roth, 2007; Zhao et al, 2007)), which is a well studied field at the lexical level and the object of evaluation campains (McCarthy and Navigli, 2009).</nextsent>
            <nextsent>Contextual phrase substitution poses the additional challenge that phrases are rarer than words, so that building contextual and grammatical models to ensure that the generated rephrasings are both semantically compatible and grammatical is more complicated (e.g.</nextsent>
            <nextsent>
               (Callison-Burch, 2008
               <papid>D08-1021</papid>
               )).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Automatic sub-sentential paraphrase.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               (Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ; Kok and Brockett, 2010
               <papid>N10-1017</papid>
               ))?
            </prevsent>
            <prevsent>pairs of related sentences (monolingual comparable corpora) allow for even higher recall but possibly lower precision (e.g.</prevsent>
         </prevsection>
         <citsent citstr=" I05-5007 ">
            (Barzilay and Lee, 2003
            <papid>N03-1003</papid>
            ; Li et al, 2005
            <papid>I05-5007</papid>
            ; Bhagat andRavichandran, 2008; Deleger and Zweigen baum, 2009) Although the precision of such techniques can insome cases be formulated with regards to a predefined reference set (Cohn et al, 2008), it should more generally be assessed in the specific context of some use of the paraphrase pair.
         </citsent>
         <aftsection>
            <nextsent>This refers tothe problem of substituability in context (e.g.</nextsent>
            <nextsent>(Con nor and Roth, 2007; Zhao et al, 2007)), which is a well studied field at the lexical level and the object of evaluation campains (McCarthy and Navigli, 2009).</nextsent>
            <nextsent>Contextual phrase substitution poses the additional challenge that phrases are rarer than words, so that building contextual and grammatical models to ensure that the generated rephrasings are both semantically compatible and grammatical is more complicated (e.g.</nextsent>
            <nextsent>
               (Callison-Burch, 2008
               <papid>D08-1021</papid>
               )).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Automatic sub-sentential paraphrase.</section>
      <citcontext>
         <prevsection>
            <prevsent>(Con nor and Roth, 2007; Zhao et al, 2007)), which is a well studied field at the lexical level and the object of evaluation campains (McCarthy and Navigli, 2009).</prevsent>
            <prevsent>Contextual phrase substitution poses the additional challenge that phrases are rarer than words, so that building contextual and grammatical models to ensure that the generated rephrasings are both semantically compatible and grammatical is more complicated (e.g.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1021 ">
            (Callison-Burch, 2008
            <papid>D08-1021</papid>
            )).
         </citsent>
         <aftsection>
            <nextsent>The present work does not aim to present any original technique for paraphrase acquisition, but rather focusses on the task of sub-sentential paraphrase validation in context.</nextsent>
            <nextsent>We thus resort to some existing repertoire of phrasal paraphrase pairs.</nextsent>
            <nextsent>As explained in section 2, we use the WICOPACO corpus as a source of sub-sentential paraphrases: the phrase after rewriting can thus be used as a potential paraphrase in context.4 To obtain other candidates of various quality, we used two knowledge sources.</nextsent>
            <nextsent>
               The first uses automatic pivot translation (Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ), where a state-of-the-art4Note, however, that in our experiments we will ask our human judges to assess anew its paraphrasing status in context.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Automatic sub-sentential paraphrase.</section>
      <citcontext>
         <prevsection>
            <prevsent>First, players propose sub-sentential paraphrases in context for selected text spans in web documents(top of Figure 3), and then raters can take part in assessing paraphrases proposed by other players (bot tom of Figure 3).</prevsent>
            <prevsent>In order to avoid any bias, players cannot evaluate games in which they played.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1024 ">
            Evaluation is sped up by using a compact word lattice view for eliciting human judgments, built using the syntactic fusion algorithm of (Pang et al, 2003
            <papid>N03-1024</papid>
            ).Data acquisition was done in French to remain coherent with our experiments on the French corpus of WICOPACO, and both players and raters were native speakers.
         </citsent>
         <aftsection>
            <nextsent>An important point is that in our experiments the context of acquisition and of evaluation were different: players were asked to generate paraphrases in contexts that are different from those of the WICOPACO corpus used for evaluation.</nextsent>
            <nextsent>To this end, web snippets were automatically retrieved for the various phrases of our dataset without contexts, so that sentences from the Web (but not fromWikipedia) were used for manual paraphrase acquisition.</nextsent>
            <nextsent>This allows us to simulate the availability of a preexisting repertoire of (contextless) sub-sentential paraphrases, and to assess the performance of our contextual validation techniques on a possibly incompatible context.</nextsent>
            <nextsent>Given a repertoire of potential phrasal paraphrases and a context for a naturally-occurring rewriting, ourtask consists in deciding automatically which potential paraphrases can be substituted with good confidence for the original phrase.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Web-based contextual validation.</section>
      <citcontext>
         <prevsection>
            <prevsent>The specific nature of the text units that we are dealing with calls for a careful treatment: in the general scenario, it is unlikely that any supervised corpus would contain enough information for appropriate modeling of the substituability in context decision.</prevsent>
            <prevsent>It is therefore tempting to consider using the Web as the largest available information source,in spite of several of its known limitations, including that data can be of varying quality.</prevsent>
         </prevsection>
         <citsent citstr=" I05-5001 ">
            It has however been shown that a large range of NLP applications can be improved by exploiting n-gram counts from the Web (using Web document counts as a proxy) (Lapata and Keller, 2005).Paraphrase identification has been addressed previously, both using features computed from an offline corpus (Brockett and Dolan, 2005
            <papid>I05-5001</papid>
            ) and features computed from Web queries (Zhao et al, 2007).
         </citsent>
         <aftsection>
            <nextsent>However, to our knowledge previous work exploiting information from the Web was limited to the identification of lexical paraphrases.</nextsent>
            <nextsent>Although the probability of finding phrase occurrences significantly increases by considering the Web, some phrases are still very rare or not present in search engine indexes.</nextsent>
            <nextsent>
               As in (Brockett and Dolan, 2005
               <papid>I05-5001</papid>
               ), we tackle our paraphrase identification task as one of monolingualclassification.
            </nextsent>
            <nextsent>More precisely, considering an original phrase p within the context of sentence s, we seek to determine whether a candidate paraphrase p?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Web-based contextual validation.</section>
      <citcontext>
         <prevsection>
            <prevsent>hedit = TER(Lemorig, Lempara) (1)Note that this model is not derived from information from the Web, in contrast to all the models described next.Language model score The likelihood of a sentence can be a good indicator of its grammatical ity (Mutton, 2006).</prevsent>
            <prevsent>Language model probabilities can now be obtained from Web counts.</prevsent>
         </prevsection>
         <citsent citstr=" N10-2012 ">
            In our experiments, we used the Microsoft Web N-gram Ser vice6 for research (Wang et al, 2010
            <papid>N10-2012</papid>
            ) to obtain log likelihood scores for text units.7 However, this scoreis certainly not sufficient as it does not take the original wording into account.
         </citsent>
         <aftsection>
            <nextsent>We therefore used a ratioof the language model score of the paraphrased sentence with the language model score of the original 6http://research.microsoft.com/en-us/ collaboration/focus/cs/web-ngram.aspx7Note that in order to query on French text, we had to remove all diacritics for the service to behave correctly, independently of encodings: careful examination of ranked hypotheses showed that this trick allowed us to obtain results coherent with expectations.</nextsent>
            <nextsent>
               sentence, after normalization by sentence length of the language model scores (Onishi et al, 2010
               <papid>P10-2001</papid>
               ): hLM ratio = LM(para) LM(orig) = lm(para)1/length(para) lm(orig)1/length(orig) (2) Contextless thematic model scores Cooccurring words are used in distributional semantics to account for common meanings of words.
            </nextsent>
            <nextsent>We build vector representations of cooccurrences for both the original phrase p and its paraphrase p?.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-1602.xml">web based validation for contextual targeted paraphrasing</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>The second one, BOUNDLM, considers a sentence as a paraphrase whenever the counts for the bigrams crossing the left and right boundary of the sub-sentential paraphrase is higher than 10.</prevsent>
            <prevsent>Syntactic dependency baseline When rewriting asubpart of a sentence, the fact that syntactic dependencies between the rewritten phrase and its context are the same than those of the original phrase and the same context can provide some information 16 about the grammatical and semantic substituability of the two phrases (Zhao et al, 2007; Max and Zock, 2008).</prevsent>
         </prevsection>
         <citsent citstr=" N07-1051 ">
            We thus build syntactic dependencies for both the original and rewritten sentence, using the French version (Candito et al, 2010) of the Berkeley probabilistic parser (Petrov and Klein, 2007
            <papid>N07-1051</papid>
            ), and consider the subset of dependencies for the two sentences that exist between a word inside the phrase under focus and a word outside it (Deporig andDeppara).
         </citsent>
         <aftsection>
            <nextsent>Our CONTDEP baseline considers a sentence as a paraphrase iff Deppara = Deporig.</nextsent>
            <nextsent>5.3 Evaluation results.</nextsent>
            <nextsent>We used the models described in Section 4 to build a SVM classifier using the LIBSVM package (Chang and Lin, 2001).</nextsent>
            <nextsent>Accuracy results are reported on Figure 5.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Our algorithm also alleviates the oracle selection problem by ensuring that a forest always contains derivations that exactly yield the reference translation.</prevsent>
            <prevsent>With millions of features trained on 519K sentences in 0.03second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</prevsent>
         </prevsection>
         <citsent citstr=" P02-1038 ">
            Discriminative model (Och and Ney, 2002
            <papid>P02-1038</papid>
            ) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade.
         </citsent>
         <aftsection>
            <nextsent>
               Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al, 2006
               <papid>P06-1096</papid>
               ; Tillmann and Zhang, 2006
               <papid>P06-1091</papid>
               ; Watanabe et al, 2007
               <papid>D07-1080</papid>
               ; Blunsom et al, 2008
               <papid>P08-1024</papid>
               ; Chiang et al, 2009
               <papid>N09-1025</papid>
               ).
            </nextsent>
            <nextsent>However, the training of the large number of features was always restricted in fairly small datasets.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>With millions of features trained on 519K sentences in 0.03second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</prevsent>
            <prevsent>
               Discriminative model (Och and Ney, 2002
               <papid>P02-1038</papid>
               ) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade.
            </prevsent>
         </prevsection>
         <citsent citstr=" P06-1096 ">
            Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al, 2006
            <papid>P06-1096</papid>
            ; Tillmann and Zhang, 2006
            <papid>P06-1091</papid>
            ; Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Blunsom et al, 2008
            <papid>P08-1024</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, the training of the large number of features was always restricted in fairly small datasets.</nextsent>
            <nextsent>Some systems limit the number of training examples, while others use short sentences to maintain efficiency.</nextsent>
            <nextsent>
               Over fitting problem often comes when training many features on a small data (Watanabe et al, 2007
               <papid>D07-1080</papid>
               ; Chiang et al, 2009
               <papid>N09-1025</papid>
               ).
            </nextsent>
            <nextsent>Obviously, using much more data can alleviate such problem.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>With millions of features trained on 519K sentences in 0.03second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</prevsent>
            <prevsent>
               Discriminative model (Och and Ney, 2002
               <papid>P02-1038</papid>
               ) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade.
            </prevsent>
         </prevsection>
         <citsent citstr=" P06-1091 ">
            Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al, 2006
            <papid>P06-1096</papid>
            ; Tillmann and Zhang, 2006
            <papid>P06-1091</papid>
            ; Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Blunsom et al, 2008
            <papid>P08-1024</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, the training of the large number of features was always restricted in fairly small datasets.</nextsent>
            <nextsent>Some systems limit the number of training examples, while others use short sentences to maintain efficiency.</nextsent>
            <nextsent>
               Over fitting problem often comes when training many features on a small data (Watanabe et al, 2007
               <papid>D07-1080</papid>
               ; Chiang et al, 2009
               <papid>N09-1025</papid>
               ).
            </nextsent>
            <nextsent>Obviously, using much more data can alleviate such problem.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>With millions of features trained on 519K sentences in 0.03second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</prevsent>
            <prevsent>
               Discriminative model (Och and Ney, 2002
               <papid>P02-1038</papid>
               ) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade.
            </prevsent>
         </prevsection>
         <citsent citstr=" D07-1080 ">
            Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al, 2006
            <papid>P06-1096</papid>
            ; Tillmann and Zhang, 2006
            <papid>P06-1091</papid>
            ; Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Blunsom et al, 2008
            <papid>P08-1024</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, the training of the large number of features was always restricted in fairly small datasets.</nextsent>
            <nextsent>Some systems limit the number of training examples, while others use short sentences to maintain efficiency.</nextsent>
            <nextsent>
               Over fitting problem often comes when training many features on a small data (Watanabe et al, 2007
               <papid>D07-1080</papid>
               ; Chiang et al, 2009
               <papid>N09-1025</papid>
               ).
            </nextsent>
            <nextsent>Obviously, using much more data can alleviate such problem.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>With millions of features trained on 519K sentences in 0.03second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</prevsent>
            <prevsent>
               Discriminative model (Och and Ney, 2002
               <papid>P02-1038</papid>
               ) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade.
            </prevsent>
         </prevsection>
         <citsent citstr=" P08-1024 ">
            Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al, 2006
            <papid>P06-1096</papid>
            ; Tillmann and Zhang, 2006
            <papid>P06-1091</papid>
            ; Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Blunsom et al, 2008
            <papid>P08-1024</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, the training of the large number of features was always restricted in fairly small datasets.</nextsent>
            <nextsent>Some systems limit the number of training examples, while others use short sentences to maintain efficiency.</nextsent>
            <nextsent>
               Over fitting problem often comes when training many features on a small data (Watanabe et al, 2007
               <papid>D07-1080</papid>
               ; Chiang et al, 2009
               <papid>N09-1025</papid>
               ).
            </nextsent>
            <nextsent>Obviously, using much more data can alleviate such problem.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>With millions of features trained on 519K sentences in 0.03second per sentence, our system achieves significant improvement by 0.84 BLEU over the baseline system on the NIST Chinese-English test sets.</prevsent>
            <prevsent>
               Discriminative model (Och and Ney, 2002
               <papid>P02-1038</papid>
               ) can easily incorporate non-independent and overlapping features, and has been dominating the research field of statistical machine translation (SMT) in the last decade.
            </prevsent>
         </prevsection>
         <citsent citstr=" N09-1025 ">
            Recent work have shown that SMT benefits a lot from exploiting large amount of features (Liang et al, 2006
            <papid>P06-1096</papid>
            ; Tillmann and Zhang, 2006
            <papid>P06-1091</papid>
            ; Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Blunsom et al, 2008
            <papid>P08-1024</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>However, the training of the large number of features was always restricted in fairly small datasets.</nextsent>
            <nextsent>Some systems limit the number of training examples, while others use short sentences to maintain efficiency.</nextsent>
            <nextsent>
               Over fitting problem often comes when training many features on a small data (Watanabe et al, 2007
               <papid>D07-1080</papid>
               ; Chiang et al, 2009
               <papid>N09-1025</papid>
               ).
            </nextsent>
            <nextsent>Obviously, using much more data can alleviate such problem.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3.</prevsent>
            <prevsent>Generally, this is done by deleting a node X0,1.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1096 ">
            ing to constructing the oracle reference (Liang et al, 2006
            <papid>P06-1096</papid>
            ; Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ),which is non-trivial for SMT and needs to be determined experimentally.
         </citsent>
         <aftsection>
            <nextsent>Given such forests, we globally learn a log-linear model using stochastic gradient descend (Section 5).</nextsent>
            <nextsent>Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data.</nextsent>
            <nextsent>To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al, 2007) together with the features used in traditional SMT system (Section 6).</nextsent>
            <nextsent>Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 BLEU.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3.</prevsent>
            <prevsent>Generally, this is done by deleting a node X0,1.</prevsent>
         </prevsection>
         <citsent citstr=" D07-1080 ">
            ing to constructing the oracle reference (Liang et al, 2006
            <papid>P06-1096</papid>
            ; Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ),which is non-trivial for SMT and needs to be determined experimentally.
         </citsent>
         <aftsection>
            <nextsent>Given such forests, we globally learn a log-linear model using stochastic gradient descend (Section 5).</nextsent>
            <nextsent>Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data.</nextsent>
            <nextsent>To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al, 2007) together with the features used in traditional SMT system (Section 6).</nextsent>
            <nextsent>Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 BLEU.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>(3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3.</prevsent>
            <prevsent>Generally, this is done by deleting a node X0,1.</prevsent>
         </prevsection>
         <citsent citstr=" N09-1025 ">
            ing to constructing the oracle reference (Liang et al, 2006
            <papid>P06-1096</papid>
            ; Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ),which is non-trivial for SMT and needs to be determined experimentally.
         </citsent>
         <aftsection>
            <nextsent>Given such forests, we globally learn a log-linear model using stochastic gradient descend (Section 5).</nextsent>
            <nextsent>Overall, both the generation of forests and the training algorithm are scalable, enabling us to train millions of features on large-scale data.</nextsent>
            <nextsent>To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al, 2007) together with the features used in traditional SMT system (Section 6).</nextsent>
            <nextsent>Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 BLEU.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Synchronous Context Free Grammar.</section>
      <citcontext>
         <prevsection>
            <prevsent>To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al, 2007) together with the features used in traditional SMT system (Section 6).</prevsent>
            <prevsent>Training on 519K sentence pairs in 0.03 seconds per sentence, we achieve significantly improvement over the traditional pipeline by 0.84 BLEU.</prevsent>
         </prevsection>
         <citsent citstr=" J07-2003 ">
            We work on synchronous context free grammar(SCFG) (Chiang, 2007
            <papid>J07-2003</papid>
            ) based translation.
         </citsent>
         <aftsection>
            <nextsent>The elementary structures in an SCFG are rewrite rules of the form: X ? ??, where ? and ? are strings of terminals and nonter minals.</nextsent>
            <nextsent>We call ? and ? as the source side and the target side of rule respectively.</nextsent>
            <nextsent>
               Here a rule means aphrase translation (Koehn et al, 2003
               <papid>N03-1017</papid>
               ) or a translation pair that contains nonterminals.
            </nextsent>
            <nextsent>We call a sequence of translation steps as aderivation.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Synchronous Context Free Grammar.</section>
      <citcontext>
         <prevsection>
            <prevsent>The elementary structures in an SCFG are rewrite rules of the form: X ? ??, where ? and ? are strings of terminals and nonter minals.</prevsent>
            <prevsent>We call ? and ? as the source side and the target side of rule respectively.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            Here a rule means aphrase translation (Koehn et al, 2003
            <papid>N03-1017</papid>
            ) or a translation pair that contains nonterminals.
         </citsent>
         <aftsection>
            <nextsent>We call a sequence of translation steps as aderivation.</nextsent>
            <nextsent>In context of SCFG, a derivation is a sequence of SCFG rules {ri}.</nextsent>
            <nextsent>
               Translation forest (Miet al, 2008; Li and Eisner, 2009
               <papid>D09-1005</papid>
               ) is a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1).
            </nextsent>
            <nextsent>A tree t in the forest corresponds to a derivation.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Synchronous Context Free Grammar.</section>
      <citcontext>
         <prevsection>
            <prevsent>We call a sequence of translation steps as aderivation.</prevsent>
            <prevsent>In context of SCFG, a derivation is a sequence of SCFG rules {ri}.</prevsent>
         </prevsection>
         <citsent citstr=" D09-1005 ">
            Translation forest (Miet al, 2008; Li and Eisner, 2009
            <papid>D09-1005</papid>
            ) is a compact representation of all the derivations for a given sentence under an SCFG (see Figure 1).
         </citsent>
         <aftsection>
            <nextsent>A tree t in the forest corresponds to a derivation.</nextsent>
            <nextsent>In our paper, tree means the same as derivation.</nextsent>
            <nextsent>More formally, a forest is a pair V,E?, where V is the set of nodes, E is the set of hyperedge.</nextsent>
            <nextsent>For a given source sentence f = fn1 , Each node v ? V is in the form Xi,j , which denotes the recognition of nonterminal X spanning the substring from the i through j (that is fi+1...fj).</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Fast Generation.</section>
      <citcontext>
         <prevsection>
            <prevsent>The changes made by the two operators are local.Considering the change of rules, the lexicalize operator deletes two rules and adds one new rule, while the generalize operator deletes one rule and adds two new rules.</prevsent>
            <prevsent>Such local changes provide us with a wayto incrementally calculate the scores of new derivations.</prevsent>
         </prevsection>
         <citsent citstr=" P09-1088 ">
            We use this method motivated by Gibbs Sampler (Blunsom et al, 2009
            <papid>P09-1088</papid>
            ) which has been used for efficiently learning rules.
         </citsent>
         <aftsection>
            <nextsent>The different lies in thatwe use the operator for decoding where the rule table is fixing.</nextsent>
            <nextsent>4.2 Initialize a Reference Derivation.</nextsent>
            <nextsent>The generation starts from an initial reference derivation with max score.</nextsent>
            <nextsent>
               This requires bi-parsing(Dyer, 2010
               <papid>N10-1033</papid>
               ) over the source sentence f and the reference translation e. In practice, we may face three problems.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Fast Generation.</section>
      <citcontext>
         <prevsection>
            <prevsent>Exhaustive search over the space under SCFG requires O(|f |3|e|3).</prevsent>
            <prevsent>
               883 To parse quickly, we only visit the tight consistent (Zhang et al, 2008
               <papid>C08-1136</papid>
               ) bi-spans with the help of word alignment a. Only visiting tight consistent spans greatly speeds up bi-parsing.
            </prevsent>
         </prevsection>
         <citsent citstr=" J07-2003 ">
            Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007
            <papid>J07-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Second is degenerate problem.</nextsent>
            <nextsent>
               If we only usethe features as traditional SCFG systems, the bi parsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al, 2006
               <papid>W06-3105</papid>
               ).
            </nextsent>
            <nextsent>That is because the translation rules with raresource/target sides always receive a very high translation probability.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Fast Generation.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Besides efficiency, adoption of this constraint receives support from the fact that heuristic SCFG rule extraction only extracts tight consistent initial phrases (Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </prevsent>
            <prevsent>Second is degenerate problem.</prevsent>
         </prevsection>
         <citsent citstr=" W06-3105 ">
            If we only usethe features as traditional SCFG systems, the bi parsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al, 2006
            <papid>W06-3105</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>That is because the translation rules with raresource/target sides always receive a very high translation probability.</nextsent>
            <nextsent>We add a prior score log(#rule)for each rule, where #rule is the number of occurrence of a rule, to reward frequent reusable rules and derivations with more rules.Finally, we may fail to create reference derivations due to the limitation in rule extraction.</nextsent>
            <nextsent>
               We create minimum trees for (f , e,a) using shift-reduce (Zhang et al, 2008
               <papid>C08-1136</papid>
               ).
            </nextsent>
            <nextsent>
               Some minimum rules in the trees may be illegal according to the definition of Chiang (2007)
               <papid>P07-1005</papid>
               .
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Fast Generation.</section>
      <citcontext>
         <prevsection>
            <prevsent>That is because the translation rules with raresource/target sides always receive a very high translation probability.</prevsent>
            <prevsent>We add a prior score log(#rule)for each rule, where #rule is the number of occurrence of a rule, to reward frequent reusable rules and derivations with more rules.Finally, we may fail to create reference derivations due to the limitation in rule extraction.</prevsent>
         </prevsection>
         <citsent citstr=" C08-1136 ">
            We create minimum trees for (f , e,a) using shift-reduce (Zhang et al, 2008
            <papid>C08-1136</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Some minimum rules in the trees may be illegal according to the definition of Chiang (2007)
               <papid>P07-1005</papid>
               .
            </nextsent>
            <nextsent>We also add these rules to the rule table, so as to make sure every sentence is reachable given the rule table.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>To demonstrate the importance of scaling up the size of training data and the effect of our method, we compare three types of training configurations which differ in the size of features and data.</prevsent>
            <prevsent>MERT.</prevsent>
         </prevsection>
         <citsent citstr=" P03-1021 ">
            We use MERT (Och, 2003
            <papid>P03-1021</papid>
            ) to training 8 features on a small data.
         </citsent>
         <aftsection>
            <nextsent>
               The 8 features is the same as Chiang (2007)
               <papid>P07-1005</papid>
               including 4 rule scores (direct and reverse translation scores; direct and reverse lexical translation scores); 1 target side language model score; 3 penalties for word counts, extracted rules and glue rule.
            </nextsent>
            <nextsent>Actually, traditional pipeline often uses such configuration.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>We use the NIST evaluation sets of 2002 (MT02) as our development set, and sets of MT03/MT04/MT05 as test sets.</prevsent>
            <prevsent>Table 2 shows the statistics of all bilingual corpus.</prevsent>
         </prevsection>
         <citsent citstr=" J03-1002 ">
            We use GIZA++ (Och and Ney, 2003
            <papid>J03-1002</papid>
            ) to perform 885 System #DATA #FEAT MT03 MT04 MT05 ALL MERT 878 8 33.03 35.12 32.32 33.85 Perceptron 878 2.4K 32.89 34.88 32.55 33.76 Our Method 187K 2.0M 33.64 35.48 32.91* 34.41*519K 13.9M 34.19* 35.72* 33.09* 34.69* Improvement over MERT +1.16 +0.60 +0.77 +0.84 Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU.
         </citsent>
         <aftsection>
            <nextsent>We also compare our fast generation method with different data (only reachable or full data).</nextsent>
            <nextsent>#Data is the size of data for training the feature weights.</nextsent>
            <nextsent>
               * means significantly (Koehn, 2004
               <papid>W04-3250</papid>
               ) better than MERT (p &lt; 0.01).word alignment in both directions, and grow-diagfinal-and (Koehn et al, 2003
               <papid>N03-1017</papid>
               ) to generate symmetric word alignment.
            </nextsent>
            <nextsent>
               We extract SCFG rules as described in Chiang (2007)
               <papid>P07-1005</papid>
               and also added rules (Sec tion 4.2).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>We also compare our fast generation method with different data (only reachable or full data).</prevsent>
            <prevsent>#Data is the size of data for training the feature weights.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3250 ">
            * means significantly (Koehn, 2004
            <papid>W04-3250</papid>
            ) better than MERT (p &lt; 0.01).word alignment in both directions, and grow-diagfinal-and (Koehn et al, 2003
            <papid>N03-1017</papid>
            ) to generate symmetric word alignment.
         </citsent>
         <aftsection>
            <nextsent>
               We extract SCFG rules as described in Chiang (2007)
               <papid>P07-1005</papid>
               and also added rules (Sec tion 4.2).
            </nextsent>
            <nextsent>Our algorithm runs on the entire training data, which requires to load all the rules into the memory.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>We also compare our fast generation method with different data (only reachable or full data).</prevsent>
            <prevsent>#Data is the size of data for training the feature weights.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            * means significantly (Koehn, 2004
            <papid>W04-3250</papid>
            ) better than MERT (p &lt; 0.01).word alignment in both directions, and grow-diagfinal-and (Koehn et al, 2003
            <papid>N03-1017</papid>
            ) to generate symmetric word alignment.
         </citsent>
         <aftsection>
            <nextsent>
               We extract SCFG rules as described in Chiang (2007)
               <papid>P07-1005</papid>
               and also added rules (Sec tion 4.2).
            </nextsent>
            <nextsent>Our algorithm runs on the entire training data, which requires to load all the rules into the memory.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>#Data is the size of data for training the feature weights.</prevsent>
            <prevsent>
               * means significantly (Koehn, 2004
               <papid>W04-3250</papid>
               ) better than MERT (p &lt; 0.01).word alignment in both directions, and grow-diagfinal-and (Koehn et al, 2003
               <papid>N03-1017</papid>
               ) to generate symmetric word alignment.
            </prevsent>
         </prevsection>
         <citsent citstr=" P07-1005 ">
            We extract SCFG rules as described in Chiang (2007)
            <papid>P07-1005</papid>
            and also added rules (Sec tion 4.2).
         </citsent>
         <aftsection>
            <nextsent>Our algorithm runs on the entire training data, which requires to load all the rules into the memory.</nextsent>
            <nextsent>To fit within memory, we cut off those composed rules which only happen once in the training data.</nextsent>
            <nextsent>Here a composed rule is a rule that can be produced by any other extracted rules.</nextsent>
            <nextsent>A 4-grams language model is trained by the SRILM toolkit(Stolcke, 2002).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Minimum error rate training (Och, 2003
               <papid>P03-1021</papid>
               ) is perhaps the most popular discriminative training for SMT.
            </prevsent>
            <prevsent>However, it fails to scale to large number of features.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1096 ">
            Researchers have propose many learning algorithms to train many features: perceptron (Shen et al, 2004; Liang et al, 2006
            <papid>P06-1096</papid>
            ), minimum risk (Smith and Eisner, 2006
            <papid>P06-2101</papid>
            ; Li et al, 2009), MIRA (Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ), gradient descent (Blunsom etal., 2008; Blunsom and Osborne, 2008
            <papid>D08-1023</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data.For efficiency, we only use neighboring derivations for training.</nextsent>
            <nextsent>
               Such motivation is same as con trastive estimation (Smith and Eisner, 2005
               <papid>P05-1044</papid>
               ; Poon et al., 2009).
            </nextsent>
            <nextsent>The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution.Furthermore, we focus on how to fast generate translation forest for training.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Minimum error rate training (Och, 2003
               <papid>P03-1021</papid>
               ) is perhaps the most popular discriminative training for SMT.
            </prevsent>
            <prevsent>However, it fails to scale to large number of features.</prevsent>
         </prevsection>
         <citsent citstr=" P06-2101 ">
            Researchers have propose many learning algorithms to train many features: perceptron (Shen et al, 2004; Liang et al, 2006
            <papid>P06-1096</papid>
            ), minimum risk (Smith and Eisner, 2006
            <papid>P06-2101</papid>
            ; Li et al, 2009), MIRA (Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ), gradient descent (Blunsom etal., 2008; Blunsom and Osborne, 2008
            <papid>D08-1023</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data.For efficiency, we only use neighboring derivations for training.</nextsent>
            <nextsent>
               Such motivation is same as con trastive estimation (Smith and Eisner, 2005
               <papid>P05-1044</papid>
               ; Poon et al., 2009).
            </nextsent>
            <nextsent>The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution.Furthermore, we focus on how to fast generate translation forest for training.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Minimum error rate training (Och, 2003
               <papid>P03-1021</papid>
               ) is perhaps the most popular discriminative training for SMT.
            </prevsent>
            <prevsent>However, it fails to scale to large number of features.</prevsent>
         </prevsection>
         <citsent citstr=" D07-1080 ">
            Researchers have propose many learning algorithms to train many features: perceptron (Shen et al, 2004; Liang et al, 2006
            <papid>P06-1096</papid>
            ), minimum risk (Smith and Eisner, 2006
            <papid>P06-2101</papid>
            ; Li et al, 2009), MIRA (Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ), gradient descent (Blunsom etal., 2008; Blunsom and Osborne, 2008
            <papid>D08-1023</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data.For efficiency, we only use neighboring derivations for training.</nextsent>
            <nextsent>
               Such motivation is same as con trastive estimation (Smith and Eisner, 2005
               <papid>P05-1044</papid>
               ; Poon et al., 2009).
            </nextsent>
            <nextsent>The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution.Furthermore, we focus on how to fast generate translation forest for training.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Minimum error rate training (Och, 2003
               <papid>P03-1021</papid>
               ) is perhaps the most popular discriminative training for SMT.
            </prevsent>
            <prevsent>However, it fails to scale to large number of features.</prevsent>
         </prevsection>
         <citsent citstr=" N09-1025 ">
            Researchers have propose many learning algorithms to train many features: perceptron (Shen et al, 2004; Liang et al, 2006
            <papid>P06-1096</papid>
            ), minimum risk (Smith and Eisner, 2006
            <papid>P06-2101</papid>
            ; Li et al, 2009), MIRA (Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ), gradient descent (Blunsom etal., 2008; Blunsom and Osborne, 2008
            <papid>D08-1023</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data.For efficiency, we only use neighboring derivations for training.</nextsent>
            <nextsent>
               Such motivation is same as con trastive estimation (Smith and Eisner, 2005
               <papid>P05-1044</papid>
               ; Poon et al., 2009).
            </nextsent>
            <nextsent>The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution.Furthermore, we focus on how to fast generate translation forest for training.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Minimum error rate training (Och, 2003
               <papid>P03-1021</papid>
               ) is perhaps the most popular discriminative training for SMT.
            </prevsent>
            <prevsent>However, it fails to scale to large number of features.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1023 ">
            Researchers have propose many learning algorithms to train many features: perceptron (Shen et al, 2004; Liang et al, 2006
            <papid>P06-1096</papid>
            ), minimum risk (Smith and Eisner, 2006
            <papid>P06-2101</papid>
            ; Li et al, 2009), MIRA (Watanabe et al, 2007
            <papid>D07-1080</papid>
            ; Chiang et al, 2009
            <papid>N09-1025</papid>
            ), gradient descent (Blunsom etal., 2008; Blunsom and Osborne, 2008
            <papid>D08-1023</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The complexity of n-best lists or packed forests generation hamper these algorithms to scale to a large amount of data.For efficiency, we only use neighboring derivations for training.</nextsent>
            <nextsent>
               Such motivation is same as con trastive estimation (Smith and Eisner, 2005
               <papid>P05-1044</papid>
               ; Poon et al., 2009).
            </nextsent>
            <nextsent>The difference lies in that the previous work actually care about their latent variables (pos tags, segmentation, dependency trees, etc), while we are only interested in their marginal distribution.Furthermore, we focus on how to fast generate translation forest for training.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1081.xml">fast generation of translation forest for largescale smt discriminative training</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>The local operators lexicalize/generalize are usefor greedy decoding.</prevsent>
            <prevsent>The idea is related to pegging?</prevsent>
         </prevsection>
         <citsent citstr=" P01-1030 ">
            algorithm (Brown et al, 1993) and greedy decoding (Germann et al, 2001
            <papid>P01-1030</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Such types of local operators are also used in Gibbs sampler for synchronous grammar induction (Blunsom et al, 2009
               <papid>P09-1088</papid>
               ; Cohn and Blunsom, 2009
               <papid>D09-1037</papid>
               ).
            </nextsent>
            <nextsent>We have presented a fast generation algorithm for translation forest which contains both reference derivations and neighboring non-reference derivations for large-scale SMT discriminative training.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The reason to divide the whole task ? This study is partially supported by CERG grant 9040861 (CityU 1318/03H), CityU Strategic Research Grant 7002037.into multiple stages is two-fold, one is each sub task asks for its favorable features, the other is at the consideration of computational efficiency.</prevsent>
            <prevsent>Generally speaking, a joint system is slower than a pipeline system in training.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3212 ">
            (Xue and Palmer, 2004
            <papid>W04-3212</papid>
            ) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification.
         </citsent>
         <aftsection>
            <nextsent>
               The results from Conll shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al, 2005
               <papid>W05-0625</papid>
               ; Surdeanu et al, 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice.In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al, 2005
               <papid>J05-1004</papid>
               ).
            </nextsent>
            <nextsent>As a complement to PropBank, NomBank (Meyers et al,2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Generally speaking, a joint system is slower than a pipeline system in training.</prevsent>
            <prevsent>
               (Xue and Palmer, 2004
               <papid>W04-3212</papid>
               ) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification.
            </prevsent>
         </prevsection>
         <citsent citstr=" W05-0625 ">
            The results from Conll shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al, 2005
            <papid>W05-0625</papid>
            ; Surdeanu et al, 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice.In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al, 2005
            <papid>J05-1004</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>As a complement to PropBank, NomBank (Meyers et al,2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank.</nextsent>
            <nextsent>
               Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006
               <papid>W06-1617</papid>
               ).
            </nextsent>
            <nextsent>
               (Pustejovsky et al, 2005
               <papid>W05-0302</papid>
               ) discussed the issue of merging various treebanks, including PropBank, NomBank, and others.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Generally speaking, a joint system is slower than a pipeline system in training.</prevsent>
            <prevsent>
               (Xue and Palmer, 2004
               <papid>W04-3212</papid>
               ) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification.
            </prevsent>
         </prevsection>
         <citsent citstr=" J05-1004 ">
            The results from Conll shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al, 2005
            <papid>W05-0625</papid>
            ; Surdeanu et al, 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice.In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al, 2005
            <papid>J05-1004</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>As a complement to PropBank, NomBank (Meyers et al,2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank.</nextsent>
            <nextsent>
               Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006
               <papid>W06-1617</papid>
               ).
            </nextsent>
            <nextsent>
               (Pustejovsky et al, 2005
               <papid>W05-0302</papid>
               ) discussed the issue of merging various treebanks, including PropBank, NomBank, and others.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The results from Conll shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al, 2005
               <papid>W05-0625</papid>
               ; Surdeanu et al, 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice.In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al, 2005
               <papid>J05-1004</papid>
               ).
            </prevsent>
            <prevsent>As a complement to PropBank, NomBank (Meyers et al,2004) annotates nominal predicates and their corresponding semantic roles using similar semantic framework as PropBank.</prevsent>
         </prevsection>
         <citsent citstr=" W06-1617 ">
            Though SRL for nominal predicates offers more challenge, it draws relatively little attention (Jiang and Ng, 2006
            <papid>W06-1617</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               (Pustejovsky et al, 2005
               <papid>W05-0302</papid>
               ) discussed the issue of merging various treebanks, including PropBank, NomBank, and others.
            </nextsent>
            <nextsent>The idea of merging these two different treebanks was implemented in the CoNLL-2008 shared task (Surdeanu et al, 2008).However, few empirical studies support the necessity of an integrated learning strategy fromNomBank and PropBank.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Adaptive Argument Pruning.</section>
      <citcontext>
         <prevsection>
            <prevsent>Thus, the training sample for SRL task needs to be pruned properly.We use a simple strategy to prune predicate candidates, namely, only verbs and nouns are chosen in this case.There are two paths to collect argument candidates over the sequence.</prevsent>
            <prevsent>One is based on an input syntactic dependency tree, the other is based on a linear path of the sentence.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3212 ">
            As for the former (hereafter it is referred to synPth), we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004
            <papid>W04-3212</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The pruning algorithm is readdressed as the following.</nextsent>
            <nextsent>Initialization: Set the given predicate as the current node; (1) The current node and all of its syntactic children are selected as argument candidates (children are traversed from left to right.).</nextsent>
            <nextsent>
               (2) Reset the current node to its syntactic head and repeat step (1) until the root is reached.Note that this pruning algorithm is slightly different from that of (Xue and Palmer, 2004
               <papid>W04-3212</papid>
               ), the predicate itself is also included in the argument candidate list as the nominal predicate sometimes takes itself as its argument.
            </nextsent>
            <nextsent>The above pruning algorithm has been shown effective.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Feature Templates.</section>
      <citcontext>
         <prevsection>
            <prevsent>We explain the last item, support verb(noun).</prevsent>
            <prevsent>From a given word to the syntactic root along the syntactic tree, the first verb/noun/preposition that is met is called as itslow support verb/noun/preposition, and the nearest one to the root is called as its high support verb/noun/preposition.</prevsent>
         </prevsection>
         <citsent citstr=" W06-1617 ">
            The concept of support verb was broadly used (Toutanova et al, 2005; Xue, 2006
            <papid>N06-1055</papid>
            ; Jiang and Ng, 2006
            <papid>W06-1617</papid>
            ) 4 , we here extend it to nouns and prepositions.
         </citsent>
         <aftsection>
            <nextsent>In addition, we introduce a slightly modified syntactic head, pphead, it returns the left most sibling of a given word if the word is headed by a preposition, otherwise it returns the original head.</nextsent>
            <nextsent>Path.</nextsent>
            <nextsent>There are two basic types of path between the predicate and the argument candidates.</nextsent>
            <nextsent>One is the linear path (linePath) in the sequence, the other is the path in the syntactic parsing tree (dp Path).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Feature Templates.</section>
      <citcontext>
         <prevsection>
            <prevsent>Feature templates in this initial set are constructed in a generalized way.</prevsent>
            <prevsent>For example, if we find that a feature template a.lm.lemma was once used in some existing work, then such three templates, a.rm.lemma, a.rn.lemma, a.ln.lemma will be also added into the set.</prevsent>
         </prevsection>
         <citsent citstr=" W06-1617 ">
            As an optimal feature template subset cannot be expected to be extracted from so large a set by hand, a greedy feature selection similar to that in(Jiang and Ng, 2006
            <papid>W06-1617</papid>
            ; Ding and Chang, 2008
            <papid>D08-1034</papid>
            ) is applied.
         </citsent>
         <aftsection>
            <nextsent>The detailed algorithm is described in Algorithm 1.</nextsent>
            <nextsent>
               Assuming that the number of feature templates in a given set is n, the algorithm of (Ding and Chang, 2008
               <papid>D08-1034</papid>
               ) requires O(n 2) times of training/test routines, it cannot handle a set that consists of hundreds of templates.
            </nextsent>
            <nextsent>As the time complexity of Algorithm 1 is only O(n), it permits alarge scale feature selection accomplished by paying a reasonable time cost.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Feature Templates.</section>
      <citcontext>
         <prevsection>
            <prevsent>Feature templates in this initial set are constructed in a generalized way.</prevsent>
            <prevsent>For example, if we find that a feature template a.lm.lemma was once used in some existing work, then such three templates, a.rm.lemma, a.rn.lemma, a.ln.lemma will be also added into the set.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1034 ">
            As an optimal feature template subset cannot be expected to be extracted from so large a set by hand, a greedy feature selection similar to that in(Jiang and Ng, 2006
            <papid>W06-1617</papid>
            ; Ding and Chang, 2008
            <papid>D08-1034</papid>
            ) is applied.
         </citsent>
         <aftsection>
            <nextsent>The detailed algorithm is described in Algorithm 1.</nextsent>
            <nextsent>
               Assuming that the number of feature templates in a given set is n, the algorithm of (Ding and Chang, 2008
               <papid>D08-1034</papid>
               ) requires O(n 2) times of training/test routines, it cannot handle a set that consists of hundreds of templates.
            </nextsent>
            <nextsent>As the time complexity of Algorithm 1 is only O(n), it permits alarge scale feature selection accomplished by paying a reasonable time cost.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Feature Templates.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               As an optimal feature template subset cannot be expected to be extracted from so large a set by hand, a greedy feature selection similar to that in(Jiang and Ng, 2006
               <papid>W06-1617</papid>
               ; Ding and Chang, 2008
               <papid>D08-1034</papid>
               ) is applied.
            </prevsent>
            <prevsent>The detailed algorithm is described in Algorithm 1.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1034 ">
            Assuming that the number of feature templates in a given set is n, the algorithm of (Ding and Chang, 2008
            <papid>D08-1034</papid>
            ) requires O(n 2) times of training/test routines, it cannot handle a set that consists of hundreds of templates.
         </citsent>
         <aftsection>
            <nextsent>As the time complexity of Algorithm 1 is only O(n), it permits alarge scale feature selection accomplished by paying a reasonable time cost.</nextsent>
            <nextsent>
               Though the time complexity of the algorithm given by (Jiang and Ng, 2006
               <papid>W06-1617</papid>
               ) is also linear, it should assume all feature templates in the initial selected set good?
            </nextsent>
            <nextsent>enough and handles other feature template candidates in astrict incremental way.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Evaluation Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>We consider three types of syntactic information to feed the SRL task.</prevsent>
            <prevsent>One is gold-standard syntactic input, and other two are based on automatically parsing results of two parsers, the state-of the-art syntactic parser described in (Johansson and Nugues, 2008) 7 (it is referred to Johansson)and an integrated parser described as the following (referred to MST ME ).</prevsent>
         </prevsection>
         <citsent citstr=" W06-2932 ">
            The parser is basically based on the Mst parser 8 using all the features presented by (McDonald et al., 2006
            <papid>W06-2932</papid>
            ) with projective parsing.
         </citsent>
         <aftsection>
            <nextsent>Moreover, we exploit three types of additional features to improve the parser.</nextsent>
            <nextsent>
               1) Chen et al (2008)
               <papid>I08-1012</papid>
               used features derived from short dependency pairs basedon large-scale auto-parsed data to enhance dependency parsing.
            </nextsent>
            <nextsent>Here, the same features are used,though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Evaluation Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The parser is basically based on the Mst parser 8 using all the features presented by (McDonald et al., 2006
               <papid>W06-2932</papid>
               ) with projective parsing.
            </prevsent>
            <prevsent>Moreover, we exploit three types of additional features to improve the parser.</prevsent>
         </prevsection>
         <citsent citstr=" I08-1012 ">
            1) Chen et al (2008)
            <papid>I08-1012</papid>
            used features derived from short dependency pairs basedon large-scale auto-parsed data to enhance dependency parsing.
         </citsent>
         <aftsection>
            <nextsent>Here, the same features are used,though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data.</nextsent>
            <nextsent>
               2) Koo et al (2008)
               <papid>P08-1068</papid>
               presented new features based on word clusters obtained fromlarge-scale unlabeled data and achieved large improvement for English and Czech.
            </nextsent>
            <nextsent>Here, the same features are also used as word clusters are generated only from the training data.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Evaluation Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               1) Chen et al (2008)
               <papid>I08-1012</papid>
               used features derived from short dependency pairs basedon large-scale auto-parsed data to enhance dependency parsing.
            </prevsent>
            <prevsent>Here, the same features are used,though all dependency pairs rather than short dependency pairs are extracted along with the dependency direction from training data rather than auto-parsed data.</prevsent>
         </prevsection>
         <citsent citstr=" P08-1068 ">
            2) Koo et al (2008)
            <papid>P08-1068</papid>
            presented new features based on word clusters obtained fromlarge-scale unlabeled data and achieved large improvement for English and Czech.
         </citsent>
         <aftsection>
            <nextsent>Here, the same features are also used as word clusters are generated only from the training data.</nextsent>
            <nextsent>
               3) Nivre and McDonald (2008)
               <papid>P08-1108</papid>
               presented an integrating method to provide additional information for graph-based and transition-based parsers.
            </nextsent>
            <nextsent>Here, we represent features based on dependency relations predicted by transition-based parsers for the MSTParer.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Evaluation Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               2) Koo et al (2008)
               <papid>P08-1068</papid>
               presented new features based on word clusters obtained fromlarge-scale unlabeled data and achieved large improvement for English and Czech.
            </prevsent>
            <prevsent>Here, the same features are also used as word clusters are generated only from the training data.</prevsent>
         </prevsection>
         <citsent citstr=" P08-1108 ">
            3) Nivre and McDonald (2008)
            <papid>P08-1108</papid>
            presented an integrating method to provide additional information for graph-based and transition-based parsers.
         </citsent>
         <aftsection>
            <nextsent>Here, we represent features based on dependency relations predicted by transition-based parsers for the MSTParer.</nextsent>
            <nextsent>Forthe sake of efficiency, we use a fast transition 7 It is a 2-order maximum spanning tree parser withpseudo-projective techniques.</nextsent>
            <nextsent>A syntactic-semantic reranking was performed to output the final results according to (Jo hansson and Nugues, 2008).</nextsent>
            <nextsent>However, only 1-best outputs of the parser before reranking are used for our evaluation.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Evaluation Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>This ratio only depends on which path, either synPth or linP th, is chosen, and how good the syntactic input is (if synPth is the case).</prevsent>
            <prevsent>From the results, we see that more than a half of argument candidates can be effectively pruned for synPth and even 2/3 for linP th.</prevsent>
         </prevsection>
         <citsent citstr=" N04-1030 ">
            As mentioned by (Pradhan et al, 2004
            <papid>N04-1030</papid>
            ), argument identification plays a bottleneck role in improving the performance of a SRL system.
         </citsent>
         <aftsection>
            <nextsent>
               The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004
               <papid>W04-3212</papid>
               ).
            </nextsent>
            <nextsent>The results also indicate that such an assumption holds that arguments trend to close with their predicate, at either type of distance, syntactic or linear.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Evaluation Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>From the results, we see that more than a half of argument candidates can be effectively pruned for synPth and even 2/3 for linP th.</prevsent>
            <prevsent>
               As mentioned by (Pradhan et al, 2004
               <papid>N04-1030</papid>
               ), argument identification plays a bottleneck role in improving the performance of a SRL system.
            </prevsent>
         </prevsection>
         <citsent citstr=" W04-3212 ">
            The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004
            <papid>W04-3212</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The results also indicate that such an assumption holds that arguments trend to close with their predicate, at either type of distance, syntactic or linear.</nextsent>
            <nextsent>Based on different syntactic inputs, we obtain different results on semantic dependency parsing 36as shown in Table 5.</nextsent>
            <nextsent>These results on different syntactic inputs also give us a chance to observe how semantic performance varies according to syntactic performance.</nextsent>
            <nextsent>The fact from there sults is that the ratio Sem-F 1/LAS becomes relatively smaller as the syntactic input becomes better.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Evaluation Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>Though not so surprised, the results do show that the argument traverse scheme synPth always outperforms the other linP th.</prevsent>
            <prevsent>The result of this comparison partially shows that an integrated semantic role labeler is sensitive to the order of how argument candidates are traversed to some extent.The performance given by synPth is compared to some other systems that participated inthe CoNLL-2008 shared task.</prevsent>
         </prevsection>
         <citsent citstr=" W08-2122 ">
            They were chosen among the 20 participating systems either because they held better results (the first four partic ipants) or because they used some joint learning techniques (Henderson et al, 2008
            <papid>W08-2122</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               The results of(Titov et al, 2009) that use the similar joint learning technique as (Henderson et al, 2008
               <papid>W08-2122</papid>
               ) are also included 9 . Results of these evaluations on the test.
            </nextsent>
            <nextsent>set are in Table 6.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" D09-1004.xml">semantic dependency parsing of nombank and propbank an efficient integrated approach via a largescale feature selection</title>
      <section>Evaluation Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>In addition, the results of our system is obtained with out using additional joint learning technique like syntactic-semantic reranking.</prevsent>
            <prevsent>It indicates that our system is expected to obtain some further performance improvement by using such techniques.</prevsent>
         </prevsection>
         <citsent citstr=" W08-2122 ">
            9 In addition, the work of (Henderson et al, 2008
            <papid>W08-2122</papid>
            ) and (Titov et al, 2009) jointly considered syntactic and semantic dependencies, that is significantly different from the others.
         </citsent>
         <aftsection>
            <nextsent>We have described a dependency-based semantic role labeling system for English from NomBank and PropBank.</nextsent>
            <nextsent>From the evaluations, the result of our system is quite close to the state of the art.</nextsent>
            <nextsent>Asto our knowledge, it is the first integrated SRL system that achieves such a competitive performance against previous pipeline systems.According to the path that the word-pair classifier traverses argument candidates, two integration schemes are presented.</nextsent>
            <nextsent>Argument candidate pruning and feature selection are performed on them, respectively.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We investigate the question of learning qualia-like semantic relations that cross part-of-speech boundaries for German, by first introducing a hand-tagged dataset of associated noun-verb pairs for this task, and then provide classification results using a general framework for supervised classification of lexical relations.</prevsent>
            <prevsent>Ever since the introduction of wordnets (Millerand Fell baum, 1991) or more generally machine readable dictionaries containing semantic relations,researchers have investigated ways to learn such examples automatically from large text corpora, or generalize them from existing instances.</prevsent>
         </prevsection>
         <citsent citstr=" C92-2082 ">
            Substantial research exists on the learning of hyperonymy relations (Hearst, 1992
            <papid>C92-2082</papid>
            ; Snow et al, 2005; Tjong Kim Sang and Hofmann, 2009), meronymy relations (Hearst, 1998; Berland and Charniak, 1999
            <papid>P99-1008</papid>
            ; Girju et al, 2003) and selectional preferences (Erk et al, 2010
            <papid>J10-4007</papid>
            ; Bergsma et al, 2008
            <papid>D08-1007</papid>
            ; O?
         </citsent>
         <aftsection>
            <nextsent>
               Seaghdha, 2010).Both lexicographic research (Chaffin and Herrmann, 1987; Morris and Hirst, 2004
               <papid>W04-2607</papid>
               ) and research in cognitive psychology (Vigliocco et al, 2004; McRae et al, 2005), argue that it is important to consider relations beyond the classical inventory of hyperonymy and meronymy relations; further more psychological research on priming (Hare et al,2009) suggests different processing for different relations, which would entail that cognitively plausible modeling of human language should model these relations explicitly rather than simply recording untyped associations between concepts (as in theevocation?
            </nextsent>
            <nextsent>relation proposed for WordNet by Boyd Graber et al, 2006).</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We investigate the question of learning qualia-like semantic relations that cross part-of-speech boundaries for German, by first introducing a hand-tagged dataset of associated noun-verb pairs for this task, and then provide classification results using a general framework for supervised classification of lexical relations.</prevsent>
            <prevsent>Ever since the introduction of wordnets (Millerand Fell baum, 1991) or more generally machine readable dictionaries containing semantic relations,researchers have investigated ways to learn such examples automatically from large text corpora, or generalize them from existing instances.</prevsent>
         </prevsection>
         <citsent citstr=" P99-1008 ">
            Substantial research exists on the learning of hyperonymy relations (Hearst, 1992
            <papid>C92-2082</papid>
            ; Snow et al, 2005; Tjong Kim Sang and Hofmann, 2009), meronymy relations (Hearst, 1998; Berland and Charniak, 1999
            <papid>P99-1008</papid>
            ; Girju et al, 2003) and selectional preferences (Erk et al, 2010
            <papid>J10-4007</papid>
            ; Bergsma et al, 2008
            <papid>D08-1007</papid>
            ; O?
         </citsent>
         <aftsection>
            <nextsent>
               Seaghdha, 2010).Both lexicographic research (Chaffin and Herrmann, 1987; Morris and Hirst, 2004
               <papid>W04-2607</papid>
               ) and research in cognitive psychology (Vigliocco et al, 2004; McRae et al, 2005), argue that it is important to consider relations beyond the classical inventory of hyperonymy and meronymy relations; further more psychological research on priming (Hare et al,2009) suggests different processing for different relations, which would entail that cognitively plausible modeling of human language should model these relations explicitly rather than simply recording untyped associations between concepts (as in theevocation?
            </nextsent>
            <nextsent>relation proposed for WordNet by Boyd Graber et al, 2006).</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We investigate the question of learning qualia-like semantic relations that cross part-of-speech boundaries for German, by first introducing a hand-tagged dataset of associated noun-verb pairs for this task, and then provide classification results using a general framework for supervised classification of lexical relations.</prevsent>
            <prevsent>Ever since the introduction of wordnets (Millerand Fell baum, 1991) or more generally machine readable dictionaries containing semantic relations,researchers have investigated ways to learn such examples automatically from large text corpora, or generalize them from existing instances.</prevsent>
         </prevsection>
         <citsent citstr=" J10-4007 ">
            Substantial research exists on the learning of hyperonymy relations (Hearst, 1992
            <papid>C92-2082</papid>
            ; Snow et al, 2005; Tjong Kim Sang and Hofmann, 2009), meronymy relations (Hearst, 1998; Berland and Charniak, 1999
            <papid>P99-1008</papid>
            ; Girju et al, 2003) and selectional preferences (Erk et al, 2010
            <papid>J10-4007</papid>
            ; Bergsma et al, 2008
            <papid>D08-1007</papid>
            ; O?
         </citsent>
         <aftsection>
            <nextsent>
               Seaghdha, 2010).Both lexicographic research (Chaffin and Herrmann, 1987; Morris and Hirst, 2004
               <papid>W04-2607</papid>
               ) and research in cognitive psychology (Vigliocco et al, 2004; McRae et al, 2005), argue that it is important to consider relations beyond the classical inventory of hyperonymy and meronymy relations; further more psychological research on priming (Hare et al,2009) suggests different processing for different relations, which would entail that cognitively plausible modeling of human language should model these relations explicitly rather than simply recording untyped associations between concepts (as in theevocation?
            </nextsent>
            <nextsent>relation proposed for WordNet by Boyd Graber et al, 2006).</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We investigate the question of learning qualia-like semantic relations that cross part-of-speech boundaries for German, by first introducing a hand-tagged dataset of associated noun-verb pairs for this task, and then provide classification results using a general framework for supervised classification of lexical relations.</prevsent>
            <prevsent>Ever since the introduction of wordnets (Millerand Fell baum, 1991) or more generally machine readable dictionaries containing semantic relations,researchers have investigated ways to learn such examples automatically from large text corpora, or generalize them from existing instances.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1007 ">
            Substantial research exists on the learning of hyperonymy relations (Hearst, 1992
            <papid>C92-2082</papid>
            ; Snow et al, 2005; Tjong Kim Sang and Hofmann, 2009), meronymy relations (Hearst, 1998; Berland and Charniak, 1999
            <papid>P99-1008</papid>
            ; Girju et al, 2003) and selectional preferences (Erk et al, 2010
            <papid>J10-4007</papid>
            ; Bergsma et al, 2008
            <papid>D08-1007</papid>
            ; O?
         </citsent>
         <aftsection>
            <nextsent>
               Seaghdha, 2010).Both lexicographic research (Chaffin and Herrmann, 1987; Morris and Hirst, 2004
               <papid>W04-2607</papid>
               ) and research in cognitive psychology (Vigliocco et al, 2004; McRae et al, 2005), argue that it is important to consider relations beyond the classical inventory of hyperonymy and meronymy relations; further more psychological research on priming (Hare et al,2009) suggests different processing for different relations, which would entail that cognitively plausible modeling of human language should model these relations explicitly rather than simply recording untyped associations between concepts (as in theevocation?
            </nextsent>
            <nextsent>relation proposed for WordNet by Boyd Graber et al, 2006).</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Ever since the introduction of wordnets (Millerand Fell baum, 1991) or more generally machine readable dictionaries containing semantic relations,researchers have investigated ways to learn such examples automatically from large text corpora, or generalize them from existing instances.</prevsent>
            <prevsent>
               Substantial research exists on the learning of hyperonymy relations (Hearst, 1992
               <papid>C92-2082</papid>
               ; Snow et al, 2005; Tjong Kim Sang and Hofmann, 2009), meronymy relations (Hearst, 1998; Berland and Charniak, 1999
               <papid>P99-1008</papid>
               ; Girju et al, 2003) and selectional preferences (Erk et al, 2010
               <papid>J10-4007</papid>
               ; Bergsma et al, 2008
               <papid>D08-1007</papid>
               ; O?
            </prevsent>
         </prevsection>
         <citsent citstr=" W04-2607 ">
            Seaghdha, 2010).Both lexicographic research (Chaffin and Herrmann, 1987; Morris and Hirst, 2004
            <papid>W04-2607</papid>
            ) and research in cognitive psychology (Vigliocco et al, 2004; McRae et al, 2005), argue that it is important to consider relations beyond the classical inventory of hyperonymy and meronymy relations; further more psychological research on priming (Hare et al,2009) suggests different processing for different relations, which would entail that cognitively plausible modeling of human language should model these relations explicitly rather than simply recording untyped associations between concepts (as in theevocation?
         </citsent>
         <aftsection>
            <nextsent>relation proposed for WordNet by Boyd Graber et al, 2006).</nextsent>
            <nextsent>
               One set of suggestions for an extended inventory of relations can be found in the telic and agentive qualia relations of Pustejovsky (1991)
               <papid>J91-4003</papid>
               which have been shown to be useful in recognizing discourse relations (Wellner et al, 2006
               <papid>W06-1317</papid>
               ), or metonymy/coercion phenomena (Verspoor, 1997; Rud and Zarcone, 2011), and have the property of linking differentparts-of-speech groups, unlike meronymy and hy peronymy/troponymy.
            </nextsent>
            <nextsent>The work we present in this paper consists of adataset of noun-verb associations for German concrete nouns, which we present in more detail in section 3, and a state-of-the-art approach to the supervised classification of such cross-part-of-speech relations using informative features from large collections of unannotated text, which we present in section 4.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Seaghdha, 2010).Both lexicographic research (Chaffin and Herrmann, 1987; Morris and Hirst, 2004
               <papid>W04-2607</papid>
               ) and research in cognitive psychology (Vigliocco et al, 2004; McRae et al, 2005), argue that it is important to consider relations beyond the classical inventory of hyperonymy and meronymy relations; further more psychological research on priming (Hare et al,2009) suggests different processing for different relations, which would entail that cognitively plausible modeling of human language should model these relations explicitly rather than simply recording untyped associations between concepts (as in theevocation?
            </prevsent>
            <prevsent>relation proposed for WordNet by Boyd Graber et al, 2006).</prevsent>
         </prevsection>
         <citsent citstr=" J91-4003 ">
            One set of suggestions for an extended inventory of relations can be found in the telic and agentive qualia relations of Pustejovsky (1991)
            <papid>J91-4003</papid>
            which have been shown to be useful in recognizing discourse relations (Wellner et al, 2006
            <papid>W06-1317</papid>
            ), or metonymy/coercion phenomena (Verspoor, 1997; Rud and Zarcone, 2011), and have the property of linking differentparts-of-speech groups, unlike meronymy and hy peronymy/troponymy.
         </citsent>
         <aftsection>
            <nextsent>The work we present in this paper consists of adataset of noun-verb associations for German concrete nouns, which we present in more detail in section 3, and a state-of-the-art approach to the supervised classification of such cross-part-of-speech relations using informative features from large collections of unannotated text, which we present in section 4.</nextsent>
            <nextsent>Experimental results are discussed in section 6.</nextsent>
            <nextsent>Most of earlier work on discovering novel instances of semantic relations was based on surface pattern matching, as presented by Hearst (1998).</nextsent>
            <nextsent>In the domain of finding qualia relations, Cimiano and Wen deroth (2005) propose patterns such as ?.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Seaghdha, 2010).Both lexicographic research (Chaffin and Herrmann, 1987; Morris and Hirst, 2004
               <papid>W04-2607</papid>
               ) and research in cognitive psychology (Vigliocco et al, 2004; McRae et al, 2005), argue that it is important to consider relations beyond the classical inventory of hyperonymy and meronymy relations; further more psychological research on priming (Hare et al,2009) suggests different processing for different relations, which would entail that cognitively plausible modeling of human language should model these relations explicitly rather than simply recording untyped associations between concepts (as in theevocation?
            </prevsent>
            <prevsent>relation proposed for WordNet by Boyd Graber et al, 2006).</prevsent>
         </prevsection>
         <citsent citstr=" W06-1317 ">
            One set of suggestions for an extended inventory of relations can be found in the telic and agentive qualia relations of Pustejovsky (1991)
            <papid>J91-4003</papid>
            which have been shown to be useful in recognizing discourse relations (Wellner et al, 2006
            <papid>W06-1317</papid>
            ), or metonymy/coercion phenomena (Verspoor, 1997; Rud and Zarcone, 2011), and have the property of linking differentparts-of-speech groups, unlike meronymy and hy peronymy/troponymy.
         </citsent>
         <aftsection>
            <nextsent>The work we present in this paper consists of adataset of noun-verb associations for German concrete nouns, which we present in more detail in section 3, and a state-of-the-art approach to the supervised classification of such cross-part-of-speech relations using informative features from large collections of unannotated text, which we present in section 4.</nextsent>
            <nextsent>Experimental results are discussed in section 6.</nextsent>
            <nextsent>Most of earlier work on discovering novel instances of semantic relations was based on surface pattern matching, as presented by Hearst (1998).</nextsent>
            <nextsent>In the domain of finding qualia relations, Cimiano and Wen deroth (2005) propose patterns such as ?.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>X is used to . . .</prevsent>
            <prevsent>?, whereas they argue that agentive qualia are best chosen from a small, fixed inventory of verbs (e.g., make, bake,create . . .</prevsent>
         </prevsection>
         <citsent citstr=" P08-2047 ">
            Katrenko and Adriaans (2008
            <papid>P08-2047</papid>
            a) additionally propose to Y a (new|complete) X?
         </citsent>
         <aftsection>
            <nextsent>and a(new|complete) X has been Yd? as patterns for agentive qualia.</nextsent>
            <nextsent>Some of the more recent work starts out from matches extracted by means of such a pattern, butuse supervised training data to learn semantic constraints that improve the precision by filtering the extracted examples.</nextsent>
            <nextsent>Berland and Charniak (1999)use some handcrafted rules to exclude abstract objects from the part-of relations they extract from a corpus, and additionally rank pattern extractions by collocation strength.</nextsent>
            <nextsent>
               Girju et al (2003)
               <papid>N03-1011</papid>
               propose an iterative refinement scheme based on taxonomic information from WordNet: In this learning approach, general constraints using top-level semantic classes(entity, abstraction, causal-agent) are passed to a decision tree learner and iteratively refined until these mantic constraints induced from the classes are no longer ambiguous.Katrenko and Adriaans (2008
               <papid>P08-2047</papid>
               b, 2010) present approaches to learn semantic constraints for the use in recognizing semantic relations between word tokens (SemEval 2007 shared task, see Girju et al, 2009
               <papid>W07-2084</papid>
               ),either in a graph-based generalization of Girjus iterative refinement approach that is able to handle sense ambiguities more gracefully, or by clustering pairs of words by the joint similarity of both relation arguments.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Some of the more recent work starts out from matches extracted by means of such a pattern, butuse supervised training data to learn semantic constraints that improve the precision by filtering the extracted examples.</prevsent>
            <prevsent>Berland and Charniak (1999)use some handcrafted rules to exclude abstract objects from the part-of relations they extract from a corpus, and additionally rank pattern extractions by collocation strength.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1011 ">
            Girju et al (2003)
            <papid>N03-1011</papid>
            propose an iterative refinement scheme based on taxonomic information from WordNet: In this learning approach, general constraints using top-level semantic classes(entity, abstraction, causal-agent) are passed to a decision tree learner and iteratively refined until these mantic constraints induced from the classes are no longer ambiguous.Katrenko and Adriaans (2008
            <papid>P08-2047</papid>
            b, 2010) present approaches to learn semantic constraints for the use in recognizing semantic relations between word tokens (SemEval 2007 shared task, see Girju et al, 2009
            <papid>W07-2084</papid>
            ),either in a graph-based generalization of Girjus iterative refinement approach that is able to handle sense ambiguities more gracefully, or by clustering pairs of words by the joint similarity of both relation arguments.
         </citsent>
         <aftsection>
            <nextsent>A complementary aspect is to improving recall beyond the possibilities of a few hand-selected patterns.</nextsent>
            <nextsent>
               Following Hearst (1998), Girju et al (2003)
               <papid>N03-1011</papid>
               show that it is possible to find usable patterns by exploiting known positive examples and looking forco-occurrences of these relation arguments in a corpus.
            </nextsent>
            <nextsent>However, these patterns usually have low precision and/or very limited recall, meaning that a more elaborate approach (such as Girju et als induction of semantic constraints) is needed to make the best use of them.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Some of the more recent work starts out from matches extracted by means of such a pattern, butuse supervised training data to learn semantic constraints that improve the precision by filtering the extracted examples.</prevsent>
            <prevsent>Berland and Charniak (1999)use some handcrafted rules to exclude abstract objects from the part-of relations they extract from a corpus, and additionally rank pattern extractions by collocation strength.</prevsent>
         </prevsection>
         <citsent citstr=" P08-2047 ">
            Girju et al (2003)
            <papid>N03-1011</papid>
            propose an iterative refinement scheme based on taxonomic information from WordNet: In this learning approach, general constraints using top-level semantic classes(entity, abstraction, causal-agent) are passed to a decision tree learner and iteratively refined until these mantic constraints induced from the classes are no longer ambiguous.Katrenko and Adriaans (2008
            <papid>P08-2047</papid>
            b, 2010) present approaches to learn semantic constraints for the use in recognizing semantic relations between word tokens (SemEval 2007 shared task, see Girju et al, 2009
            <papid>W07-2084</papid>
            ),either in a graph-based generalization of Girjus iterative refinement approach that is able to handle sense ambiguities more gracefully, or by clustering pairs of words by the joint similarity of both relation arguments.
         </citsent>
         <aftsection>
            <nextsent>A complementary aspect is to improving recall beyond the possibilities of a few hand-selected patterns.</nextsent>
            <nextsent>
               Following Hearst (1998), Girju et al (2003)
               <papid>N03-1011</papid>
               show that it is possible to find usable patterns by exploiting known positive examples and looking forco-occurrences of these relation arguments in a corpus.
            </nextsent>
            <nextsent>However, these patterns usually have low precision and/or very limited recall, meaning that a more elaborate approach (such as Girju et als induction of semantic constraints) is needed to make the best use of them.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Girju et al (2003)
               <papid>N03-1011</papid>
               propose an iterative refinement scheme based on taxonomic information from WordNet: In this learning approach, general constraints using top-level semantic classes(entity, abstraction, causal-agent) are passed to a decision tree learner and iteratively refined until these mantic constraints induced from the classes are no longer ambiguous.Katrenko and Adriaans (2008
               <papid>P08-2047</papid>
               b, 2010) present approaches to learn semantic constraints for the use in recognizing semantic relations between word tokens (SemEval 2007 shared task, see Girju et al, 2009
               <papid>W07-2084</papid>
               ),either in a graph-based generalization of Girjus iterative refinement approach that is able to handle sense ambiguities more gracefully, or by clustering pairs of words by the joint similarity of both relation arguments.
            </prevsent>
            <prevsent>A complementary aspect is to improving recall beyond the possibilities of a few hand-selected pat terns.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1011 ">
            Following Hearst (1998), Girju et al (2003)
            <papid>N03-1011</papid>
            show that it is possible to find usable patterns by exploiting known positive examples and looking forco-occurrences of these relation arguments in a corpus.
         </citsent>
         <aftsection>
            <nextsent>However, these patterns usually have low precision and/or very limited recall, meaning that a more elaborate approach (such as Girju et als induction of semantic constraints) is needed to make the best use of them.</nextsent>
            <nextsent>Yamada and Baldwin (2004) propose to use acombination of templates typical of telic and agentive qualia relations (X is worth Y ing, X deservesY ing, a well-Y ed X) and a statistical ranking combining association and a classifier learned on positive and negative examples for that role.</nextsent>
            <nextsent>They find that the combination of association statistic and classification worked somewhat better than the templates alone.</nextsent>
            <nextsent>One approach targeted at exploiting a greater number of patterns for hyperonymy relations can befound in the work of Snow et al (2005): they extract patterns consisting of the shortest path in the dependency graph plus an optional satellite and usethe set of all found paths as features in a linear classifier.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>In particular, the resultsof using a linear classifier with informative corpus based features that are quite close to those that canbe achieved using a (more accurate, but computationally quite expensive) string kernel or those thatO?</prevsent>
            <prevsent>Seaghdha (2007) achieves using taxonomic information from WordNet.</prevsent>
         </prevsection>
         <citsent citstr=" C08-1114 ">
            Turney (2008)
            <papid>C08-1114</papid>
            presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wildcard.
         </citsent>
         <aftsection>
            <nextsent>Using standard machine learning tools (a support vector machine with radial base function kernel), he is able to reach results that are close to those possible with previous more specialized approaches.</nextsent>
            <nextsent>
               Similarly, Herdagdelen and Baroni (2009)
               <papid>W09-0205</papid>
               tacklea variety of problems in semantic relation classification using a unified approach where frequent unigrams and bigrams are extracted from co-occurrencecontexts of the target word pair (in addition to features extracted from general occurrence contexts of each word).
            </nextsent>
            <nextsent>Herdagdelen and Baronis approach uses a linear SVM (which is faster and better-suited to large datasets in general than either kernelizedsupport vector machines or nearest-neighbour ap proaches) yet is able to reach competitive accuracy.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Turney (2008)
               <papid>C08-1114</papid>
               presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wildcard.
            </prevsent>
            <prevsent>Using standard machine learning tools (a support vector machine with radial base function kernel), he is able to reach results that are close to those possible with previous more specialized approaches.</prevsent>
         </prevsection>
         <citsent citstr=" W09-0205 ">
            Similarly, Herdagdelen and Baroni (2009)
            <papid>W09-0205</papid>
            tacklea variety of problems in semantic relation classification using a unified approach where frequent unigrams and bigrams are extracted from co-occurrencecontexts of the target word pair (in addition to features extracted from general occurrence contexts of each word).
         </citsent>
         <aftsection>
            <nextsent>Herdagdelen and Baronis approach uses a linear SVM (which is faster and better-suited to large datasets in general than either kernelizedsupport vector machines or nearest-neighbour ap proaches) yet is able to reach competitive accuracy.</nextsent>
            <nextsent>In contrast to approaches using generic machine learning, O?</nextsent>
            <nextsent>Seaghdha and Cope stake (2009) and Nakov and Kozareva (2011) model the similarities between related word pairs more explicitly in termsof distributional kernels (O? Seaghdha and Copes take), or as a similarity metric between word pairs (Nakov and Kozareva).</nextsent>
            <nextsent>Such approaches allow more flexibility in the modeling of similarity and the combination of lexical and relational similarity measures, but are less well-suited for scaling up to more training data.1 Because of the need for sufficient training data, purely supervised approaches to learning relations 1O?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Kurc and Piasecki (2008) apply the semi-supervised approach of Pantel and Pen nachiotti (2006) for learning hyperonymy relations,but modify the patterns used to enforce morphosyntactic agreement and accommodate a more flexible word order.</prevsent>
            <prevsent>Versley (2007) uses Web pattern queries for finding hyperonymy relations and mentions the fact that greater morphological richness and the smaller size of the German Web make the use of Web queries more complex than for English.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3205 ">
            Outside the realm of hyperonymy, Regneri (2006) uses Web-based pattern search to classify verb-verb associations into the semantic classes proposed for English by Chklovski and Pantel (2004)
            <papid>W04-3205</papid>
            .
         </citsent>
         <aftsection>
            <nextsent>Rud and Zarcone (2011) perform a corpus study of patterns indicative of telic and agentive qualia relations in aGerman Web corpus, but perform no automatic classification.</nextsent>
            <nextsent>In summary, the research of Tjong Kim Sang and hofmann (2009) seems to indicate that at least hy peronymy relations can be found using a shallow pattern approach despite greater word order flexibility of languages such as Dutch and German.</nextsent>
            <nextsent>For cross-part-of-speech relations, such as telic and agentive qualia, such a question has been unaddressed as of yet, which prompted us to create a dataset that is suitable for such an investigation.</nextsent>
            <nextsent>In order to investigate general-domain Noun-Verbrelations in German, we first had to create an appropriate dataset that captures a realistic notion ofthe relationships that humans infer in a text.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Material.</section>
      <citcontext>
         <prevsection>
            <prevsent>For cross-part-of-speech relations, such as telic and agentive qualia, such a question has been unaddressed as of yet, which prompted us to create a dataset that is suitable for such an investigation.</prevsent>
            <prevsent>In order to investigate general-domain Noun-Verbrelations in German, we first had to create an appropriate dataset that captures a realistic notion ofthe relationships that humans infer in a text.</prevsent>
         </prevsection>
         <citsent citstr=" W07-2084 ">
            Existing datasets that explore this space (most of themfor English) use a variety of approaches: One approach starts from examples (such as the popular analogy dataset for English introduced by Turney and Littman, 2003); other approaches such as the data collection for the SemEval task on identifying relations between nominals (Girju et al, 2009
            <papid>W07-2084</papid>
            ; Hendrickx et al, 2010
            <papid>W09-2415</papid>
            ) start from common semantic re 14lations and use patterns to gather positive and negative examples by Web queries.In our case, we started from noun-verb associations found in a sample of human-produced associations to concrete noun stimuli (Melinger et al, 2006
            <papid>W06-2506</papid>
            ); starting from the original association data, we excluded items that were produced by less than three subjects and used the part-of-speech information attached to the data to retrieve only the verb associates.The classification scheme was motivated by existing generative lexicon research (Pustejovsky, 1991; Lenci et al, 2003), but was modeled to achieve a good fit to the associations present in the data rather than to force a good fit to any particular theory.
         </citsent>
         <aftsection>
            <nextsent>agentive relations exist between an artifact andan event that creates or procures it (e.g. bread bake) ? the telic relations exist between an entity and an event that is related to its purpose or (actual or intended) role: ? telic-artifact holds between an artifact and its intended usage (e.g. plane-fly)?</nextsent>
            <nextsent>telic-role holds between a role (i.e., a profession, organizational position etc.) and activities related to that role (e.g. cowboy ride) ? telic-bodypart holds between a body part and its intended uses (e.g. eye-see) ? the behaviour group of relations hold between an entity and events that are caused by it, but are not necessarily intentional or related to a role that it fulfills: ? behaviour-animate are typical activities performed by animate entities that are unrelated to the role that they fulfill for humans (e.g., dog-bark)?</nextsent>
            <nextsent>behaviour-artifact relates artifacts to (usu ally) unintended behaviour associated with them (e.g., moped-rattle) ? behaviour-environment relates elements of the environment to events that go on around them (e.g., sun-shine) ? location relations hold between elements of the environment and activities typically performed in or at them (e.g., mountain-climb) ? grooming relations hold between artifacts and activities that contribute to the readiness of an artifact (or body part) for its intended use but are not directly related to it (e.g., plant-water, hair-dye)In comparison to standard schemes such as SIMPLE (Lenci et al, 2003), we have extended the set of telic and agentive qualia from the original generative lexicon approach by supplementing it with relations that describe the affordances of objects or guides the interpretative linking of objects and events, namely location for affordances of elements of the environment and grooming for object-related actions that may not be necessary for a differently-built object with that same function, and finally behaviour describes events that co-occur with objects but are usually not part of a human agents action plan.</nextsent>
            <nextsent>As a refinement, we subdivided the telic qualia and behaviour relations, in particular specifying any telic relation with the reason a concrete object may be relevant for goal-directed processing ? either byteleological interpretation of body parts, by the creation of artifacts with a specific purpose, or the establishment of roles with social conventions supporting certain types of actions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Material.</section>
      <citcontext>
         <prevsection>
            <prevsent>For cross-part-of-speech relations, such as telic and agentive qualia, such a question has been unaddressed as of yet, which prompted us to create a dataset that is suitable for such an investigation.</prevsent>
            <prevsent>In order to investigate general-domain Noun-Verbrelations in German, we first had to create an appropriate dataset that captures a realistic notion ofthe relationships that humans infer in a text.</prevsent>
         </prevsection>
         <citsent citstr=" W09-2415 ">
            Existing datasets that explore this space (most of themfor English) use a variety of approaches: One approach starts from examples (such as the popular analogy dataset for English introduced by Turney and Littman, 2003); other approaches such as the data collection for the SemEval task on identifying relations between nominals (Girju et al, 2009
            <papid>W07-2084</papid>
            ; Hendrickx et al, 2010
            <papid>W09-2415</papid>
            ) start from common semantic re 14lations and use patterns to gather positive and negative examples by Web queries.In our case, we started from noun-verb associations found in a sample of human-produced associations to concrete noun stimuli (Melinger et al, 2006
            <papid>W06-2506</papid>
            ); starting from the original association data, we excluded items that were produced by less than three subjects and used the part-of-speech information attached to the data to retrieve only the verb associates.The classification scheme was motivated by existing generative lexicon research (Pustejovsky, 1991; Lenci et al, 2003), but was modeled to achieve a good fit to the associations present in the data rather than to force a good fit to any particular theory.
         </citsent>
         <aftsection>
            <nextsent>agentive relations exist between an artifact andan event that creates or procures it (e.g. bread bake) ? the telic relations exist between an entity and an event that is related to its purpose or (actual or intended) role: ? telic-artifact holds between an artifact and its intended usage (e.g. plane-fly)?</nextsent>
            <nextsent>telic-role holds between a role (i.e., a profession, organizational position etc.) and activities related to that role (e.g. cowboy ride) ? telic-bodypart holds between a body part and its intended uses (e.g. eye-see) ? the behaviour group of relations hold between an entity and events that are caused by it, but are not necessarily intentional or related to a role that it fulfills: ? behaviour-animate are typical activities performed by animate entities that are unrelated to the role that they fulfill for humans (e.g., dog-bark)?</nextsent>
            <nextsent>behaviour-artifact relates artifacts to (usu ally) unintended behaviour associated with them (e.g., moped-rattle) ? behaviour-environment relates elements of the environment to events that go on around them (e.g., sun-shine) ? location relations hold between elements of the environment and activities typically performed in or at them (e.g., mountain-climb) ? grooming relations hold between artifacts and activities that contribute to the readiness of an artifact (or body part) for its intended use but are not directly related to it (e.g., plant-water, hair-dye)In comparison to standard schemes such as SIMPLE (Lenci et al, 2003), we have extended the set of telic and agentive qualia from the original generative lexicon approach by supplementing it with relations that describe the affordances of objects or guides the interpretative linking of objects and events, namely location for affordances of elements of the environment and grooming for object-related actions that may not be necessary for a differently-built object with that same function, and finally behaviour describes events that co-occur with objects but are usually not part of a human agents action plan.</nextsent>
            <nextsent>As a refinement, we subdivided the telic qualia and behaviour relations, in particular specifying any telic relation with the reason a concrete object may be relevant for goal-directed processing ? either byteleological interpretation of body parts, by the creation of artifacts with a specific purpose, or the establishment of roles with social conventions supporting certain types of actions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Material.</section>
      <citcontext>
         <prevsection>
            <prevsent>For cross-part-of-speech relations, such as telic and agentive qualia, such a question has been unaddressed as of yet, which prompted us to create a dataset that is suitable for such an investigation.</prevsent>
            <prevsent>In order to investigate general-domain Noun-Verbrelations in German, we first had to create an appropriate dataset that captures a realistic notion ofthe relationships that humans infer in a text.</prevsent>
         </prevsection>
         <citsent citstr=" W06-2506 ">
            Existing datasets that explore this space (most of themfor English) use a variety of approaches: One approach starts from examples (such as the popular analogy dataset for English introduced by Turney and Littman, 2003); other approaches such as the data collection for the SemEval task on identifying relations between nominals (Girju et al, 2009
            <papid>W07-2084</papid>
            ; Hendrickx et al, 2010
            <papid>W09-2415</papid>
            ) start from common semantic re 14lations and use patterns to gather positive and negative examples by Web queries.In our case, we started from noun-verb associations found in a sample of human-produced associations to concrete noun stimuli (Melinger et al, 2006
            <papid>W06-2506</papid>
            ); starting from the original association data, we excluded items that were produced by less than three subjects and used the part-of-speech information attached to the data to retrieve only the verb associates.The classification scheme was motivated by existing generative lexicon research (Pustejovsky, 1991; Lenci et al, 2003), but was modeled to achieve a good fit to the associations present in the data rather than to force a good fit to any particular theory.
         </citsent>
         <aftsection>
            <nextsent>agentive relations exist between an artifact andan event that creates or procures it (e.g. bread bake) ? the telic relations exist between an entity and an event that is related to its purpose or (actual or intended) role: ? telic-artifact holds between an artifact and its intended usage (e.g. plane-fly)?</nextsent>
            <nextsent>telic-role holds between a role (i.e., a profession, organizational position etc.) and activities related to that role (e.g. cowboy ride) ? telic-bodypart holds between a body part and its intended uses (e.g. eye-see) ? the behaviour group of relations hold between an entity and events that are caused by it, but are not necessarily intentional or related to a role that it fulfills: ? behaviour-animate are typical activities performed by animate entities that are unrelated to the role that they fulfill for humans (e.g., dog-bark)?</nextsent>
            <nextsent>behaviour-artifact relates artifacts to (usu ally) unintended behaviour associated with them (e.g., moped-rattle) ? behaviour-environment relates elements of the environment to events that go on around them (e.g., sun-shine) ? location relations hold between elements of the environment and activities typically performed in or at them (e.g., mountain-climb) ? grooming relations hold between artifacts and activities that contribute to the readiness of an artifact (or body part) for its intended use but are not directly related to it (e.g., plant-water, hair-dye)In comparison to standard schemes such as SIMPLE (Lenci et al, 2003), we have extended the set of telic and agentive qualia from the original generative lexicon approach by supplementing it with relations that describe the affordances of objects or guides the interpretative linking of objects and events, namely location for affordances of elements of the environment and grooming for object-related actions that may not be necessary for a differently-built object with that same function, and finally behaviour describes events that co-occur with objects but are usually not part of a human agents action plan.</nextsent>
            <nextsent>As a refinement, we subdivided the telic qualia and behaviour relations, in particular specifying any telic relation with the reason a concrete object may be relevant for goal-directed processing ? either byteleological interpretation of body parts, by the creation of artifacts with a specific purpose, or the establishment of roles with social conventions supporting certain types of actions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Classification Approach.</section>
      <citcontext>
         <prevsection>
            <prevsent>The other relations have between 2 and 17 instances each (see table 3).</prevsent>
            <prevsent>The relationship data is therefore heavily skewed.</prevsent>
         </prevsection>
         <citsent citstr=" W09-0205 ">
            Our classification approach is aimed at a practical toolkit for supervised classification of lexical semantic relations, similar in spirit to the BagPack approach of Herdagdelen and Baroni (2009)
            <papid>W09-0205</papid>
            but adapted for the use in morphologically-rich languages, in particular German.In addition to the surface-based unigram and bigram features, we use features based on dependency syntax, which is more robust against variation in word order, and allows to reattach separable verb prefixes.
         </citsent>
         <aftsection>
            <nextsent>4.1 Preprocessing.</nextsent>
            <nextsent>To see why a very shallow approach may be less useful for German, let us consider a simple direct (ac cusative) object relation such as between aufessen (eat up) and Kuchen (cake): this relation could be realized in a variety of ways depending on clause type and constituent order, as illustrated in example (1).</nextsent>
            <nextsent>(1) a. Peter isst den Kuchen auf.</nextsent>
            <nextsent>Peter eats the cake up.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Classification Approach.</section>
      <citcontext>
         <prevsection>
            <prevsent>For the processing of this 1.7 billion word corpus, we use a pipeline that relies on deterministic dependency parsing to provide complete dependency parses at a speed that is suitable for the processing of Web-scale corpora.</prevsent>
            <prevsent>The parsing model is based on Malt parser, a transition-based parser, and uses part-of-speech and morphological information as input.</prevsent>
         </prevsection>
         <citsent citstr=" C08-1098 ">
            Morphological information is annotated using RFTagger (Schmid and Laws, 2008
            <papid>C08-1098</papid>
            ), a state-of-the-art morphological tagger based on decision trees and a large context window (which allows it to model morphological agreement more accurately than a normaltrigram-based sequence tagger).
         </citsent>
         <aftsection>
            <nextsent>While transition based parsers are quite fast in general, an SVM classifier (which is used in Malt parser by default) becomes slower with increasing training set.</nextsent>
            <nextsent>In contrast, using the Malt parser interface to LibLinear by Cassel (2009), we were able to reach a much larger speed of 55 sentences per second (against 0.4sentences per second for a more feature-rich Svm based model that reaches state of the art perfor mance).</nextsent>
            <nextsent>For lemmatization, we use the syntax-based TuBa-D/Z lemmatizer (Versley et al, 2010), which uses a separate morphological analyzer and some fallback heuristics.</nextsent>
            <nextsent>The SMOR morphology(Schmid et al, 2004) serves to provide morphological analyses for novel words, covering inflection,derivation and composition processes.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Classification Approach.</section>
      <citcontext>
         <prevsection>
            <prevsent>Hence, most results we report in the later part only use the standard SVM learner.</prevsent>
            <prevsent>4.3 Features.</prevsent>
         </prevsection>
         <citsent citstr=" W09-0205 ">
            The first group of surface-based features uses a similar technique to Herdagdelen and Baroni (2009)
            <papid>W09-0205</papid>
            : given the co-occurrences of two words X and Y with at most 4 words in-between, we extract frequent unigrams and bigrams.
         </citsent>
         <aftsection>
            <nextsent>Because we can maintain the sparsity of the resulting feature vector (see section 5), we can use a larger list of 10 000 each of the most frequent unigrams and bigrams (w12) alternatively to a list with only 2 000 entries each (w12:2k).</nextsent>
            <nextsent>The lem12 feature uses the same approach, but uses lemmas instead.</nextsent>
            <nextsent>A second group of features uses a path-basedrepresentation based on a modified version of the dependency parse (where the main verb, and not the auxiliary verb is the head of a clause and is connected to both the subject and its other arguments).</nextsent>
            <nextsent>In the path-based representation, we can extract the (shortest) path between the two target words in the dependency graph.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Classification Approach.</section>
      <citcontext>
         <prevsection>
            <prevsent>In order to emulate the feature extraction of Snow et al (2005), we introduce a relsat feature, which pairs the path (as in the rel feature) with one dependent of either target word.</prevsent>
            <prevsent>The relsat feature would be able to model patterns such as w1 and other w2?, where a modifier (other?)</prevsent>
         </prevsection>
         <citsent citstr=" P10-4004 ">
            is not part of the shortest dependency path between w1 and w2.In addition, a feature based on GermaNet (Henrich and Hin richs, 2010
            <papid>P10-4004</papid>
            ) uses taxonomic informa tion: possible hypernyms of the noun and verb in the pair are extracted, and are used by themselves (e.g. noun is a hyponym of thing?
         </citsent>
         <aftsection>
            <nextsent>?, or verb is a hyponym of communicate?</nextsent>
            <nextsent>and in combinations of up to two of these possible hypernym labels.In addition to taxonomic information from GermaNet, we use distributional similarity features for single words.</nextsent>
            <nextsent>For the nouns, we use distributional features based on the co-occurrence of pre modifying adjectives, which Versley and Panchenko(2012) found to work better than other grammatical relation-based collocates (attr1), while we use Pado?</nextsent>
            <nextsent>and Lapatas (2007) method of gathering and weighting collocates based on distance in the dependency graph for the verbs (pl2).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Classification Approach.</section>
      <citcontext>
         <prevsection>
            <prevsent>For the nouns, we use distributional features based on the co-occurrence of pre modifying adjectives, which Versley and Panchenko(2012) found to work better than other grammatical relation-based collocates (attr1), while we use Pado?</prevsent>
            <prevsent>and Lapatas (2007) method of gathering and weighting collocates based on distance in the dependency graph for the verbs (pl2).</prevsent>
         </prevsection>
         <citsent citstr=" W09-0205 ">
            Herdagdelen and Baroni (2009)
            <papid>W09-0205</papid>
            simply use a window-based approach for gathering collocates, which we reimplemented as a simpler way of capturing distributional similarity.
         </citsent>
         <aftsection>
            <nextsent>The resulting features are named w1 and w2.</nextsent>
            <nextsent>17 Seine Tante backt taglich lec keren Kuchen DET NSUBJ ADV OBJA AMOD His aunt bakes daily luscious cake his aunt bakes luscious cake every day?</nextsent>
            <nextsent>w12 Seinew2,w1 Tantew2,w1 w2 taglichw1 lem12 seinw2,w1 Tantew2,w1 w2 taglichw1 rel OBJA sat w2ADV:taglich w1AMOD:lecker w1NSUBJ:Tante triples w1 OBJAw2 relsat OBJA/w2ADV:taglich OBJA/w1AMOD:lecker OBJA/w1NSUBJ:Tante Due to the short path between w1 and w2, the triples and rel features are not very different in the example.</nextsent>
            <nextsent>In case of more complicated constructions, the triples approach would yield multiple simpler features whereas rel would yield one single complex string.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Count Transformations.</section>
      <citcontext>
         <prevsection>
            <prevsent>Figure 1: Kinds of features</prevsent>
            <prevsent>It is a well-known fact in distributional semantics that raw observation counts for context items (be they elements surrounding single word occurrences or elements extracted from the occurences of two words together) are incomparable for different target words/target pairs (since their frequency can differ)as well as for different context items.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1146 ">
            As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004
            <papid>C04-1146</papid>
            , Turney and Pantel, 2010, inter alia).
         </citsent>
         <aftsection>
            <nextsent>
               In our case, we implemented L1 normalization(which normalizes for target word frequency), a conservative estimate for pointwise mutual information (which normalizes for the frequencies of both target word and feature), and the G2 log-likelihood measure of Dunning (1993)
               <papid>J93-1003</papid>
               , which gives significance scores (i.e., numbers that invariably grow both with target and feature frequency, even if the association strength ? the relation between actual occurrences and those that would be expected when assuming no association ? is constant).
            </nextsent>
            <nextsent>In both cases, very frequent features would be emphasized in comparison to medium- and low-frequency features.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Count Transformations.</section>
      <citcontext>
         <prevsection>
            <prevsent>It is a well-known fact in distributional semantics that raw observation counts for context items (be they elements surrounding single word occurrences or elements extracted from the occurences of two words together) are incomparable for different target words/target pairs (since their frequency can differ)as well as for different context items.</prevsent>
            <prevsent>
               As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004
               <papid>C04-1146</papid>
               , Turney and Pantel, 2010, inter alia).
            </prevsent>
         </prevsection>
         <citsent citstr=" J93-1003 ">
            In our case, we implemented L1 normalization(which normalizes for target word frequency), a conservative estimate for pointwise mutual information (which normalizes for the frequencies of both target word and feature), and the G2 log-likelihood measure of Dunning (1993)
            <papid>J93-1003</papid>
            , which gives significance scores (i.e., numbers that invariably grow both with target and feature frequency, even if the association strength ? the relation between actual occurrences and those that would be expected when assuming no association ? is constant).
         </citsent>
         <aftsection>
            <nextsent>In both cases, very frequent features would be emphasized in comparison to medium- and low-frequency features.</nextsent>
            <nextsent>In the realm of supervised learning, an additional choice has to be made among learning methods that can classify words or word pairs using large feature vectors ? most commonly using nearest-neighbourclassification (Nakov and Kozareva, 2011), using custom kernels in support vector classification (O? Seaghdha and Cope stake, 2009; Turney, 2008), or by using appropriate techniques to represent the feature vectors in linear classification.In comparison to the former methods, linear classification scales better with the number of examples (where nearest-neigbour and kernel-based techniques both show strongly superlinear behaviour) and would be the method of choice for large-scale classification.</nextsent>
            <nextsent>
               Herdagdelen and Baroni (2009)
               <papid>W09-0205</papid>
               propose to map the values computed by association statistics by computing mean and standard deviation of each feature and mapping the range [2?, ?+2?] of association scores for that feature (seen over the values of that feature for all target pairs) to the range [0, 1]in the input for the classifier, clamping values out side that range to 0 or 1, respectively.
            </nextsent>
            <nextsent>Unfortunately, the approach proposed by Herdagdelen and Baroni has the property that an association score of 0 is mapped to a non-zero feature value for the classifier, which means that feature vectors are no longer sparse (i.e., instead of only storing non-zero values for context items that are informative, values for all context items have to be processed).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W12-3402.xml">supervised learning of german qualia relations</title>
      <section>Count Transformations.</section>
      <citcontext>
         <prevsection>
            <prevsent>In both cases, very frequent features would be emphasized in comparison to medium- and low-frequency features.</prevsent>
            <prevsent>In the realm of supervised learning, an additional choice has to be made among learning methods that can classify words or word pairs using large feature vectors ? most commonly using nearest-neighbourclassification (Nakov and Kozareva, 2011), using custom kernels in support vector classification (O? Seaghdha and Cope stake, 2009; Turney, 2008), or by using appropriate techniques to represent the feature vectors in linear classification.In comparison to the former methods, linear classification scales better with the number of examples (where nearest-neigbour and kernel-based techniques both show strongly superlinear behaviour) and would be the method of choice for large-scale classification.</prevsent>
         </prevsection>
         <citsent citstr=" W09-0205 ">
            Herdagdelen and Baroni (2009)
            <papid>W09-0205</papid>
            propose to map the values computed by association statistics by computing mean and standard deviation of each feature and mapping the range [2?, ?+2?] of association scores for that feature (seen over the values of that feature for all target pairs) to the range [0, 1]in the input for the classifier, clamping values out side that range to 0 or 1, respectively.
         </citsent>
         <aftsection>
            <nextsent>Unfortunately, the approach proposed by Herdagdelen and Baroni has the property that an association score of 0 is mapped to a non-zero feature value for the classifier, which means that feature vectors are no longer sparse (i.e., instead of only storing non-zero values for context items that are informative, values for all context items have to be processed).</nextsent>
            <nextsent>To keep the sparsity of the transformed counts, we always use 0 as the lower bound of the mapping(such that zero values stay zero values).</nextsent>
            <nextsent>In addition to the Herdagdelen and Baronis mean/variancebased threshold, we investigated the following possibilities for fixing the upper bound: ? MI scale: use a constant upper bound of 1 on (a conservative estimate of) the pointwise mutual information.2 2To yield a conservative MI estimate, we use the discounting factor introduced by Pantel and Lin (2002).</nextsent>
            <nextsent>The pointwise mutual information value normalizes the frequency of both words of a pair, hence all mutual information values are on a commonscale.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Our models can easily incorporate a rich set of linguistic features,and automatically learn their weights, eliminating the need for ad-hoc parameter tuning.Our methods achieve state-of-the-art correlation with human judgments on two different prediction tasks across a diverse set of standard evaluations (NIST OpenMT06,08; WMT06 08).</prevsent>
            <prevsent>Research in automatic machine translation (MT) evaluation metrics has been a key driving force behind the recent advances of statistical machine translation (SMT) systems.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1072 ">
            The early seminal work on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches (Papineni et al 2002
            <papid>C04-1072</papid>
            ; Doddington, 2002).
         </citsent>
         <aftsection>
            <nextsent>Despite their simplicity,these measures have shown good correlation with human judgments, and enabled large-scale evaluations across many different MT systems, without incurring the huge labor cost of human evaluation (Callison Burch et al(2009; 2010; 2011), inter alia).</nextsent>
            <nextsent>
               Recent studies have also confirmed that tuning MT systems against better MT metrics ? using algorithms like mert (Och, 2003
               <papid>P03-1021</papid>
               ) ? leads to better system performance (He and Way, 2009; Liu et al 2011
               <papid>D11-1035</papid>
               ).
            </nextsent>
            <nextsent>
               Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al 1990), and paraphrasing (Snover et al 2009; Denkowski and Lavie, 2010
               <papid>N10-1031</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The early seminal work on automatic MT metrics (e.g., BLEU and NIST) is largely based on n-gram matches (Papineni et al 2002
               <papid>C04-1072</papid>
               ; Doddington, 2002).
            </prevsent>
            <prevsent>Despite their simplicity,these measures have shown good correlation with human judgments, and enabled large-scale evaluations across many different MT systems, without incurring the huge labor cost of human evaluation (Callison Burch et al(2009; 2010; 2011), inter alia).</prevsent>
         </prevsection>
         <citsent citstr=" P03-1021 ">
            Recent studies have also confirmed that tuning MT systems against better MT metrics ? using algorithms like mert (Och, 2003
            <papid>P03-1021</papid>
            ) ? leads to better system performance (He and Way, 2009; Liu et al 2011
            <papid>D11-1035</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Later metrics that move beyond n-grams achieve higher accuracy and improved robustness from resources like WordNet synonyms (Miller et al 1990), and paraphrasing (Snover et al 2009; Denkowski and Lavie, 2010
               <papid>N10-1031</papid>
               ).
            </nextsent>
            <nextsent>But a common problem in these metrics is they typically resort to ad-hoc tuning methods instead of principled approaches to incorporate linguistic features.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>A FSM defines a language by acceptinga string of input tokens in the language, and rejecting those that are not.</prevsent>
            <prevsent>A probabilistic FSM defines the probability that a string is in a language, extending on the concept of a FSM.</prevsent>
         </prevsection>
         <citsent citstr=" P02-1001 ">
            Commonly used models such as HMMs, n-gram models, Markov Chains and probabilistic finite state transducers all fall in the broad family of pFSMs (Knight and Al-Onaizan, 1998; Eisner, 2002
            <papid>P02-1001</papid>
            ; Kumar and Byrne, 2003
            <papid>N03-1019</papid>
            ; Vidal et al 2005).
         </citsent>
         <aftsection>
            <nextsent>Unlike all the other applications of FSMs where tokens in the language are words, in our language tokens are edit operations.</nextsent>
            <nextsent>A string of tokens that our pFSM accepts is an edit sequence that transforms a reference translation (denoted as ref ) into a system translation (sys).</nextsent>
            <nextsent>Our pFSM has a unique start and stop state, andone state per edit operation (i.e., Insert, Delete, Sub stitution).</nextsent>
            <nextsent>The probability of an edit sequence e is generated by the model is the product of the state transition probabilities in the pFSM, formally described as: w(e | s,r) = ? |e| k=1 exp ? ?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>A FSM defines a language by acceptinga string of input tokens in the language, and rejecting those that are not.</prevsent>
            <prevsent>A probabilistic FSM defines the probability that a string is in a language, extending on the concept of a FSM.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1019 ">
            Commonly used models such as HMMs, n-gram models, Markov Chains and probabilistic finite state transducers all fall in the broad family of pFSMs (Knight and Al-Onaizan, 1998; Eisner, 2002
            <papid>P02-1001</papid>
            ; Kumar and Byrne, 2003
            <papid>N03-1019</papid>
            ; Vidal et al 2005).
         </citsent>
         <aftsection>
            <nextsent>Unlike all the other applications of FSMs where tokens in the language are words, in our language tokens are edit operations.</nextsent>
            <nextsent>A string of tokens that our pFSM accepts is an edit sequence that transforms a reference translation (denoted as ref ) into a system translation (sys).</nextsent>
            <nextsent>Our pFSM has a unique start and stop state, andone state per edit operation (i.e., Insert, Delete, Sub stitution).</nextsent>
            <nextsent>The probability of an edit sequence e is generated by the model is the product of the state transition probabilities in the pFSM, formally described as: w(e | s,r) = ? |e| k=1 exp ? ?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The regression model learns to combine these fine-grained scores more intelligently, by optimizing their weights to regress human judgments.</prevsent>
            <prevsent>See Pado et al(2009) for more discussion.</prevsent>
         </prevsection>
         <citsent citstr=" J93-2004 ">
            988combine features from multiple MT evaluation metrics (MT), as well as rich linguistic features from a textual entailment system (RTE).In all of our experiments, each reference and system translation sentence pair is tokenized using the Penn Treebank (Marcus et al 1993
            <papid>J93-2004</papid>
            ) tokenization script, and lemmatized by the Porter Stemmer (Porter,1980).
         </citsent>
         <aftsection>
            <nextsent>
               For the overall sentence structure experiment, translations are additionally part-of-speech tagged with MXPOST tagger (Ratnaparkhi, 1996), and parsed withMSTParser (McDonald et al 2005
               <papid>P05-1012</papid>
               ) 7 labeled dependency parser.
            </nextsent>
            <nextsent>
               Statistical significance tests are performed using the paired bootstrap resam pling method (Koehn, 2004
               <papid>W04-3250</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>See Pado et al(2009) for more discussion.</prevsent>
            <prevsent>
               988combine features from multiple MT evaluation metrics (MT), as well as rich linguistic features from a textual entailment system (RTE).In all of our experiments, each reference and system translation sentence pair is tokenized using the Penn Treebank (Marcus et al 1993
               <papid>J93-2004</papid>
               ) tokenization script, and lemmatized by the Porter Stemmer (Porter,1980).
            </prevsent>
         </prevsection>
         <citsent citstr=" P05-1012 ">
            For the overall sentence structure experiment, translations are additionally part-of-speech tagged with MXPOST tagger (Ratnaparkhi, 1996), and parsed withMSTParser (McDonald et al 2005
            <papid>P05-1012</papid>
            ) 7 labeled dependency parser.
         </citsent>
         <aftsection>
            <nextsent>
               Statistical significance tests are performed using the paired bootstrap resam pling method (Koehn, 2004
               <papid>W04-3250</papid>
               ).
            </nextsent>
            <nextsent>We divide our experiments into two sections, basedon two different prediction tasks ? predicting absolute scores and predicting pairwise preference.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               988combine features from multiple MT evaluation metrics (MT), as well as rich linguistic features from a textual entailment system (RTE).In all of our experiments, each reference and system translation sentence pair is tokenized using the Penn Treebank (Marcus et al 1993
               <papid>J93-2004</papid>
               ) tokenization script, and lemmatized by the Porter Stemmer (Porter,1980).
            </prevsent>
            <prevsent>
               For the overall sentence structure experiment, translations are additionally part-of-speech tagged with MXPOST tagger (Ratnaparkhi, 1996), and parsed withMSTParser (McDonald et al 2005
               <papid>P05-1012</papid>
               ) 7 labeled dependency parser.
            </prevsent>
         </prevsection>
         <citsent citstr=" W04-3250 ">
            Statistical significance tests are performed using the paired bootstrap resam pling method (Koehn, 2004
            <papid>W04-3250</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We divide our experiments into two sections, basedon two different prediction tasks ? predicting absolute scores and predicting pairwise preference.</nextsent>
            <nextsent>4.1 Exp. 1: Predicting Absolute Scores.</nextsent>
            <nextsent>The first task is to evaluate a system translation on a seven point Likert scale against a single reference.</nextsent>
            <nextsent>Higher scores indicate translations that are closer to the meaning intended by the reference.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We cansee that the pPDA extension gave modest improvements on the Urdu test set, but at a small decrease in performance on the Arabic data.</prevsent>
            <prevsent>However, for Chinese, there is a substantial gain, particularly with jump distances of five or longer.</prevsent>
         </prevsection>
         <citsent citstr=" W09-0434 ">
            This trend is even more pronounced at the long jump distance of 10, consistent with the observation that Chinese-Englishtranslations exhibit much more medium and long distance reordering than languages like Arabic (Birch et al., 2009
            <papid>W09-0434</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>4.1.2 Evaluating Linguistic Features Experimental results evaluating the benefits of each linguistic feature set are presented in Table 3.</nextsent>
            <nextsent>The first row is the pPDA model with jump distance limit 5, without other additional features.</nextsent>
            <nextsent>The next three rows are the results of adding each of the three feature sets described in Section 3.Overall, we observed that only paraphrase matching features gave a significant boost to performance.</nextsent>
            <nextsent>989 DataSet Our Metrics Baseline Metrics Combined Metrics train test pFSM pPDA pPDA+f BLEUR NISTR TERR METR MTR RTER MT+RTER A+C U 54.6 55.3 57.2 49.9 49.5 50.1 49.1 50.1 54.5 55.6 A+U C 59.9 63.8 65.8 53.9 53.1 50.3 61.1 57.3 58.0 62.7 C+U A 61.2 60.4 59.8 52.5 50.4 54.5 60.1 55.2 59.9 61.1 MT08 MT06 65.2 63.4 64.5 57.6 55.1 63.8 62.1 62.6 62.2 65.2 Table 2: Overall Comparison: Results from OpenMT08 and OpenMT06 evaluation datasets.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Features and Representation One of the findings in our experimentation is that paraphrasing helps boosting model accuracy, and the idea of using paraphrases in MT evaluation was first proposed by Zhou et al(2006).</prevsent>
            <prevsent>Several recent studies have introduced metrics over dependency parses (Liu et al 2005; Owczarzak et al 2008; He etal., 2010), but their improvements over n-gram models at the sentence level are not always consistent (Liu et al 2005; Peterson and Przybocki, 2010).</prevsent>
         </prevsection>
         <citsent citstr=" P11-1023 ">
            Other than string-based methods, recent work has explored more alternative representations for MT evaluation, such as network properties (Amancio et al 2011), semantic role structures (Lo and Wu, 2011
            <papid>P11-1023</papid>
            ), and the quality of word order (Birch and Osborne, 2011
            <papid>P11-1103</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Modeling The idea of using extended edit distance models with block movements was also explored in Leusch et al.</nextsent>
            <nextsent>(2003).</nextsent>
            <nextsent>However, their model is largely empirical and not in a probabilistic learning setting.</nextsent>
            <nextsent>The line of work on probabilistic tree-edit distance models bears a strong connection to this work (McCallumet al 2005; Bernard et al 2008; Wang and Manning, 2010; Emms, 2012).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Features and Representation One of the findings in our experimentation is that paraphrasing helps boosting model accuracy, and the idea of using paraphrases in MT evaluation was first proposed by Zhou et al(2006).</prevsent>
            <prevsent>Several recent studies have introduced metrics over dependency parses (Liu et al 2005; Owczarzak et al 2008; He etal., 2010), but their improvements over n-gram models at the sentence level are not always consistent (Liu et al 2005; Peterson and Przybocki, 2010).</prevsent>
         </prevsection>
         <citsent citstr=" P11-1103 ">
            Other than string-based methods, recent work has explored more alternative representations for MT evaluation, such as network properties (Amancio et al 2011), semantic role structures (Lo and Wu, 2011
            <papid>P11-1023</papid>
            ), and the quality of word order (Birch and Osborne, 2011
            <papid>P11-1103</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Modeling The idea of using extended edit distance models with block movements was also explored in Leusch et al.</nextsent>
            <nextsent>(2003).</nextsent>
            <nextsent>However, their model is largely empirical and not in a probabilistic learning setting.</nextsent>
            <nextsent>The line of work on probabilistic tree-edit distance models bears a strong connection to this work (McCallumet al 2005; Bernard et al 2008; Wang and Manning, 2010; Emms, 2012).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The line of work on probabilistic tree-edit distance models bears a strong connection to this work (McCallumet al 2005; Bernard et al 2008; Wang and Manning, 2010; Emms, 2012).</prevsent>
            <prevsent>In particular, our pFSMmodel and the log-linear parameterization were inspired by Wang and Manning (2010).</prevsent>
         </prevsection>
         <citsent citstr=" C96-2141 ">
            Another body of literature that is closely related to this work is FSM models for word alignment (Vogel et al 1996
            <papid>C96-2141</papid>
            ; Saers et al 2010
            <papid>N10-1050</papid>
            ; Berg-Kirkpatrick et al 2010
            <papid>N10-1083</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The stochastic Inversion Transduction Grammar in Saers et al(2010) for instance, is a pFSM with specialconstraints.</nextsent>
            <nextsent>More recently, Saers and Wu (2011) further explored the connection between Linear Transduction Grammars and FSMs.</nextsent>
            <nextsent>There is a close tie 992 between our pFSM model and the HMM model in Berg-Kirkpatrick et al(2010).</nextsent>
            <nextsent>Both models adopted a log-linear parameterization for the state transition distribution, 13 but in their case the HMM model and the pFSM arc weights are normalized locally, and the objective is non-convex.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The line of work on probabilistic tree-edit distance models bears a strong connection to this work (McCallumet al 2005; Bernard et al 2008; Wang and Manning, 2010; Emms, 2012).</prevsent>
            <prevsent>In particular, our pFSMmodel and the log-linear parameterization were inspired by Wang and Manning (2010).</prevsent>
         </prevsection>
         <citsent citstr=" N10-1050 ">
            Another body of literature that is closely related to this work is FSM models for word alignment (Vogel et al 1996
            <papid>C96-2141</papid>
            ; Saers et al 2010
            <papid>N10-1050</papid>
            ; Berg-Kirkpatrick et al 2010
            <papid>N10-1083</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The stochastic Inversion Transduction Grammar in Saers et al(2010) for instance, is a pFSM with specialconstraints.</nextsent>
            <nextsent>More recently, Saers and Wu (2011) further explored the connection between Linear Transduction Grammars and FSMs.</nextsent>
            <nextsent>There is a close tie 992 between our pFSM model and the HMM model in Berg-Kirkpatrick et al(2010).</nextsent>
            <nextsent>Both models adopted a log-linear parameterization for the state transition distribution, 13 but in their case the HMM model and the pFSM arc weights are normalized locally, and the objective is non-convex.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1090.xml">probabilistic finite state machines for regression based mt evaluation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The line of work on probabilistic tree-edit distance models bears a strong connection to this work (McCallumet al 2005; Bernard et al 2008; Wang and Manning, 2010; Emms, 2012).</prevsent>
            <prevsent>In particular, our pFSMmodel and the log-linear parameterization were inspired by Wang and Manning (2010).</prevsent>
         </prevsection>
         <citsent citstr=" N10-1083 ">
            Another body of literature that is closely related to this work is FSM models for word alignment (Vogel et al 1996
            <papid>C96-2141</papid>
            ; Saers et al 2010
            <papid>N10-1050</papid>
            ; Berg-Kirkpatrick et al 2010
            <papid>N10-1083</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The stochastic Inversion Transduction Grammar in Saers et al(2010) for instance, is a pFSM with specialconstraints.</nextsent>
            <nextsent>More recently, Saers and Wu (2011) further explored the connection between Linear Transduction Grammars and FSMs.</nextsent>
            <nextsent>There is a close tie 992 between our pFSM model and the HMM model in Berg-Kirkpatrick et al(2010).</nextsent>
            <nextsent>Both models adopted a log-linear parameterization for the state transition distribution, 13 but in their case the HMM model and the pFSM arc weights are normalized locally, and the objective is non-convex.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Response generation should also be beneficial in building chatterbots?</prevsent>
            <prevsent>(Weizenbaum, 1966) for entertainment purposes or companionship (Wilks, 2006).</prevsent>
         </prevsection>
         <citsent citstr=" N07-1022 ">
            However, we are most excited by the future potential of data-driven response generation when used inside larger dialogue systems, where direct consideration of the users utterance could be combined with dialogue state (Wong and Mooney, 2007
            <papid>N07-1022</papid>
            ; Langner et al, 2010) to generate locally coherent, purposeful dialogue.
         </citsent>
         <aftsection>
            <nextsent>In this work, we investigate statistical machine translation as an approach for response generation.</nextsent>
            <nextsent>We are motivated by the following observation: In naturally occurring discourse, there is often a strong structural relationship between adjacent utterances(Hobbs, 1985).</nextsent>
            <nextsent>For example, consider the stimulus response pair from the data: Stimulus: Im slowly making this soup ...... and it smells gorgeous!</nextsent>
            <nextsent>583 Response: Ill bet it looks delicious too!</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>As far as we are aware, this is the first work to investigate the feasibility of SMTs application to generating responses to open-domain linguistic stimuli.</prevsent>
            <prevsent>There has been a long history of chatterbots?</prevsent>
         </prevsection>
         <citsent citstr=" W10-2708 ">
            (Weizenbaum, 1966; Isbell et al, 2000; Shaikh et al., 2010
            <papid>W10-2708</papid>
            ), which attempt to engage users, typically leading the topic of conversation.
         </citsent>
         <aftsection>
            <nextsent>They usually limit interactions to a specific scenario (e.g. a Rogerian psychotherapist), and use a set of template rules for generating responses.</nextsent>
            <nextsent>In contrast, we focus on the simpler task of generating an appropriate response to a single utterance.</nextsent>
            <nextsent>
               We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic.Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000
               <papid>A00-2026</papid>
               ; Rambow et al,2001).
            </nextsent>
            <nextsent>
               Currently, most dialogue systems relyon either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004
               <papid>W04-2302</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>They usually limit interactions to a specific scenario (e.g. a Rogerian psychotherapist), and use a set of template rules for generating responses.</prevsent>
            <prevsent>In contrast, we focus on the simpler task of generating an appropriate response to a single utterance.</prevsent>
         </prevsection>
         <citsent citstr=" A00-2026 ">
            We leverage large amounts of conversational training data to scale to our Social Media domain, where conversations can be on just about any topic.Additionally, there has been work on generating more natural utterances in goal-directed dialogue systems (Ratnaparkhi, 2000
            <papid>A00-2026</papid>
            ; Rambow et al,2001).
         </citsent>
         <aftsection>
            <nextsent>
               Currently, most dialogue systems relyon either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004
               <papid>W04-2302</papid>
               ).
            </nextsent>
            <nextsent>Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al, 2010).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Currently, most dialogue systems relyon either canned responses or templates for generation, which can result in utterances which sound very unnatural in context (Chambers and Allen, 2004
               <papid>W04-2302</papid>
               ).
            </prevsent>
            <prevsent>Recent work has investigated the use of SMT in translating internal dialogue state into natural language (Langner et al, 2010).</prevsent>
         </prevsection>
         <citsent citstr=" J08-1001 ">
            In addition to dialogue state, we believe it may be beneficial to consider the users utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005
            <papid>J08-1001</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Data-driven generation based on users?</nextsent>
            <nextsent>
               utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al, 2001; Knight and Hatzivassiloglou, 1995
               <papid>P95-1034</papid>
               ).
            </nextsent>
            <nextsent>
               Statistical machine translation has been applied to a smorgasbord of NLP problems, including question answering (Echihabi and Marcu, 2003
               <papid>P03-1003</papid>
               ), semantic parsing and generation (Wong and Mooney, 2006
               <papid>N06-1056</papid>
               ; Wong and Mooney, 2007
               <papid>N07-1022</papid>
               ), summarization (DaumeIII and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al, 2010), spelling correction (Sun et al, 2010
               <papid>P10-1028</papid>
               ), paraphrase (Dolan et al,2004; Quirk et al, 2004) and query expansion (Rie zler et al, 2007).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               In addition to dialogue state, we believe it may be beneficial to consider the users utterance when generating responses in order to generate locally coherent discourse (Barzilay and Lapata, 2005
               <papid>J08-1001</papid>
               ).
            </prevsent>
            <prevsent>Data-driven generation based on users?</prevsent>
         </prevsection>
         <citsent citstr=" P95-1034 ">
            utterances might also be a useful way to fill in knowledge gaps in the system (Galley et al, 2001; Knight and Hatzivassiloglou, 1995
            <papid>P95-1034</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Statistical machine translation has been applied to a smorgasbord of NLP problems, including question answering (Echihabi and Marcu, 2003
               <papid>P03-1003</papid>
               ), semantic parsing and generation (Wong and Mooney, 2006
               <papid>N06-1056</papid>
               ; Wong and Mooney, 2007
               <papid>N07-1022</papid>
               ), summarization (DaumeIII and Marcu, 2009), generating bid-phrases in online advertising (Ravi et al, 2010), spelling correction (Sun et al, 2010
               <papid>P10-1028</papid>
               ), paraphrase (Dolan et al,2004; Quirk et al, 2004) and query expansion (Rie zler et al, 2007).
            </nextsent>
            <nextsent>
               Most relevant to our efforts is the work by Soricut and Marcu (2006)
               <papid>P06-2103</papid>
               , who applied theIBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Data.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Most relevant to our efforts is the work by Soricut and Marcu (2006)
               <papid>P06-2103</papid>
               , who applied theIBM word alignment models to a discourse ordering task, exploiting the same intuition investigated 584in this paper: certain words (or phrases) tend to trigger the usage of other words in subsequent discourse units.
            </prevsent>
            <prevsent>As far as we are aware, ours is the first workto explore the use of phrase-based translation in generating responses to open-domain linguistic stimuli,although the analogy between translation and dialogue has been drawn (Leuski and Traum, 2010).</prevsent>
         </prevsection>
         <citsent citstr=" N10-1020 ">
            For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al, 2010
            <papid>N10-1020</papid>
            ; Danescu Niculescu-Mizil et al, 2011).
         </citsent>
         <aftsection>
            <nextsent>Twitter conversations dont occur in real-time as in IRC; rather as in email, users typically take turns responding to each other.Twitters 140 character limit, however, keeps conversations chat-like.</nextsent>
            <nextsent>
               In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008
               <papid>P08-1095</papid>
               ; Wang and Oard, 2009
               <papid>N09-1023</papid>
               ).
            </nextsent>
            <nextsent>The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the authors followers (a status message).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Data.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al, 2010
               <papid>N10-1020</papid>
               ; Danescu Niculescu-Mizil et al, 2011).
            </prevsent>
            <prevsent>Twitter conversations dont occur in real-time as in IRC; rather as in email, users typically take turns responding to each other.Twitters 140 character limit, however, keeps conversations chat-like.</prevsent>
         </prevsection>
         <citsent citstr=" P08-1095 ">
            In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008
            <papid>P08-1095</papid>
            ; Wang and Oard, 2009
            <papid>N09-1023</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the authors followers (a status message).</nextsent>
            <nextsent>For the purposes of this paper, we limit the dataset to only the first two utterances from each conversation.</nextsent>
            <nextsent>As a result of this constraint, any system trained with this data will be specialized for responding to Twitter status posts.</nextsent>
            <nextsent>When applied to conversations, SMT models the probability of a response r given the input status post s using a log-linear combination of feature functions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Data.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               For learning response-generation models, we use a corpus of roughly 1.3 million conversations scraped from Twitter (Ritter et al, 2010
               <papid>N10-1020</papid>
               ; Danescu Niculescu-Mizil et al, 2011).
            </prevsent>
            <prevsent>Twitter conversations dont occur in real-time as in IRC; rather as in email, users typically take turns responding to each other.Twitters 140 character limit, however, keeps conversations chat-like.</prevsent>
         </prevsection>
         <citsent citstr=" N09-1023 ">
            In addition, the Twitter API maintains a reference from each reply to the post it responds to, so unlike IRC, there is no need for conversation disentanglement (Elsner and Charniak, 2008
            <papid>P08-1095</papid>
            ; Wang and Oard, 2009
            <papid>N09-1023</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The first message of a conversation is typically unique, not directed at any particular user but instead broadcast to the authors followers (a status message).</nextsent>
            <nextsent>For the purposes of this paper, we limit the dataset to only the first two utterances from each conversation.</nextsent>
            <nextsent>As a result of this constraint, any system trained with this data will be specialized for responding to Twitter status posts.</nextsent>
            <nextsent>When applied to conversations, SMT models the probability of a response r given the input status post s using a log-linear combination of feature functions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Response Generation as Translation.</section>
      <citcontext>
         <prevsection>
            <prevsent>Because there is a wide range of acceptable responses to any status, these identical pairs have the strongest associations in the data, and therefore dominate the phrase table.</prevsent>
            <prevsent>In order to discourage lexically similar translations, we filter out all phrase-pairs where one phrase is a substring of the other, and introduce a novel feature to penalize lexical similarity: lex(s, t) = J(s, t) Where J(s, t) is the Jaccard similarity between the set of words in s and t. 4.2 Challenge: Word Alignment.</prevsent>
         </prevsection>
         <citsent citstr=" J91-3005 ">
            Alignment is more difficult in conversational data than bilingual data (Brown et al, 1990
            <papid>J91-3005</papid>
            ), or textual entailment data (Brockett, 2006; MacCartney et al, 2008).
         </citsent>
         <aftsection>
            <nextsent>In conversational data, there are some cases in which there is a decomposable alignment between 585 if . . .</nextsent>
            <nextsent>anyones . . .</nextsent>
            <nextsent>still . . .</nextsent>
            <nextsent>awake . . .</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Response Generation as Translation.</section>
      <citcontext>
         <prevsection>
            <prevsent>words, as seen in figure 1, and some difficult cases where alignment between large phrases is required, for example figure 2.</prevsent>
            <prevsent>These difficult sentence pairs confuse the IBM word alignment models which have no way to distinguish between the easy and hard cases.</prevsent>
         </prevsection>
         <citsent citstr=" J03-1002 ">
            We aligned words in our parallel data using the widely used tool GIZA++ (Och and Ney, 2003
            <papid>J03-1002</papid>
            ); however, the standard growing heuristic resulted in very noisy alignments.
         </citsent>
         <aftsection>
            <nextsent>Precision could be improved considerably by using the intersection of GIZA++ trained in two directions (s? r, and r ? s), but the alignment also became extremely sparse.</nextsent>
            <nextsent>The average number of alignments-per status/response pair in our data was only 1.7, as compared to a dataset of aligned French-English sentence pairs (the WMT08 news commentary data) where the average number of intersection alignments is 14.</nextsent>
            <nextsent>Direct Phrase Pair Extraction Because word alignment in status/response pairs isa difficult problem, instead of relying on local alignments for extracting phrase pairs, we exploit information from all occurrences of the pair in deter min C(s, t) C(s,t) C(s) C(s, t) C(s,t) N ? C(s) C(t) N ? C(t) N Figure 3: Contingency table for phrase pair (s,t).</nextsent>
            <nextsent>Fishers Exact Test estimates the probability of seeing this event, or one more extreme assuming s and t are independent.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Response Generation as Translation.</section>
      <citcontext>
         <prevsection>
            <prevsent>Direct Phrase Pair Extraction Because word alignment in status/response pairs isa difficult problem, instead of relying on local alignments for extracting phrase pairs, we exploit information from all occurrences of the pair in deter min C(s, t) C(s,t) C(s) C(s, t) C(s,t) N ? C(s) C(t) N ? C(t) N Figure 3: Contingency table for phrase pair (s,t).</prevsent>
            <prevsent>Fishers Exact Test estimates the probability of seeing this event, or one more extreme assuming s and t are independent.</prevsent>
         </prevsection>
         <citsent citstr=" D07-1103 ">
            ing whether its phrases form a valid mapping.We consider all possible phrase-pairs in the training data,1 then use Fishers Exact Test to filter out pairs with low correlation (Johnson et al, 2007
            <papid>D07-1103</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Given a source and target phrase s and t, we consider the contingency table illustrated in figure 3, which includes co-occurrence counts for s and t, the number of sentence-pairs containing s, but not t and vice versa, in addition to the number of pairs containing neither s nor t. Fishers Exact Test provides us with an estimate of the probability of observing this table,or one more extreme, assuming s and t are indepen dent; in other words it gives us a measure of how strongly associated they are.</nextsent>
            <nextsent>In contrast to statistical tests such as 2, or the G2 Log Likelihood Ratio, Fishers Exact Test produces accurate p-values even when the expected counts are small (as is extremely common in our case).In Fishers Exact Test, the hypergeometric probability distribution is used to compute the exact probability of a particular joint frequency assuming a model of independence: C(s)!C(s)!C(t)!C(t)!</nextsent>
            <nextsent>N !C(s, t)!C(s, t)!C(s,t)!C(s,t)!The statistic is computed by summing the probability for the joint frequency in Table 3, and every more extreme joint frequency consistent with the marginal frequencies.</nextsent>
            <nextsent>
               Moore (2004)
               <papid>W04-3243</papid>
               illustrates several tricks which make this computation feasible in practice.We found that this approach generates phrase table entries which appear quite reasonable upon manual inspection.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Response Generation as Translation.</section>
      <citcontext>
         <prevsection>
            <prevsent>In contrast to statistical tests such as 2, or the G2 Log Likelihood Ratio, Fishers Exact Test produces accurate p-values even when the expected counts are small (as is extremely common in our case).In Fishers Exact Test, the hypergeometric probability distribution is used to compute the exact probability of a particular joint frequency assuming a model of independence: C(s)!C(s)!C(t)!C(t)!</prevsent>
            <prevsent>N !C(s, t)!C(s, t)!C(s,t)!C(s,t)!The statistic is computed by summing the probability for the joint frequency in Table 3, and every more extreme joint frequency consistent with the marginal frequencies.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3243 ">
            Moore (2004)
            <papid>W04-3243</papid>
            illustrates several tricks which make this computation feasible in practice.We found that this approach generates phrase table entries which appear quite reasonable upon manual inspection.
         </citsent>
         <aftsection>
            <nextsent>The top 20 phrase-pairs (after filtering out identical source/target phrases, sub strings, 1We define a possible phrase-pair as any pair of phrases found in a sentence-pair from our training corpus, where both phrases consist of 4 tokens or fewer.</nextsent>
            <nextsent>The total number of phrase pairs in a sentence pair (s, r) is O(|s| ? |r|).</nextsent>
            <nextsent>586 Source Target rt [retweet] thanks for the potter harry ice cream how are you you ? good morning chuck norris watching movie i miss miss you too are you i m my birthday happy birthday wish me luck good luck how was it was miss you i miss swine flu i love you love you too how are are you ? did you i did jackson michael how are you i m good michael mj Table 1: Top 20 Phrase Pairs ranked by the Fisher Exact Test statistic.</nextsent>
            <nextsent>Slight variations (substrings or symmetricpairs) were removed to show more variety.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Response Generation as Translation.</section>
      <citcontext>
         <prevsection>
            <prevsent>We do not use any form of SMT reordering model, as the position of the phrase in the response does not seem to be very correlated with the corresponding position in the status.</prevsent>
            <prevsent>Instead we let the language model drive reordering.</prevsent>
         </prevsection>
         <citsent citstr=" C08-1074 ">
            We used the default feature weights provided by Moses.4 Because automatic evaluation of response generation is an open problem, we avoided the use of discriminative training algorithms such as Minimum Error-Rate Training (Och, 2003
            <papid>C08-1074</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>One straightforward data-driven approach to response generation is nearest neighbour, or information retrieval.</nextsent>
            <nextsent>This general approach has been applied previously by several authors (Isbell et al, 2000; Swanson and Gordon, 2008; Jafarpour and burges, 2010), and is used as a point of comparison in our experiments.</nextsent>
            <nextsent>Given a novel status s and a training corpus of status/response pairs, two retrieval strategies can be used to return a best response r?: IR-STATUS [rargmaxi sim(s,si)] Retrieve the re-sponse ri whose associated status message si is most similar to the users input s.IR-RESPONSE [rargmaxi sim(s,ri)] Retrieve the re-sponse ri which has highest similarity when directly compared to s. At first glance, IR-STATUS may appear to be the most promising option; intuitively, if an input status is very similar to a training status, we might expect the corresponding training response to pair well with the input.</nextsent>
            <nextsent>However, as we describe in 6, it turns out that directly retrieving the most similar response (IR-RESPONSE) tends to return acceptable replies more reliably, as judged by human annotators.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>To implement our two IR response generators, we relyon the default similarity measure implemented in the Lucene5 Information Retrieval Library, which is an IDF-weighted Vector-Space similarity.</prevsent>
            <prevsent>In order to compare various approaches to automated response generation, we used human evalu 4The language model weight was set to 0.5, the translation model weights in both directions were both set to 0.2, the lexical similarity weight was set to -0.2.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1027 ">
            5http://lucene.apache.org/ 587 ators from Amazons Mechanical Turk (Snow et al, 2008
            <papid>D08-1027</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Human evaluation also provides us with data for a preliminary investigation into the feasibility of automatic evaluation metrics.</nextsent>
            <nextsent>While automated evaluation has been investigated in the area of spoken dialogue systems (Jung et al, 2009), it is unclear how well it will correlate with human judgment inopen-domain conversations where the range of possible responses is very large.</nextsent>
            <nextsent>6.1 Experimental Conditions.</nextsent>
            <nextsent>We performed pairwise comparisons of severalresponse-generation systems.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>Table 2: Summary of systems compared experimentally exact Binomial significance test; note that all differences are significant with above 95% confidence.</prevsent>
            <prevsent>Table 4 also shows the S coefficient (Bennett et al, 1954) as a measure of agreement between annotators for each experiment.</prevsent>
         </prevsection>
         <citsent citstr=" J08-4004 ">
            S is equivalent the commonly used Kappa, except it assumes the uniform distribution when computing expected or chance agreement(Artstein and Poesio, 2008
            <papid>J08-4004</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The uniform distribution is appropriate in our setup, since annotators are not told which system generated each output, and the order of choices is randomized.</nextsent>
            <nextsent>If annotators were indeed choosing randomly, they would be expected to agree in 50% of cases.</nextsent>
            <nextsent>In most comparisons the value of S is in the range of 0.2 to 0.4, which can be considered Fair agreement?</nextsent>
            <nextsent>(Landis and Koch, 1977).</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>The winning system is indicated with an asterisk?.</prevsent>
            <prevsent>All differences are significant.</prevsent>
         </prevsection>
         <citsent citstr=" W10-1703 ">
            589 of the WMT09 shared tasks (Callison-Burch et al, 2009
            <papid>W10-1703</papid>
            ).6 The results of the paired evaluations provide a clear ordering on the automatic systems: IR-STATUS is outperformed by IR-RESPONSE, which is in turn outperformed by MT-CHAT.
         </citsent>
         <aftsection>
            <nextsent>These results are somewhat surprising.</nextsent>
            <nextsent>We had expected that matching status to status would create a more natural and effective IR system, but in practice, it appears thatthe additional level of indirection employed by IRSTATUS created only more opportunity for confusion and error.</nextsent>
            <nextsent>Also, we did not necessarily expectMT-CHATs output to be preferred by human anno tators: the SMT system is the only one that generates a completely novel response, and is therefore the system most likely to make fluency errors.</nextsent>
            <nextsent>We had expected human annotators to pick up on these fluency errors, giving the the advantage to theIR systems.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D11-1054.xml">data driven response generation in social media</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>However, the human evaluation shows a clear preference for MT-CHATs output: raters favour responses that are tailored to the stimulus.</prevsent>
            <prevsent>6.3 Automatic Evaluation.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1072 ">
            The field of SMT has benefited greatly from the existence of an automatic evaluation metric,BLEU (Papineni et al, 2002
            <papid>C04-1072</papid>
            ), which grades an out put candidate according to n-gram matches to one or more reference outputs.
         </citsent>
         <aftsection>
            <nextsent>To evaluate whether BLEU is an appropriate automatic evaluation measure for response generation, we attempted to measure its agreement with the human judgments.</nextsent>
            <nextsent>We calculate BLEU using a single reference derived from our parallel corpus.</nextsent>
            <nextsent>We show the smoothed BLEU 1-4 scores for each system on each dataset evaluated in Table 4.</nextsent>
            <nextsent>Although these scores are extremely low,the overall BLEU scores agree with overall annotator judgments in all cases except when comparing MT-CHAT and IR-RESPONSE.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>First, one skeleton or backbone sentence is selected.</prevsent>
            <prevsent>Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates.</prevsent>
         </prevsection>
         <citsent citstr=" E06-1005 ">
            The alignment method is either model-based (Matusov et al, 2006
            <papid>E06-1005</papid>
            ; He et al, 2008
            <papid>D08-1011</papid>
            ) in which astatistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie,2005; Sim et al, 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al, 2006).
         </citsent>
         <aftsection>
            <nextsent>The new translation hypothesis is generated by selecting the best path through the network.We present a novel method for system combination which exploits the syntactic similarity of systemoutputs.</nextsent>
            <nextsent>
               Instead of constructing a string-based confusion network, we generate a packed forest (Billotand Lang, 1989; Mi et al, 2008
               <papid>P10-5002</papid>
               ) which encodes exponentially many parse trees in a polynomial space.
            </nextsent>
            <nextsent>The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>First, one skeleton or backbone sentence is selected.</prevsent>
            <prevsent>Then, other hypotheses are aligned against the skeleton, forming a lattice with each arc representing alternative word candidates.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1011 ">
            The alignment method is either model-based (Matusov et al, 2006
            <papid>E06-1005</papid>
            ; He et al, 2008
            <papid>D08-1011</papid>
            ) in which astatistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie,2005; Sim et al, 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al, 2006).
         </citsent>
         <aftsection>
            <nextsent>The new translation hypothesis is generated by selecting the best path through the network.We present a novel method for system combination which exploits the syntactic similarity of systemoutputs.</nextsent>
            <nextsent>
               Instead of constructing a string-based confusion network, we generate a packed forest (Billotand Lang, 1989; Mi et al, 2008
               <papid>P10-5002</papid>
               ) which encodes exponentially many parse trees in a polynomial space.
            </nextsent>
            <nextsent>The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The alignment method is either model-based (Matusov et al, 2006
               <papid>E06-1005</papid>
               ; He et al, 2008
               <papid>D08-1011</papid>
               ) in which astatistical word aligner is used to compute hypothesis alignment, or edit-based (Jayaraman and Lavie,2005; Sim et al, 2007) in which alignment is measured by an evaluation metric, such as translation error rate (TER) (Snover et al, 2006).
            </prevsent>
            <prevsent>The new translation hypothesis is generated by selecting the best path through the network.We present a novel method for system combination which exploits the syntactic similarity of systemoutputs.</prevsent>
         </prevsection>
         <citsent citstr=" P10-5002 ">
            Instead of constructing a string-based confusion network, we generate a packed forest (Billotand Lang, 1989; Mi et al, 2008
            <papid>P10-5002</papid>
            ) which encodes exponentially many parse trees in a polynomial space.
         </citsent>
         <aftsection>
            <nextsent>The packed forest, or confusion forest, is constructed by merging the MT outputs with regard to their syntactic consensus.</nextsent>
            <nextsent>We employ a grammar-basedmethod to generate the confusion forest: First, system outputs are parsed.</nextsent>
            <nextsent>Second, a set of rules are extracted from the parse trees.</nextsent>
            <nextsent>
               Third, a packed forest is generated using a variant of Earleys algorithm (Earley, 1970
               <papid>J87-1004</papid>
               ) starting from the unique root symbol.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We employ a grammar-basedmethod to generate the confusion forest: First, system outputs are parsed.</prevsent>
            <prevsent>Second, a set of rules are extracted from the parse trees.</prevsent>
         </prevsection>
         <citsent citstr=" J87-1004 ">
            Third, a packed forest is generated using a variant of Earleys algorithm (Earley, 1970
            <papid>J87-1004</papid>
            ) starting from the unique root symbol.
         </citsent>
         <aftsection>
            <nextsent>New hypotheses are selected by searching the best derivation in the forest.</nextsent>
            <nextsent>The grammar, a set of rules,is limited to those found in the parse trees.</nextsent>
            <nextsent>Spurious ambiguity during the generation step is further reduced by encoding the tree local contextual information in each non-terminal symbol, such as parent and sibling labels, using the state representation in Earleys algorithm.</nextsent>
            <nextsent>
               1249 Experiments were carried out for the system combination task of the fifth workshop on statistical machine translation (WMT10) in four directions, {Czech, French, German, Spanish}-to English (Callison-Burch et al, 2010
               <papid>W10-1703</papid>
               ), and we found comparable performance to the conventional confusion network based system combination in two language pairs, and statistically significant improvements in the others.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Combination by Confusion Network.</section>
      <citcontext>
         <prevsection>
            <prevsent>Experiments are presentedin Section 5 followed by discussion and our conclusion.</prevsent>
            <prevsent>The system combination framework based on confusion network starts from computing pairwise alignment between hypotheses by taking one hypothesis as a reference.</prevsent>
         </prevsection>
         <citsent citstr=" J03-1002 ">
            Matusov et al (2006) employs a model based approach in which a statistical word aligner, such as GIZA++ (Och and Ney, 2003
            <papid>J03-1002</papid>
            ), isused to align the hypotheses.
         </citsent>
         <aftsection>
            <nextsent>Sim et al (2007) introduced TER (Snover et al, 2006) to measure the edit-based alignment.</nextsent>
            <nextsent>Then, one hypothesis is selected, for example by employing a minimum Bayes risk criterion (Sim et al., 2007), as a skeleton, or a backbone, which servesas a building block for aligning the rest of the hypotheses.</nextsent>
            <nextsent>Other hypotheses are aligned against the skeleton using the pairwise alignment.</nextsent>
            <nextsent>Figure 1(b)illustrates an example of a confusion network constructed from the four hypotheses in Figure 1(a), assuming the first hypothesis is selected as our skeleton.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Combination by Confusion Network.</section>
      <citcontext>
         <prevsection>
            <prevsent>in the third hypothesis is aligned with forest?</prevsent>
            <prevsent>in the skeleton.</prevsent>
         </prevsection>
         <citsent citstr=" W08-0329 ">
            Rosti et al (2008)
            <papid>W08-0329</papid>
            introduces an incremental method so that hypotheses are aligned incrementally to the growing confusion network, not only the . . ..* ..I ..saw ..the . ..forest . . .
         </citsent>
         <aftsection>
            <nextsent>..I ..walked ..the ..blue ..forest . . .</nextsent>
            <nextsent>..I ..saw ..the . ..green ..trees . . .</nextsent>
            <nextsent>..the . ..forest ..was ..found (a) Pairwise alignment using the first starred hypothesis as a skeleton.</nextsent>
            <nextsent>.I .?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Combination by Confusion Forest.</section>
      <citcontext>
         <prevsection>
            <prevsent>.blue . ..green . ..NN ..forest . ..trees . ..DT@2.2.1 . ..the . ..NN@2.2.2 . ..forest . ..VP @2 . ..S @Figure 2: An example packed forest representing hypotheses in Figure 1(a).</prevsent>
            <prevsent>ments among parse trees.</prevsent>
         </prevsection>
         <citsent citstr=" W05-1506 ">
            The forest is represented as a hypergraph which is exploited in parsing (Klein and Manning, 2001; Huang and Chiang, 2005
            <papid>W05-1506</papid>
            ) and machine translation (Chiang, 2007
            <papid>J07-2003</papid>
            ; Huang and Chiang, 2007).
         </citsent>
         <aftsection>
            <nextsent>More formally, a hypergraph is a pair V,Ewhere V is the set of nodes and E is the set of hyperedges.</nextsent>
            <nextsent>Each node in V is represented as X@p where X ? N is a non-terminal symbol and pis an address (Shieber et al, 1995) that encapsulates each node id relative to its parent.</nextsent>
            <nextsent>The root node is given the address ? and the address of the first child of node p is given p.1.</nextsent>
            <nextsent>Each hyperedge e ? E is represented as a pair head(e), tails(e)?</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Combination by Confusion Forest.</section>
      <citcontext>
         <prevsection>
            <prevsent>.blue . ..green . ..NN ..forest . ..trees . ..DT@2.2.1 . ..the . ..NN@2.2.2 . ..forest . ..VP @2 . ..S @Figure 2: An example packed forest representing hypotheses in Figure 1(a).</prevsent>
            <prevsent>ments among parse trees.</prevsent>
         </prevsection>
         <citsent citstr=" J07-2003 ">
            The forest is represented as a hypergraph which is exploited in parsing (Klein and Manning, 2001; Huang and Chiang, 2005
            <papid>W05-1506</papid>
            ) and machine translation (Chiang, 2007
            <papid>J07-2003</papid>
            ; Huang and Chiang, 2007).
         </citsent>
         <aftsection>
            <nextsent>More formally, a hypergraph is a pair V,Ewhere V is the set of nodes and E is the set of hyperedges.</nextsent>
            <nextsent>Each node in V is represented as X@p where X ? N is a non-terminal symbol and pis an address (Shieber et al, 1995) that encapsulates each node id relative to its parent.</nextsent>
            <nextsent>The root node is given the address ? and the address of the first child of node p is given p.1.</nextsent>
            <nextsent>Each hyperedge e ? E is represented as a pair head(e), tails(e)?</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Combination by Confusion Forest.</section>
      <citcontext>
         <prevsection>
            <prevsent>Third, a forest is generated from the unique root symbol of the extracted grammar through non-terminal rewriting.</prevsent>
            <prevsent>3.1 Forest Generation.</prevsent>
         </prevsection>
         <citsent citstr=" J87-1004 ">
            Given the extracted grammar, we apply a variant ofEarleys algorithm (Earley, 1970
            <papid>J87-1004</papid>
            ) which can generate strings in a left-to-right manner from the unique root symbol, TOP.
         </citsent>
         <aftsection>
            <nextsent>
               Figure 3 presents the deductive inference rules (Goodman, 1999
               <papid>J99-4004</papid>
               ) for our generation algorithm.
            </nextsent>
            <nextsent>We use capital letters X ? N to denote non-terminals and x ? T for terminals.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Combination by Confusion Forest.</section>
      <citcontext>
         <prevsection>
            <prevsent>3.1 Forest Generation.</prevsent>
            <prevsent>
               Given the extracted grammar, we apply a variant ofEarleys algorithm (Earley, 1970
               <papid>J87-1004</papid>
               ) which can generate strings in a left-to-right manner from the unique root symbol, TOP.
            </prevsent>
         </prevsection>
         <citsent citstr=" J99-4004 ">
            Figure 3 presents the deductive inference rules (Goodman, 1999
            <papid>J99-4004</papid>
            ) for our generation algorithm.
         </citsent>
         <aftsection>
            <nextsent>We use capital letters X ? N to denote non-terminals and x ? T for terminals.</nextsent>
            <nextsent>Lower case Greek letters ?, ? and ? are strings of terminals andnon-terminals (T ? N )?.</nextsent>
            <nextsent>u and v are weights associated with each item.The major difference compared to Earleys parsing algorithm is that we ignore the terminal span information each non-terminal covers and keep track of the height of derivations by h. The scanning step will always succeed by moving the dot to the right.</nextsent>
            <nextsent>Combined with the prediction and completion steps, our algorithm may potentially generate a spu riously deep forest.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Combination by Confusion Forest.</section>
      <citcontext>
         <prevsection>
            <prevsent>Likewise, constraining the vertical order generates a deeper forest by ignoring the sequence of symbols leading to a particular node.</prevsent>
            <prevsent>3.3 Forest Rescoring.</prevsent>
         </prevsection>
         <citsent citstr=" W05-1506 ">
            From the packed forest F , new k-best derivations are extracted from all possible derivations D by efficient forest-based algorithms for k-best parsing(Huang and Chiang, 2005
            <papid>W05-1506</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We use a linear combi . . .S . . .NP ..PRP . ..I . ..VP . . .VBD . ..saw . ..NP ..DT ..the . ..NN ..forest (a) A parse tree for I saw the forest?</nextsent>
            <nextsent>.S . . .</nextsent>
            <nextsent>S+ ? NP : VP . . S + ? NP : VP + ? PRP . . .I . . .</nextsent>
            <nextsent>S+NP : VP . . .</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Then, k-best derivations are extracted from the rescored forest using algorithm 3 of Huang and Chiang (2005).</prevsent>
            <prevsent>Consensus translations have been extensively studied with many granularities.</prevsent>
         </prevsection>
         <citsent citstr=" P04-1063 ">
            One of the simplest forms is a sentence-based combination in which hypotheses are simply reranked without merging (Nomoto, 2004
            <papid>P04-1063</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Frederking and Nirenburg (1994)
               <papid>A94-1016</papid>
               1252proposed a phrasal combination by merging hypotheses in a chart structure, while others depended on confusion networks, or similar structures, as a building block for merging hypotheses at the word level (Bangalore et al, 2001
               <papid>D07-1105</papid>
               ; Matusov et al, 2006
               <papid>E06-1005</papid>
               ; He et al, 2008
               <papid>D08-1011</papid>
               ; Jayaraman and Lavie, 2005
               <papid>P05-3026</papid>
               ; Simet al, 2007).
            </nextsent>
            <nextsent>Our work is the first to explicitly exploit syntactic similarity for system combination by merging hypotheses into a syntactic packed forest.The confusion forest approach may suffer from parsing errors such as the confusion network construction influenced by alignment errors.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>New translations are generated by decoding the source sentence again using the newly extracted phrase table.</prevsent>
            <prevsent>Ourgrammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding.</prevsent>
         </prevsection>
         <citsent citstr=" A00-2023 ">
            In terms of generation, our approach is an instance of statistical generation (Langkilde and knight, 1998; Langkilde, 2000
            <papid>A00-2023</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Instead of generating forests from semantic representations (Langkilde, 2000
               <papid>A00-2023</papid>
               ), we generate forests from a CFG encoding the consensus among parsed hypotheses.
            </nextsent>
            <nextsent>
               Liu et al (2009)
               <papid>P09-1065</papid>
               present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Ourgrammar-based approach can be regarded as a regeneration approach in which an off-the-shelf monolingual parser, instead of a word aligner, is used to annotate syntactic information to each hypothesis, then, a new translation is generated from the merged forest, not from the input source sentence through decoding.</prevsent>
            <prevsent>
               In terms of generation, our approach is an instance of statistical generation (Langkilde and knight, 1998; Langkilde, 2000
               <papid>A00-2023</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" A00-2023 ">
            Instead of generating forests from semantic representations (Langkilde, 2000
            <papid>A00-2023</papid>
            ), we generate forests from a CFG encoding the consensus among parsed hypotheses.
         </citsent>
         <aftsection>
            <nextsent>
               Liu et al (2009)
               <papid>P09-1065</papid>
               present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs.
            </nextsent>
            <nextsent>Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both sys tems.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               In terms of generation, our approach is an instance of statistical generation (Langkilde and knight, 1998; Langkilde, 2000
               <papid>A00-2023</papid>
               ).
            </prevsent>
            <prevsent>
               Instead of generating forests from semantic representations (Langkilde, 2000
               <papid>A00-2023</papid>
               ), we generate forests from a CFG encoding the consensus among parsed hypotheses.
            </prevsent>
         </prevsection>
         <citsent citstr=" P09-1065 ">
            Liu et al (2009)
            <papid>P09-1065</papid>
            present joint decoding in which a translation forest is constructed from two distinct MT systems, tree-to-string and string-to-string, by merging forest outputs.
         </citsent>
         <aftsection>
            <nextsent>Their merging method is either translation-level in which no new translation is generated, or derivation-level in that the rules sharing the same left-hand-side are used in both systems.</nextsent>
            <nextsent>While our work is similar in that a new forestis constructed by sharing rules among systems, although their work involves no consensus translation and requires structures internal to each system such as model combinations (DeNero et al, 2010).</nextsent>
            <nextsent>cz-en de-en es-en fr-en # of systems 6 16 8 14 avg.</nextsent>
            <nextsent>words tune 10.6K 10.9K 10.9K 11.0K test 50.5K 52.1K 52.1K 52.4K sentences tune 455 test 2,034 Table 1: WMT10 system combination tuning/testing data</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>words tune 10.6K 10.9K 10.9K 11.0K test 50.5K 52.1K 52.1K 52.4K sentences tune 455 test 2,034 Table 1: WMT10 system combination tuning/testing data</prevsent>
            <prevsent>5.1 Setup.</prevsent>
         </prevsection>
         <citsent citstr=" W10-1703 ">
            We ran our experiments for the WMT10 system combination task usinge four language pairs, {Czech, French, German, Spanish}-to-English(Callison-Burch et al, 2010
            <papid>W10-1703</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The data is summarized in Table 1.</nextsent>
            <nextsent>
               The system outputs are retok enized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003
               <papid>P03-1054</papid>
               ), and lower-cased.We implemented our confusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009
               <papid>E09-1061</papid>
               ), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </nextsent>
            <nextsent>Input to our system is a collection of hypergraphs,a set of parsed hypotheses, from which rules are extracted and a new forest is generated as described in Section 3.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               We ran our experiments for the WMT10 system combination task usinge four language pairs, {Czech, French, German, Spanish}-to-English(Callison-Burch et al, 2010
               <papid>W10-1703</papid>
               ).
            </prevsent>
            <prevsent>The data is summarized in Table 1.</prevsent>
         </prevsection>
         <citsent citstr=" P03-1054 ">
            The system outputs are retok enized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003
            <papid>P03-1054</papid>
            ), and lower-cased.We implemented our confusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009
            <papid>E09-1061</papid>
            ), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007
            <papid>J07-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Input to our system is a collection of hypergraphs,a set of parsed hypotheses, from which rules are extracted and a new forest is generated as described in Section 3.</nextsent>
            <nextsent>Our baseline, also implemented in cicada, is a confusion network-based system combination method (2) which incrementally aligns hypotheses to the growing network using TER (Rosti et al, 2008) and merges multiple networks into alarge single network.</nextsent>
            <nextsent>After performing epsilon removal, the network is transformed into a forest by parsing with monotone rules of S ? X, S ? S X and X ? x. k-best translations are extracted fromthe forest using the forest-based algorithms in Section 3.3.</nextsent>
            <nextsent>5.2 Features.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               We ran our experiments for the WMT10 system combination task usinge four language pairs, {Czech, French, German, Spanish}-to-English(Callison-Burch et al, 2010
               <papid>W10-1703</papid>
               ).
            </prevsent>
            <prevsent>The data is summarized in Table 1.</prevsent>
         </prevsection>
         <citsent citstr=" E09-1061 ">
            The system outputs are retok enized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003
            <papid>P03-1054</papid>
            ), and lower-cased.We implemented our confusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009
            <papid>E09-1061</papid>
            ), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007
            <papid>J07-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Input to our system is a collection of hypergraphs,a set of parsed hypotheses, from which rules are extracted and a new forest is generated as described in Section 3.</nextsent>
            <nextsent>Our baseline, also implemented in cicada, is a confusion network-based system combination method (2) which incrementally aligns hypotheses to the growing network using TER (Rosti et al, 2008) and merges multiple networks into alarge single network.</nextsent>
            <nextsent>After performing epsilon removal, the network is transformed into a forest by parsing with monotone rules of S ? X, S ? S X and X ? x. k-best translations are extracted fromthe forest using the forest-based algorithms in Section 3.3.</nextsent>
            <nextsent>5.2 Features.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               We ran our experiments for the WMT10 system combination task usinge four language pairs, {Czech, French, German, Spanish}-to-English(Callison-Burch et al, 2010
               <papid>W10-1703</papid>
               ).
            </prevsent>
            <prevsent>The data is summarized in Table 1.</prevsent>
         </prevsection>
         <citsent citstr=" J07-2003 ">
            The system outputs are retok enized to match the Penn-treebank standard, parsed by the Stanford Parser (Klein and Manning, 2003
            <papid>P03-1054</papid>
            ), and lower-cased.We implemented our confusion forest system combination using an in-house developed hypergraph-based toolkit cicada which is motivated by generic weighted logic programming (Lopez, 2009
            <papid>E09-1061</papid>
            ), originally developed for a synchronous-CFG based machine translation system (Chiang, 2007
            <papid>J07-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Input to our system is a collection of hypergraphs,a set of parsed hypotheses, from which rules are extracted and a new forest is generated as described in Section 3.</nextsent>
            <nextsent>Our baseline, also implemented in cicada, is a confusion network-based system combination method (2) which incrementally aligns hypotheses to the growing network using TER (Rosti et al, 2008) and merges multiple networks into alarge single network.</nextsent>
            <nextsent>After performing epsilon removal, the network is transformed into a forest by parsing with monotone rules of S ? X, S ? S X and X ? x. k-best translations are extracted fromthe forest using the forest-based algorithms in Section 3.3.</nextsent>
            <nextsent>5.2 Features.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>After performing epsilon removal, the network is transformed into a forest by parsing with monotone rules of S ? X, S ? S X and X ? x. k-best translations are extracted fromthe forest using the forest-based algorithms in Section 3.3.</prevsent>
            <prevsent>5.2 Features.</prevsent>
         </prevsection>
         <citsent citstr=" P09-1019 ">
            The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al, 2009
            <papid>P09-1019</papid>
            ).We use three lower-cased 5-gram language mod 1253 els hilm(d): English Gigaword Fourth edition1, the English side of French-English 109 corpus and the news commentary English data2.
         </citsent>
         <aftsection>
            <nextsent>The count based features ht(d) and he(d) count the number of terminals and the number of hyperedges in d, respectively.</nextsent>
            <nextsent>
               We employ M confidence measures hms (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al, 2007a).Following Macherey and Och (2007)
               <papid>D07-1105</papid>
               , BLEU (Pa pineni et al, 2002) correlations are also incorporated in our system combination.
            </nextsent>
            <nextsent>GivenM system outputs e1...eM , M BLEU scores are computed for d using each of the system outputs em as a reference hmb (d) = BP (e, em) ? exp ( 1 4 4 ? n=1 log n(e, em) ) where e = yield(d) is a terminal yield of d, BP (?)</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The feature weight vector w in Equation 1 is tuned by MERT over hypergraphs (Kumar et al, 2009
               <papid>P09-1019</papid>
               ).We use three lower-cased 5-gram language mod 1253 els hilm(d): English Gigaword Fourth edition1, the English side of French-English 109 corpus and the news commentary English data2.
            </prevsent>
            <prevsent>The count based features ht(d) and he(d) count the number of terminals and the number of hyperedges in d, respec tively.</prevsent>
         </prevsection>
         <citsent citstr=" D07-1105 ">
            We employ M confidence measures hms (d) for M systems, which basically count the number of rules used in d originally extracted from mth system hypothesis (Rosti et al, 2007a).Following Macherey and Och (2007)
            <papid>D07-1105</papid>
            , BLEU (Pa pineni et al, 2002) correlations are also incorporated in our system combination.
         </citsent>
         <aftsection>
            <nextsent>GivenM system outputs e1...eM , M BLEU scores are computed for d using each of the system outputs em as a reference hmb (d) = BP (e, em) ? exp ( 1 4 4 ? n=1 log n(e, em) ) where e = yield(d) is a terminal yield of d, BP (?)</nextsent>
            <nextsent>and n(?)</nextsent>
            <nextsent>respectively denote brevity penalty andn-gram precision.</nextsent>
            <nextsent>
               Here, we use approximated unclipped n-gram counts (Dreyer et al, 2007
               <papid>W07-0414</papid>
               ) for computing n(?)
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P11-1125.xml">machine translation system combination by confusion forest</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>and n(?)</prevsent>
            <prevsent>respectively denote brevity penalty andn-gram precision.</prevsent>
         </prevsection>
         <citsent citstr=" W07-0414 ">
            Here, we use approximated unclipped n-gram counts (Dreyer et al, 2007
            <papid>W07-0414</papid>
            ) for computing n(?)
         </citsent>
         <aftsection>
            <nextsent>
               with a compact state representation (Li and Khudanpur, 2009
               <papid>N09-2003</papid>
               ).Our baseline confusion network system has an additional penalty feature, hp(m), which is the total edits required to construct a confusion network using themth system hypothesis as a skeleton, normalized by the number of nodes in the network (Rosti et al., 2007b).
            </nextsent>
            <nextsent>5.3 Results.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The lack of simple deterministic rules for mapping syntax to shallow semantics motivates the use of statistical methods.</prevsent>
            <prevsent>Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters.</prevsent>
         </prevsection>
         <citsent citstr=" C10-2107 ">
            These resources are scarce and expensive to create, and even the largest of them have low coverage (Palmer and Sporleder, 2010
            <papid>C10-2107</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Moreover, these models are domain-specific, and their performance drops substantially when they are used in a new domain (Pradhan et al 2008
               <papid>J08-2006</papid>
               ).
            </nextsent>
            <nextsent>Such domain specificity is arguably unavoidable for a semantic analyzer, as even the definitions of semantic roles are typically predicate specific, and different domains can have radically different distributions of predicates (and their senses).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Although current statistical approaches have been successful in predicting shallow semantic representations, they typically require large amounts of annotated data to estimate model parameters.</prevsent>
            <prevsent>
               These resources are scarce and expensive to create, and even the largest of them have low coverage (Palmer and Sporleder, 2010
               <papid>C10-2107</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" J08-2006 ">
            Moreover, these models are domain-specific, and their performance drops substantially when they are used in a new domain (Pradhan et al 2008
            <papid>J08-2006</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Such domain specificity is arguably unavoidable for a semantic analyzer, as even the definitions of semantic roles are typically predicate specific, and different domains can have radically different distributions of predicates (and their senses).</nextsent>
            <nextsent>The necessity for a large amounts of human-annotated data for every language and domain is one of the major obstacles to the wide-spread adoption of semantic role representations.These challenges motivate the need for unsupervised methods which, instead of relying on labeled data, can exploit large amounts of unlabeledtexts.</nextsent>
            <nextsent>In this paper, we propose simple and effi 12 cient hierarchical Bayesian models for this task.</nextsent>
            <nextsent>It is natural to split the SRL task into twostages: the identification of arguments (the identification stage) and the assignment of semantic roles (the labeling stage).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction</title>
      <section>Empirical Evaluation.</section>
      <citcontext>
         <prevsection>
            <prevsent>We evaluate our models on gold and automatically generated parses, and use either gold PropBank annotations or the heuristic from Section 2 to identify arguments, resulting in four experimental regimes.</prevsent>
            <prevsent>In order to reduce the sparsity of predicate argument fillers we consider replacing lemmas of their syntactic heads with word clusters induced by a clustering algorithm as a preprocessing step.</prevsent>
         </prevsection>
         <citsent citstr=" J92-4003 ">
            In particular, we use Brown (Br) clustering (Brown et al 1992
            <papid>J92-4003</papid>
            ) induced over RCV1 corpus (Turian et al2010).
         </citsent>
         <aftsection>
            <nextsent>Although the clustering is hierarchical, we only use a cluster at the lowest level of the hierarchy for each word.We use the purity (PU) and collocation (CO) metrics as well as their harmonic mean (F1) to measure the quality of the resulting clusters.</nextsent>
            <nextsent>Purity measures the degree to which each cluster contains arguments sharing the same gold role: PU = 1 N ? i max j |Gj ? Ci|where if Ci is the set of arguments in the i-th induced cluster,Gj is the set of arguments in the jth10The coupled model without discounting still outperforms the factored counterpart in our experiments.</nextsent>
            <nextsent>18gold cluster, and N is the total number of arguments.</nextsent>
            <nextsent>Collocation evaluates the degree to which arguments with the same gold roles are assigned to a single cluster.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Due to the space constraints we are not able to present detailed analysis of the induced similarity graph D, however,argument-key pairs with the highest induced similarity encode, among other things, passivization, benefactive alternations, near-interchangeabilityof some subordinating conjunctions and prepositions (e.g., if and whether), as well as, restoring some of the unnecessary splits introduced by the argument key definition (e.g., semantic roles for adverbials do not normally depend on whether the construction is passive or active).</prevsent>
            <prevsent>Most of SRL research has focused on the supervised setting (Carreras and Ma`rquez, 2005; Surdeanu et al 2008), however, lack of annotated resources for most languages and insufficient cover age provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision.</prevsent>
         </prevsection>
         <citsent citstr=" D09-1002 ">
            This work includes methods based on graph alignment between labeled and unlabeled data (Furstenau and Lapata, 2009
            <papid>D09-1002</papid>
            ), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009
            <papid>D09-1003</papid>
            ), and projection of annotation across languages (Pado and Lapata, 2009; vander Plas et al 2011).
         </citsent>
         <aftsection>
            <nextsent>
               Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Lianget al 2009; Titov and Kozhevnikov, 2010
               <papid>P10-1098</papid>
               ; Goldwasser et al 2011; Liang et al 2011
               <papid>P11-1060</papid>
               ).Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar ver balizations of relations (Lin and Pantel, 2001;Banko et al 2007).
            </nextsent>
            <nextsent>
               Early unsupervised approaches to the SRL problem include the workby Swier and Stevenson (2004)
               <papid>E09-1026</papid>
               , where the Verb Net verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Due to the space constraints we are not able to present detailed analysis of the induced similarity graph D, however,argument-key pairs with the highest induced similarity encode, among other things, passivization, benefactive alternations, near-interchangeabilityof some subordinating conjunctions and prepositions (e.g., if and whether), as well as, restoring some of the unnecessary splits introduced by the argument key definition (e.g., semantic roles for adverbials do not normally depend on whether the construction is passive or active).</prevsent>
            <prevsent>Most of SRL research has focused on the supervised setting (Carreras and Ma`rquez, 2005; Surdeanu et al 2008), however, lack of annotated resources for most languages and insufficient cover age provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision.</prevsent>
         </prevsection>
         <citsent citstr=" D09-1003 ">
            This work includes methods based on graph alignment between labeled and unlabeled data (Furstenau and Lapata, 2009
            <papid>D09-1002</papid>
            ), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009
            <papid>D09-1003</papid>
            ), and projection of annotation across languages (Pado and Lapata, 2009; vander Plas et al 2011).
         </citsent>
         <aftsection>
            <nextsent>
               Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Lianget al 2009; Titov and Kozhevnikov, 2010
               <papid>P10-1098</papid>
               ; Goldwasser et al 2011; Liang et al 2011
               <papid>P11-1060</papid>
               ).Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar ver balizations of relations (Lin and Pantel, 2001;Banko et al 2007).
            </nextsent>
            <nextsent>
               Early unsupervised approaches to the SRL problem include the workby Swier and Stevenson (2004)
               <papid>E09-1026</papid>
               , where the Verb Net verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Most of SRL research has focused on the supervised setting (Carreras and Ma`rquez, 2005; Surdeanu et al 2008), however, lack of annotated resources for most languages and insufficient cover age provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision.</prevsent>
            <prevsent>
               This work includes methods based on graph alignment between labeled and unlabeled data (Furstenau and Lapata, 2009
               <papid>D09-1002</papid>
               ), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009
               <papid>D09-1003</papid>
               ), and projection of annotation across languages (Pado and Lapata, 2009; vander Plas et al 2011).
            </prevsent>
         </prevsection>
         <citsent citstr=" P10-1098 ">
            Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Lianget al 2009; Titov and Kozhevnikov, 2010
            <papid>P10-1098</papid>
            ; Goldwasser et al 2011; Liang et al 2011
            <papid>P11-1060</papid>
            ).Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar ver balizations of relations (Lin and Pantel, 2001;Banko et al 2007).
         </citsent>
         <aftsection>
            <nextsent>
               Early unsupervised approaches to the SRL problem include the workby Swier and Stevenson (2004)
               <papid>E09-1026</papid>
               , where the Verb Net verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface.
            </nextsent>
            <nextsent>More recently, the role induction problem has been studied in Lang and Lapata (2010) whereit has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Most of SRL research has focused on the supervised setting (Carreras and Ma`rquez, 2005; Surdeanu et al 2008), however, lack of annotated resources for most languages and insufficient cover age provided by the existing resources motivates the need for using unlabeled data or other forms of weak supervision.</prevsent>
            <prevsent>
               This work includes methods based on graph alignment between labeled and unlabeled data (Furstenau and Lapata, 2009
               <papid>D09-1002</papid>
               ), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009
               <papid>D09-1003</papid>
               ), and projection of annotation across languages (Pado and Lapata, 2009; vander Plas et al 2011).
            </prevsent>
         </prevsection>
         <citsent citstr=" P11-1060 ">
            Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Lianget al 2009; Titov and Kozhevnikov, 2010
            <papid>P10-1098</papid>
            ; Goldwasser et al 2011; Liang et al 2011
            <papid>P11-1060</papid>
            ).Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar ver balizations of relations (Lin and Pantel, 2001;Banko et al 2007).
         </citsent>
         <aftsection>
            <nextsent>
               Early unsupervised approaches to the SRL problem include the workby Swier and Stevenson (2004)
               <papid>E09-1026</papid>
               , where the Verb Net verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface.
            </nextsent>
            <nextsent>More recently, the role induction problem has been studied in Lang and Lapata (2010) whereit has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" E12-1003.xml">a bayesian approach to unsupervised semantic role induction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               This work includes methods based on graph alignment between labeled and unlabeled data (Furstenau and Lapata, 2009
               <papid>D09-1002</papid>
               ), using unlabeled data to improve lexical generalization (Deschacht and Moens, 2009
               <papid>D09-1003</papid>
               ), and projection of annotation across languages (Pado and Lapata, 2009; vander Plas et al 2011).
            </prevsent>
            <prevsent>
               Semi-supervised and weakly-supervised techniques have also been explored for other types of semantic representations but these studies have mostly focused on restricted domains (Kate and Mooney, 2007; Lianget al 2009; Titov and Kozhevnikov, 2010
               <papid>P10-1098</papid>
               ; Goldwasser et al 2011; Liang et al 2011
               <papid>P11-1060</papid>
               ).Unsupervised learning has been one of the central paradigms for the closely-related area of relation extraction, where several techniques have been proposed to cluster semantically similar ver balizations of relations (Lin and Pantel, 2001;Banko et al 2007).
            </prevsent>
         </prevsection>
         <citsent citstr=" E09-1026 ">
            Early unsupervised approaches to the SRL problem include the workby Swier and Stevenson (2004)
            <papid>E09-1026</papid>
            , where the Verb Net verb lexicon was used to guide unsupervised learning, and a generative model of Grenager and Manning (2006) which exploits linguistic priors on syntactic-semantic interface.
         </citsent>
         <aftsection>
            <nextsent>More recently, the role induction problem has been studied in Lang and Lapata (2010) whereit has been reformulated as a problem of detecting alterations and mapping non-standard linkings to the canonical ones.</nextsent>
            <nextsent>Later, Lang and La pata (2011a) proposed an algorithmic approach to clustering argument signatures which achieves higher accuracy and outperforms the syntactic baseline.</nextsent>
            <nextsent>In Lang and Lapata (2011b), the role induction problem is formulated as a graph partitioning problem: each vertex in the graph corresponds to a predicate occurrence and edges represent lexical and syntactic similarities between theoccurrences.</nextsent>
            <nextsent>Unsupervised induction of semantics has also been studied in Poon and Domingos (2009) and Titov and Klementiev (2010) butthe induced representations are not entirely compatible with the PropBank-style annotations andthey have been evaluated only on a question answering task for the biomedical domain.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Stanford dependencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence.</prevsent>
            <prevsent>This semantically-oriented representation is intuitive and easy to apply, requiring little linguistic expertise.</prevsent>
         </prevsection>
         <citsent citstr=" W09-1401 ">
            Consequently, Stanford dependencies are widely used: in biomedical text mining (Kim et al, 2009
            <papid>W09-1401</papid>
            ),as well as in textual entailment (Androutsopoulos and Malakasiotis, 2010), information extraction (Wu and Weld, 2010
            <papid>P10-1013</papid>
            ; Banko et al, 2007
            <papid>C10-2058</papid>
            ) and sentiment analysis (Meena and Prabhakar, 2007).In addition to English, there is a Chinese version of Stanford dependencies (Chang et al, 2009
            <papid>W09-2307</papid>
            ), (a) A constituent parse tree.
         </citsent>
         <aftsection>
            <nextsent>(b) Stanford dependencies.</nextsent>
            <nextsent>Figure 1: A sample Chinese constituent parse tree and its corresponding Stanford dependencies for the sentence China (??)</nextsent>
            <nextsent>encourages (??)</nextsent>
            <nextsent>private (??)</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Stanford dependencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence.</prevsent>
            <prevsent>This semantically-oriented representation is intuitive and easy to apply, requiring little linguistic expertise.</prevsent>
         </prevsection>
         <citsent citstr=" P10-1013 ">
            Consequently, Stanford dependencies are widely used: in biomedical text mining (Kim et al, 2009
            <papid>W09-1401</papid>
            ),as well as in textual entailment (Androutsopoulos and Malakasiotis, 2010), information extraction (Wu and Weld, 2010
            <papid>P10-1013</papid>
            ; Banko et al, 2007
            <papid>C10-2058</papid>
            ) and sentiment analysis (Meena and Prabhakar, 2007).In addition to English, there is a Chinese version of Stanford dependencies (Chang et al, 2009
            <papid>W09-2307</papid>
            ), (a) A constituent parse tree.
         </citsent>
         <aftsection>
            <nextsent>(b) Stanford dependencies.</nextsent>
            <nextsent>Figure 1: A sample Chinese constituent parse tree and its corresponding Stanford dependencies for the sentence China (??)</nextsent>
            <nextsent>encourages (??)</nextsent>
            <nextsent>private (??)</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Stanford dependencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence.</prevsent>
            <prevsent>This semantically-oriented representation is intuitive and easy to apply, requiring little linguistic expertise.</prevsent>
         </prevsection>
         <citsent citstr=" C10-2058 ">
            Consequently, Stanford dependencies are widely used: in biomedical text mining (Kim et al, 2009
            <papid>W09-1401</papid>
            ),as well as in textual entailment (Androutsopoulos and Malakasiotis, 2010), information extraction (Wu and Weld, 2010
            <papid>P10-1013</papid>
            ; Banko et al, 2007
            <papid>C10-2058</papid>
            ) and sentiment analysis (Meena and Prabhakar, 2007).In addition to English, there is a Chinese version of Stanford dependencies (Chang et al, 2009
            <papid>W09-2307</papid>
            ), (a) A constituent parse tree.
         </citsent>
         <aftsection>
            <nextsent>(b) Stanford dependencies.</nextsent>
            <nextsent>Figure 1: A sample Chinese constituent parse tree and its corresponding Stanford dependencies for the sentence China (??)</nextsent>
            <nextsent>encourages (??)</nextsent>
            <nextsent>private (??)</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Stanford dependencies (de Marneffe and Manning, 2008) provide a simple description of relations between pairs of words in a sentence.</prevsent>
            <prevsent>This semantically-oriented representation is intuitive and easy to apply, requiring little linguistic expertise.</prevsent>
         </prevsection>
         <citsent citstr=" W09-2307 ">
            Consequently, Stanford dependencies are widely used: in biomedical text mining (Kim et al, 2009
            <papid>W09-1401</papid>
            ),as well as in textual entailment (Androutsopoulos and Malakasiotis, 2010), information extraction (Wu and Weld, 2010
            <papid>P10-1013</papid>
            ; Banko et al, 2007
            <papid>C10-2058</papid>
            ) and sentiment analysis (Meena and Prabhakar, 2007).In addition to English, there is a Chinese version of Stanford dependencies (Chang et al, 2009
            <papid>W09-2307</papid>
            ), (a) A constituent parse tree.
         </citsent>
         <aftsection>
            <nextsent>(b) Stanford dependencies.</nextsent>
            <nextsent>Figure 1: A sample Chinese constituent parse tree and its corresponding Stanford dependencies for the sentence China (??)</nextsent>
            <nextsent>encourages (??)</nextsent>
            <nextsent>private (??)</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>infrastructure (??)</prevsent>
            <prevsent>construction (??).</prevsent>
         </prevsection>
         <citsent citstr=" D11-1123 ">
            which is also useful for many applications, such as Chinese sentiment analysis (Wu et al, 2011
            <papid>D11-1123</papid>
            ; Wu etal., 2009; Zhuang et al, 2006) and relation extraction (Huang et al, 2008).
         </citsent>
         <aftsection>
            <nextsent>Figure 1 shows a sample constituent parse tree and the corresponding Stanford dependencies for a sentence in Chinese.</nextsent>
            <nextsent>Although there are several variants of Stanford dependencies for English,1 so far only a basic version (i.e, dependency tree structures) is available for Chinese.</nextsent>
            <nextsent>Stanford dependencies were originally obtained from constituent trees, using rules (de Marneffe etal., 2006).</nextsent>
            <nextsent>But as dependency parsing technologies mature (Kubler et al, 2009), they offer increasingly attractive alternatives that eliminate the need for an intermediate representation.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Stanford dependencies were originally obtained from constituent trees, using rules (de Marneffe etal., 2006).</prevsent>
            <prevsent>But as dependency parsing technologies mature (Kubler et al, 2009), they offer increasingly attractive alternatives that eliminate the need for an intermediate representation.</prevsent>
         </prevsection>
         <citsent citstr=" P03-1054 ">
            Cer et al (2010) reported that Stanfords implementation (Klein and Manning, 2003
            <papid>P03-1054</papid>
            ) underperforms other constituent 1nlp.stanford.edu/software/dependencies_manual.pdf 11 Type Parser Version Algorithm URL Constituent Berkeley 1.1 PCFG code.google.com/p/berkeleyparser Bikel 1.2 PCFG www.cis.upenn.edu/dbikel/download.html Charniak Nov. 2009 PCFG www.cog.brown.edu/mj/Software.htm Stanford 2.0 Factored nlp.stanford.edu/software/lex-parser.shtml Dependency Malt parser 1.6.1 Arc-Eager maltparser.org Mate 2.0 2nd-order MST code.google.com/p/mate-tools Mst parser 0.5 MST sourceforge.net/projects/mstparser Table 1: Basic information for the seven parsers included in our experiments.
         </citsent>
         <aftsection>
            <nextsent>parsers, for English, on both accuracy and speed.Their thorough investigation also showed that constituent parsers systematically outperform parsing directly to Stanford dependencies.</nextsent>
            <nextsent>
               Nevertheless, relative standings could have changed in recent years:dependency parsers are now significantly more accurate, thanks to advances like the high-order maximum spanning tree (MST) model (Koo and Collins,2010) for graph-based dependency parsing (McDonald and Pereira, 2006
               <papid>E06-1011</papid>
               ).
            </nextsent>
            <nextsent>Therefore, we deemed it important to re-evaluate the performance of constituent and dependency parsers.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Methodology.</section>
      <citcontext>
         <prevsection>
            <prevsent>2.1 Parsers.</prevsent>
            <prevsent>We considered four constituent parsers.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1055 ">
            They are:Berkeley (Petrov et al, 2006
            <papid>P06-1055</papid>
            ), Bikel (2004)
            <papid>W04-3224</papid>
            , Char niak (2000) and Stanford (Klein and Manning, 2003
            <papid>P03-1054</papid>
            ) chinese factored, which is also the default used by Stanford dependencies.
         </citsent>
         <aftsection>
            <nextsent>
               The three dependency parsers are: Malt parser (Nivre et al, 2006), Mate (Bohnet, 2010
               <papid>C10-1011</papid>
               )2 and Mst parser (McDonald and Pereira, 2006
               <papid>E06-1011</papid>
               ).
            </nextsent>
            <nextsent>Table 1 has more information.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Methodology.</section>
      <citcontext>
         <prevsection>
            <prevsent>2.1 Parsers.</prevsent>
            <prevsent>We considered four constituent parsers.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3224 ">
            They are:Berkeley (Petrov et al, 2006
            <papid>P06-1055</papid>
            ), Bikel (2004)
            <papid>W04-3224</papid>
            , Char niak (2000) and Stanford (Klein and Manning, 2003
            <papid>P03-1054</papid>
            ) chinese factored, which is also the default used by Stanford dependencies.
         </citsent>
         <aftsection>
            <nextsent>
               The three dependency parsers are: Malt parser (Nivre et al, 2006), Mate (Bohnet, 2010
               <papid>C10-1011</papid>
               )2 and Mst parser (McDonald and Pereira, 2006
               <papid>E06-1011</papid>
               ).
            </nextsent>
            <nextsent>Table 1 has more information.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Methodology.</section>
      <citcontext>
         <prevsection>
            <prevsent>2.1 Parsers.</prevsent>
            <prevsent>We considered four constituent parsers.</prevsent>
         </prevsection>
         <citsent citstr=" P03-1054 ">
            They are:Berkeley (Petrov et al, 2006
            <papid>P06-1055</papid>
            ), Bikel (2004)
            <papid>W04-3224</papid>
            , Char niak (2000) and Stanford (Klein and Manning, 2003
            <papid>P03-1054</papid>
            ) chinese factored, which is also the default used by Stanford dependencies.
         </citsent>
         <aftsection>
            <nextsent>
               The three dependency parsers are: Malt parser (Nivre et al, 2006), Mate (Bohnet, 2010
               <papid>C10-1011</papid>
               )2 and Mst parser (McDonald and Pereira, 2006
               <papid>E06-1011</papid>
               ).
            </nextsent>
            <nextsent>Table 1 has more information.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Methodology.</section>
      <citcontext>
         <prevsection>
            <prevsent>We considered four constituent parsers.</prevsent>
            <prevsent>
               They are:Berkeley (Petrov et al, 2006
               <papid>P06-1055</papid>
               ), Bikel (2004)
               <papid>W04-3224</papid>
               , Char niak (2000) and Stanford (Klein and Manning, 2003
               <papid>P03-1054</papid>
               ) chinese factored, which is also the default used by Stanford dependencies.
            </prevsent>
         </prevsection>
         <citsent citstr=" C10-1011 ">
            The three dependency parsers are: Malt parser (Nivre et al, 2006), Mate (Bohnet, 2010
            <papid>C10-1011</papid>
            )2 and Mst parser (McDonald and Pereira, 2006
            <papid>E06-1011</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Table 1 has more information.</nextsent>
            <nextsent>2A second-order MST parser (with the speed optimization).</nextsent>
            <nextsent>2.2 Corpus.</nextsent>
            <nextsent>We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Methodology.</section>
      <citcontext>
         <prevsection>
            <prevsent>We considered four constituent parsers.</prevsent>
            <prevsent>
               They are:Berkeley (Petrov et al, 2006
               <papid>P06-1055</papid>
               ), Bikel (2004)
               <papid>W04-3224</papid>
               , Char niak (2000) and Stanford (Klein and Manning, 2003
               <papid>P03-1054</papid>
               ) chinese factored, which is also the default used by Stanford dependencies.
            </prevsent>
         </prevsection>
         <citsent citstr=" E06-1011 ">
            The three dependency parsers are: Malt parser (Nivre et al, 2006), Mate (Bohnet, 2010
            <papid>C10-1011</papid>
            )2 and Mst parser (McDonald and Pereira, 2006
            <papid>E06-1011</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Table 1 has more information.</nextsent>
            <nextsent>2A second-order MST parser (with the speed optimization).</nextsent>
            <nextsent>2.2 Corpus.</nextsent>
            <nextsent>We used the latest Chinese TreeBank (CTB) 7.0 in all experiments.3 CTB 7.0 is larger and has more sources (e.g., web text), compared to previous versions.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Methodology.</section>
      <citcontext>
         <prevsection>
            <prevsent>2.4 Features.</prevsent>
            <prevsent>Unlike constituent parsers, dependency models require exogenous part-of-speech (POS) tags, both intraining and in inference.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1033 ">
            We used the Stanford tagger (Toutanova et al, 2003
            <papid>N03-1033</papid>
            ) v3.1, with the MEMM model,4 in combination with 10-way jackknifing.5 Word lemmas ? which are generalizations of words ? are another feature known to be useful for dependency parsing.
         </citsent>
         <aftsection>
            <nextsent>
               Here we lemmatized each Chinese word down to its last character, since ? in contrast to English ? a Chinese words suffix often carries that words core sense (Tseng et al, 2005
               <papid>I05-3005</papid>
               ).
            </nextsent>
            <nextsent>For example, bicycle (???), car (??)</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Methodology.</section>
      <citcontext>
         <prevsection>
            <prevsent>Unlike constituent parsers, dependency models require exogenous part-of-speech (POS) tags, both intraining and in inference.</prevsent>
            <prevsent>
               We used the Stanford tagger (Toutanova et al, 2003
               <papid>N03-1033</papid>
               ) v3.1, with the MEMM model,4 in combination with 10-way jackknifing.5 Word lemmas ? which are generalizations of words ? are another feature known to be useful for dependency parsing.
            </prevsent>
         </prevsection>
         <citsent citstr=" I05-3005 ">
            Here we lemmatized each Chinese word down to its last character, since ? in contrast to English ? a Chinese words suffix often carries that words core sense (Tseng et al, 2005
            <papid>I05-3005</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>For example, bicycle (???), car (??)</nextsent>
            <nextsent>and train (??)</nextsent>
            <nextsent>are all various kinds of vehicle (?).</nextsent>
            <nextsent>3www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Results.</section>
      <citcontext>
         <prevsection>
            <prevsent>Our replication of Cer et als (2010, Table 1) evaluation revealed a bug: Mst parser normalized all numbers to a &lt;num&gt; symbol, which decreased its scoresin the evaluation tool used with Stanford dependencies.</prevsent>
            <prevsent>After fixing this glitch, Mst parsers performance improved from 78.8 (reported) to 82.5%, thus making it more accurate than Malt parser (81.1%) and hence the better dependency parser for English, consistent with our results for Chinese (see Table 3).</prevsent>
         </prevsection>
         <citsent citstr=" P05-1022 ">
            6ilk.uvt.nl/conll/software/eval.plOur finding does not contradict the main qualitative result of Cer et al (2010), however, since the constituent parser of Charniak and Johnson (2005)
            <papid>P05-1022</papid>
            still scores substantially higher (89.1%), for English, compared to all dependency parsers.7 In a separate experiment (parsing web data),8 we found Mate tobe less accurate than Charniak-Johnson ? and improvement from jackknifing smaller ? on English.
         </citsent>
         <aftsection>
            <nextsent>To further compare the constituent and dependency approaches to generating Stanford dependencies, we focused on Mate and Berkeley parsers ? the best of each type.</nextsent>
            <nextsent>Overall, the difference between their accuracies is not statistically significant (p &gt; 0.05).9 Table 4 highlights performance (F1 scores) for the most frequent relation labels.</nextsent>
            <nextsent>
               Mate does better on most relations, noun compound modifiers (nn) and adjectival modifiers (amod) in particular; and the Berkeley parser is better at root and dep.10 Mate seems to excel at short-distance dependencies, possibly because it uses more local features (even with a second-order model) than the Berkeley parser, whose PCFG can capture longer-distance rules.Since POS-tags are especially informative of Chinese dependencies (Li et al, 2011
               <papid>D11-1109</papid>
               ), we harmonized training and test data, using 10-way jackknifing (see 2.4).
            </nextsent>
            <nextsent>This method is more robust than training a 7One (small) factor contributing to the difference between the two languages is that in the Chinese setup we stop with basic stanford dependencies ? there is no penalty for further conver sion; another is not using discriminative reranking for Chinese.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Analysis.</section>
      <citcontext>
         <prevsection>
            <prevsent>To further compare the constituent and dependency approaches to generating Stanford dependencies, we focused on Mate and Berkeley parsers ? the best of each type.</prevsent>
            <prevsent>Overall, the difference between their accuracies is not statistically significant (p &gt; 0.05).9 Table 4 highlights performance (F1 scores) for the most frequent relation labels.</prevsent>
         </prevsection>
         <citsent citstr=" D11-1109 ">
            Mate does better on most relations, noun compound modifiers (nn) and adjectival modifiers (amod) in particular; and the Berkeley parser is better at root and dep.10 Mate seems to excel at short-distance dependencies, possibly because it uses more local features (even with a second-order model) than the Berkeley parser, whose PCFG can capture longer-distance rules.Since POS-tags are especially informative of Chinese dependencies (Li et al, 2011
            <papid>D11-1109</papid>
            ), we harmonized training and test data, using 10-way jackknifing (see 2.4).
         </citsent>
         <aftsection>
            <nextsent>This method is more robust than training a 7One (small) factor contributing to the difference between the two languages is that in the Chinese setup we stop with basic stanford dependencies ? there is no penalty for further conver sion; another is not using discriminative reranking for Chinese.</nextsent>
            <nextsent>
               8sites.google.com/site/sancl2012/home/shared-task 9For LAS, p ? 0.11; and for UAS, p ? 0.25, according to www.cis.upenn.edu/dbikel/download/compare.pl 10An unmatched (default) relation (Chang et al, 2009
               <papid>W09-2307</papid>
               , 3.1).
            </nextsent>
            <nextsent>13 Relation Count Mate Berkeley nn 7,783 91.3 89.3 dep 4,651 69.4 70.3 nsubj 4,531 87.1 85.5 advmod 4,028 94.3 93.8 dobj 3,990 86.0 85.0 conj 2,159 76.0 75.8 prep 2,091 94.3 94.1 root 2,079 81.2 82.3 nummod 1,614 97.4 96.7 assmod 1,593 86.3 84.1 assm 1,590 88.9 87.2 pobj 1,532 84.2 82.9 amod 1,440 85.6 81.1 rcmod 1,433 74.0 70.6 cpm 1,371 84.4 83.2Table 4: Performance (F1 scores) for the fifteen most frequent dependency relations in the CTB 7.0 development dataset attained by both Mate and Berkeley parsers.parser with gold tags because it improves consistency, particularly for Chinese, where tagging accuracies are lower than in English.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Analysis.</section>
      <citcontext>
         <prevsection>
            <prevsent>13 Relation Count Mate Berkeley nn 7,783 91.3 89.3 dep 4,651 69.4 70.3 nsubj 4,531 87.1 85.5 advmod 4,028 94.3 93.8 dobj 3,990 86.0 85.0 conj 2,159 76.0 75.8 prep 2,091 94.3 94.1 root 2,079 81.2 82.3 nummod 1,614 97.4 96.7 assmod 1,593 86.3 84.1 assm 1,590 88.9 87.2 pobj 1,532 84.2 82.9 amod 1,440 85.6 81.1 rcmod 1,433 74.0 70.6 cpm 1,371 84.4 83.2Table 4: Performance (F1 scores) for the fifteen most frequent dependency relations in the CTB 7.0 development dataset attained by both Mate and Berkeley parsers.parser with gold tags because it improves consistency, particularly for Chinese, where tagging accuracies are lower than in English.</prevsent>
            <prevsent>On development data, Mate scored worse given gold tags (75.4 versus 78.2%).11 Lemmatization offered additional useful cues for overcoming data sparse ness (77.8 without, versus 78.2% with lemma features).</prevsent>
         </prevsection>
         <citsent citstr=" P08-1068 ">
            Unsupervised word clusters could thus also help (Koo et al, 2008
            <papid>P08-1068</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Our results suggest that if accuracy is of primary concern, then Mate should be preferred;12 however, Berkeley parser offers a trade-off between accuracy and speed.</nextsent>
            <nextsent>If neither parser satisfies the demands of a practical application (e.g., real-time processing or bulk-parsing the web), then Malt parser (liblinear) may be the only viable option.</nextsent>
            <nextsent>
               Fortunately, it comes with much headroom for improving accuracy, including a tunable margin parameter C for the classifier, richer feature sets (Zhang and Nivre, 2011
               <papid>P11-2033</papid>
               ) and ensemble models (Surdeanu and Manning, 2010
               <papid>N10-1091</papid>
               ).
            </nextsent>
            <nextsent>Stanford dependencies are not the only popular dependency representation.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Discussion.</section>
      <citcontext>
         <prevsection>
            <prevsent>Our results suggest that if accuracy is of primary concern, then Mate should be preferred;12 however, Berkeley parser offers a trade-off between accuracy and speed.</prevsent>
            <prevsent>If neither parser satisfies the demands of a practical application (e.g., real-time processing or bulk-parsing the web), then Malt parser (liblinear) may be the only viable option.</prevsent>
         </prevsection>
         <citsent citstr=" P11-2033 ">
            Fortunately, it comes with much headroom for improving accuracy, including a tunable margin parameter C for the classifier, richer feature sets (Zhang and Nivre, 2011
            <papid>P11-2033</papid>
            ) and ensemble models (Surdeanu and Manning, 2010
            <papid>N10-1091</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Stanford dependencies are not the only popular dependency representation.</nextsent>
            <nextsent>We also considered the 11Berkeleys performance suffered with jackknifed tags (76.5 versus 77.0%), possibly because it parses and tags better jointly.</nextsent>
            <nextsent>12Although Mates performance was not significantly better than Berkeleys in our setting, it has the potential to tap richer features and other advantages of dependency parsers (Nivre and mcdonald, 2008) to further boost accuracy, which may be difficult in the generative framework of a typical constituent parser.</nextsent>
            <nextsent>
               conversion scheme of the Penn2Malt tool,13 used in a series of CoNLL shared tasks (Buchholz and Marsi, 2006
               <papid>W06-2920</papid>
               ; Nivre et al, 2007; Surdeanu et al, 2008; Hajic?
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P12-2003.xml">a comparison of chinese parsers for stanford dependencies</title>
      <section>Discussion.</section>
      <citcontext>
         <prevsection>
            <prevsent>Our results suggest that if accuracy is of primary concern, then Mate should be preferred;12 however, Berkeley parser offers a trade-off between accuracy and speed.</prevsent>
            <prevsent>If neither parser satisfies the demands of a practical application (e.g., real-time processing or bulk-parsing the web), then Malt parser (liblinear) may be the only viable option.</prevsent>
         </prevsection>
         <citsent citstr=" N10-1091 ">
            Fortunately, it comes with much headroom for improving accuracy, including a tunable margin parameter C for the classifier, richer feature sets (Zhang and Nivre, 2011
            <papid>P11-2033</papid>
            ) and ensemble models (Surdeanu and Manning, 2010
            <papid>N10-1091</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Stanford dependencies are not the only popular dependency representation.</nextsent>
            <nextsent>We also considered the 11Berkeleys performance suffered with jackknifed tags (76.5 versus 77.0%), possibly because it parses and tags better jointly.</nextsent>
            <nextsent>12Although Mates performance was not significantly better than Berkeleys in our setting, it has the potential to tap richer features and other advantages of dependency parsers (Nivre and mcdonald, 2008) to further boost accuracy, which may be difficult in the generative framework of a typical constituent parser.</nextsent>
            <nextsent>
               conversion scheme of the Penn2Malt tool,13 used in a series of CoNLL shared tasks (Buchholz and Marsi, 2006
               <papid>W06-2920</papid>
               ; Nivre et al, 2007; Surdeanu et al, 2008; Hajic?
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W12-3210.xml">towards an acl anthology corpus with logical document structure an overview of the acl 2012 contributed task</title>
      <section>An Overview of the Contributed Task.</section>
      <citcontext>
         <prevsection>
            <prevsent>Its rich text XML markup will contain information on logical document structure such as section headings, footnotes, table and figure captions, bibliographic references, italics/emphasized text portions, non-latin scripts, etc. The initial source are the PDF documents of the Anthology, processed with different text extraction methods and tools that output XML/HTML.</prevsent>
            <prevsent>The input to the task itself then consists of two XML for mats:?</prevsent>
         </prevsection>
         <citsent citstr=" P11-4002 ">
            Paper xml from the ACL Anthology Search bench6 (Schfer et al, 2011
            <papid>P11-4002</papid>
            ) provided by DFKI Saarbrcken, of all approximately22,500 papers currently in the Anthology (except ROCLING which are mostly in Chinese).
         </citsent>
         <aftsection>
            <nextsent>These were obtained by running a commercial OCR program and applying logical markup postprocessing and conversion to XML (Schfer &amp; Weitz, 2012).</nextsent>
            <nextsent>5 http://www.tei-c.org/Roma/ 6 http://aclasb.dfki.de 90 &lt;xml version="1.0" encoding="UTF-8"?&gt; &lt;TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 aclarc.tei.xsd" xml:lang="en"&gt; &lt;teiHeader&gt; &lt;fileDesc&gt; &lt;titleStmt&gt; &lt;title&gt;Task-oriented Evaluation of Syntactic Parsers and Their Representations&lt;/title&gt; &lt;author&gt; Yusuke Miyao?</nextsent>
            <nextsent>Rune Stre?</nextsent>
            <nextsent>Kenji Sagae?</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>To this end, we propose bag of pivot language N-grams (BPNG) as a robust, broad-coverage,and knowledge-lean semantic representation for natural language sentences.</prevsent>
            <prevsent>Most importantly, BPNG does not depend on lexical or syntactic similarity, allowing us to address the conflicting requirementsof paraphrase evaluation.</prevsent>
         </prevsection>
         <citsent citstr=" N10-1145 ">
            The only linguistic re 923 source required to evaluate BPNG is a parallel textof the target language and an arbitrary other language, known as the pivot language.We highlight that paraphrase evaluation and paraphrase recognition (Heilman and Smith, 2010
            <papid>N10-1145</papid>
            ; Das and Smith, 2009
            <papid>P09-1053</papid>
            ; Wan et al, 2006; Qiu et al, 2006
            <papid>W06-1603</papid>
            ) are related yet distinct tasks.
         </citsent>
         <aftsection>
            <nextsent>Consider two sentencesS1 and S2 that are the same except for the substitution of a single synonym.</nextsent>
            <nextsent>A paraphrase recognition system should assign them a very high score, but aparaphrase evaluation system would assign a relatively low one.</nextsent>
            <nextsent>Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier.</nextsent>
            <nextsent>The rest of the paper is organized as follows.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>To this end, we propose bag of pivot language N-grams (BPNG) as a robust, broad-coverage,and knowledge-lean semantic representation for natural language sentences.</prevsent>
            <prevsent>Most importantly, BPNG does not depend on lexical or syntactic similarity, allowing us to address the conflicting requirementsof paraphrase evaluation.</prevsent>
         </prevsection>
         <citsent citstr=" P09-1053 ">
            The only linguistic re 923 source required to evaluate BPNG is a parallel textof the target language and an arbitrary other language, known as the pivot language.We highlight that paraphrase evaluation and paraphrase recognition (Heilman and Smith, 2010
            <papid>N10-1145</papid>
            ; Das and Smith, 2009
            <papid>P09-1053</papid>
            ; Wan et al, 2006; Qiu et al, 2006
            <papid>W06-1603</papid>
            ) are related yet distinct tasks.
         </citsent>
         <aftsection>
            <nextsent>Consider two sentencesS1 and S2 that are the same except for the substitution of a single synonym.</nextsent>
            <nextsent>A paraphrase recognition system should assign them a very high score, but aparaphrase evaluation system would assign a relatively low one.</nextsent>
            <nextsent>Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier.</nextsent>
            <nextsent>The rest of the paper is organized as follows.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>To this end, we propose bag of pivot language N-grams (BPNG) as a robust, broad-coverage,and knowledge-lean semantic representation for natural language sentences.</prevsent>
            <prevsent>Most importantly, BPNG does not depend on lexical or syntactic similarity, allowing us to address the conflicting requirementsof paraphrase evaluation.</prevsent>
         </prevsection>
         <citsent citstr=" W06-1603 ">
            The only linguistic re 923 source required to evaluate BPNG is a parallel textof the target language and an arbitrary other language, known as the pivot language.We highlight that paraphrase evaluation and paraphrase recognition (Heilman and Smith, 2010
            <papid>N10-1145</papid>
            ; Das and Smith, 2009
            <papid>P09-1053</papid>
            ; Wan et al, 2006; Qiu et al, 2006
            <papid>W06-1603</papid>
            ) are related yet distinct tasks.
         </citsent>
         <aftsection>
            <nextsent>Consider two sentencesS1 and S2 that are the same except for the substitution of a single synonym.</nextsent>
            <nextsent>A paraphrase recognition system should assign them a very high score, but aparaphrase evaluation system would assign a relatively low one.</nextsent>
            <nextsent>Indeed, the latter is often a better indicator of how useful a PG system potentially is for the applications of PG described earlier.</nextsent>
            <nextsent>The rest of the paper is organized as follows.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Related work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Later works attempt totake more syntactic and semantic features into consideration (see (Callison-Burch et al, 2009) for an overview).</prevsent>
            <prevsent>The whole spectrum of NLP resourceshas found application in machine translation evaluation, including POS tags, constituent and dependency parses, WordNet (Fellbaum, 1998), semantic roles, textual entailment features, and more.</prevsent>
         </prevsection>
         <citsent citstr=" W10-1754 ">
            Manyof these metrics have been shown to correlate better with human judges than BLEU (Chan and Ng,2008; Liu et al, 2010
            <papid>W10-1754</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Interestingly, few MT evaluation metrics exploit parallel texts as a source of information, when statistical MT is centered almost entirely around mining parallel texts.</nextsent>
            <nextsent>
               Compared to these MT evaluation metrics, our method focuses on addressing the unique requirement of paraphrase evaluation: that lexical closeness does not necessarily entail goodness, contrary to the basis of MT evaluation.Inspired by the success of automatic MT evaluation, Lin (2004)
               <papid>W04-1013</papid>
               and Hovy et al (2006) propose automatic metrics for summary evaluation.
            </nextsent>
            <nextsent>The former is entirely lexical based, whereas the latter also exploits constituent and dependency parses, and semantic features derived from WordNet.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Related work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Manyof these metrics have been shown to correlate better with human judges than BLEU (Chan and Ng,2008; Liu et al, 2010
               <papid>W10-1754</papid>
               ).
            </prevsent>
            <prevsent>Interestingly, few MT evaluation metrics exploit parallel texts as a source of information, when statistical MT is centered almost entirely around mining parallel texts.</prevsent>
         </prevsection>
         <citsent citstr=" W04-1013 ">
            Compared to these MT evaluation metrics, our method focuses on addressing the unique requirement of paraphrase evaluation: that lexical closeness does not necessarily entail goodness, contrary to the basis of MT evaluation.Inspired by the success of automatic MT evaluation, Lin (2004)
            <papid>W04-1013</papid>
            and Hovy et al (2006) propose automatic metrics for summary evaluation.
         </citsent>
         <aftsection>
            <nextsent>The former is entirely lexical based, whereas the latter also exploits constituent and dependency parses, and semantic features derived from WordNet.</nextsent>
            <nextsent>
               The only prior attempt to devise an automatic evaluation metric for paraphrases that we are aware of is ParaMetric (Callison-Burch et al, 2008
               <papid>C08-1013</papid>
               ), which compares the collection of paraphrases discovered by automatic paraphrasing algorithms against amanual gold standard collected over the same sen tences.
            </nextsent>
            <nextsent>The recall and precision of several current paraphrase generation systems are evaluated.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Related work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Compared to these MT evaluation metrics, our method focuses on addressing the unique requirement of paraphrase evaluation: that lexical closeness does not necessarily entail goodness, contrary to the basis of MT evaluation.Inspired by the success of automatic MT evaluation, Lin (2004)
               <papid>W04-1013</papid>
               and Hovy et al (2006) propose automatic metrics for summary evaluation.
            </prevsent>
            <prevsent>The former is entirely lexical based, whereas the latter also exploits constituent and dependency parses, and semantic features derived from WordNet.</prevsent>
         </prevsection>
         <citsent citstr=" C08-1013 ">
            The only prior attempt to devise an automatic evaluation metric for paraphrases that we are aware of is ParaMetric (Callison-Burch et al, 2008
            <papid>C08-1013</papid>
            ), which compares the collection of paraphrases discovered by automatic paraphrasing algorithms against amanual gold standard collected over the same sen tences.
         </citsent>
         <aftsection>
            <nextsent>The recall and precision of several current paraphrase generation systems are evaluated.</nextsent>
            <nextsent>ParaMetric does not attempt to propose a single metric to correlate well with human judgments.</nextsent>
            <nextsent>Rather, it consists of a few indirect and partial measures of the quality of PG systems.</nextsent>
            <nextsent>The first step in defining a paraphrase evaluation metric is to define a good paraphrase.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Paraphrase Evaluation Metric (PEM).</section>
      <citcontext>
         <prevsection>
            <prevsent>4.1 Phrase-level semantic representation.</prevsent>
            <prevsent>Without loss of generality, suppose we are to evaluate English paraphrases, and have been supplied many sentence-aligned parallel texts of French and English as an additional resource.</prevsent>
         </prevsection>
         <citsent citstr=" J03-1002 ">
            We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003
            <papid>J03-1002</papid>
            ) or the Berkeley aligner (Liang et al, 2006
            <papid>N06-1014</papid>
            ; Haghighi et al, 2009
            <papid>P09-1104</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text.</nextsent>
            <nextsent>If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning.</nextsent>
            <nextsent>
               Similar observations have been made by previous researchers (Wu and Zhou, 2003
               <papid>P03-1016</papid>
               ; Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ; Callison-Burch et al, 2006
               <papid>N06-1003</papid>
               ; Snover et al, 2009
               <papid>W09-0441</papid>
               ).
            </nextsent>
            <nextsent>We can treat the distribution of aligned French phrases as a semantic representation of the English phrase.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Paraphrase Evaluation Metric (PEM).</section>
      <citcontext>
         <prevsection>
            <prevsent>4.1 Phrase-level semantic representation.</prevsent>
            <prevsent>Without loss of generality, suppose we are to evaluate English paraphrases, and have been supplied many sentence-aligned parallel texts of French and English as an additional resource.</prevsent>
         </prevsection>
         <citsent citstr=" N06-1014 ">
            We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003
            <papid>J03-1002</papid>
            ) or the Berkeley aligner (Liang et al, 2006
            <papid>N06-1014</papid>
            ; Haghighi et al, 2009
            <papid>P09-1104</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text.</nextsent>
            <nextsent>If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning.</nextsent>
            <nextsent>
               Similar observations have been made by previous researchers (Wu and Zhou, 2003
               <papid>P03-1016</papid>
               ; Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ; Callison-Burch et al, 2006
               <papid>N06-1003</papid>
               ; Snover et al, 2009
               <papid>W09-0441</papid>
               ).
            </nextsent>
            <nextsent>We can treat the distribution of aligned French phrases as a semantic representation of the English phrase.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Paraphrase Evaluation Metric (PEM).</section>
      <citcontext>
         <prevsection>
            <prevsent>4.1 Phrase-level semantic representation.</prevsent>
            <prevsent>Without loss of generality, suppose we are to evaluate English paraphrases, and have been supplied many sentence-aligned parallel texts of French and English as an additional resource.</prevsent>
         </prevsection>
         <citsent citstr=" P09-1104 ">
            We can then align the parallel texts at word level automatically using well-known algorithms such as GIZA++ (Och and Ney, 2003
            <papid>J03-1002</papid>
            ) or the Berkeley aligner (Liang et al, 2006
            <papid>N06-1014</papid>
            ; Haghighi et al, 2009
            <papid>P09-1104</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text.</nextsent>
            <nextsent>If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning.</nextsent>
            <nextsent>
               Similar observations have been made by previous researchers (Wu and Zhou, 2003
               <papid>P03-1016</papid>
               ; Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ; Callison-Burch et al, 2006
               <papid>N06-1003</papid>
               ; Snover et al, 2009
               <papid>W09-0441</papid>
               ).
            </nextsent>
            <nextsent>We can treat the distribution of aligned French phrases as a semantic representation of the English phrase.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Paraphrase Evaluation Metric (PEM).</section>
      <citcontext>
         <prevsection>
            <prevsent>To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text.</prevsent>
            <prevsent>If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning.</prevsent>
         </prevsection>
         <citsent citstr=" P03-1016 ">
            Similar observations have been made by previous researchers (Wu and Zhou, 2003
            <papid>P03-1016</papid>
            ; Bannard and Callison-Burch, 2005
            <papid>P05-1074</papid>
            ; Callison-Burch et al, 2006
            <papid>N06-1003</papid>
            ; Snover et al, 2009
            <papid>W09-0441</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We can treat the distribution of aligned French phrases as a semantic representation of the English phrase.</nextsent>
            <nextsent>
               The semantic distance between two English phrases can then be measured by their degree of overlap in this representation.In this work, we use the widely-used phrase extraction heuristic in (Koehn et al, 2003
               <papid>N03-1017</papid>
               ) to extract phrase pairs from parallel texts into a phrase table1.
            </nextsent>
            <nextsent>The phrases extracted do not necessarily correspond to the speakers?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Paraphrase Evaluation Metric (PEM).</section>
      <citcontext>
         <prevsection>
            <prevsent>To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text.</prevsent>
            <prevsent>If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1074 ">
            Similar observations have been made by previous researchers (Wu and Zhou, 2003
            <papid>P03-1016</papid>
            ; Bannard and Callison-Burch, 2005
            <papid>P05-1074</papid>
            ; Callison-Burch et al, 2006
            <papid>N06-1003</papid>
            ; Snover et al, 2009
            <papid>W09-0441</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We can treat the distribution of aligned French phrases as a semantic representation of the English phrase.</nextsent>
            <nextsent>
               The semantic distance between two English phrases can then be measured by their degree of overlap in this representation.In this work, we use the widely-used phrase extraction heuristic in (Koehn et al, 2003
               <papid>N03-1017</papid>
               ) to extract phrase pairs from parallel texts into a phrase table1.
            </nextsent>
            <nextsent>The phrases extracted do not necessarily correspond to the speakers?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Paraphrase Evaluation Metric (PEM).</section>
      <citcontext>
         <prevsection>
            <prevsent>To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text.</prevsent>
            <prevsent>If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning.</prevsent>
         </prevsection>
         <citsent citstr=" N06-1003 ">
            Similar observations have been made by previous researchers (Wu and Zhou, 2003
            <papid>P03-1016</papid>
            ; Bannard and Callison-Burch, 2005
            <papid>P05-1074</papid>
            ; Callison-Burch et al, 2006
            <papid>N06-1003</papid>
            ; Snover et al, 2009
            <papid>W09-0441</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We can treat the distribution of aligned French phrases as a semantic representation of the English phrase.</nextsent>
            <nextsent>
               The semantic distance between two English phrases can then be measured by their degree of overlap in this representation.In this work, we use the widely-used phrase extraction heuristic in (Koehn et al, 2003
               <papid>N03-1017</papid>
               ) to extract phrase pairs from parallel texts into a phrase table1.
            </nextsent>
            <nextsent>The phrases extracted do not necessarily correspond to the speakers?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Paraphrase Evaluation Metric (PEM).</section>
      <citcontext>
         <prevsection>
            <prevsent>To measure adequacy without relying on lexical similarity, we make the key observation that the aligned French texts can act as a proxy of the semantics to a fragment of an English text.</prevsent>
            <prevsent>If two English phrases are often mapped to the same French phrase, they can be considered similar in meaning.</prevsent>
         </prevsection>
         <citsent citstr=" W09-0441 ">
            Similar observations have been made by previous researchers (Wu and Zhou, 2003
            <papid>P03-1016</papid>
            ; Bannard and Callison-Burch, 2005
            <papid>P05-1074</papid>
            ; Callison-Burch et al, 2006
            <papid>N06-1003</papid>
            ; Snover et al, 2009
            <papid>W09-0441</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We can treat the distribution of aligned French phrases as a semantic representation of the English phrase.</nextsent>
            <nextsent>
               The semantic distance between two English phrases can then be measured by their degree of overlap in this representation.In this work, we use the widely-used phrase extraction heuristic in (Koehn et al, 2003
               <papid>N03-1017</papid>
               ) to extract phrase pairs from parallel texts into a phrase table1.
            </nextsent>
            <nextsent>The phrases extracted do not necessarily correspond to the speakers?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Paraphrase Evaluation Metric (PEM).</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Similar observations have been made by previous researchers (Wu and Zhou, 2003
               <papid>P03-1016</papid>
               ; Bannard and Callison-Burch, 2005
               <papid>P05-1074</papid>
               ; Callison-Burch et al, 2006
               <papid>N06-1003</papid>
               ; Snover et al, 2009
               <papid>W09-0441</papid>
               ).
            </prevsent>
            <prevsent>We can treat the distribution of aligned French phrases as a semantic representation of the English phrase.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            The semantic distance between two English phrases can then be measured by their degree of overlap in this representation.In this work, we use the widely-used phrase extraction heuristic in (Koehn et al, 2003
            <papid>N03-1017</papid>
            ) to extract phrase pairs from parallel texts into a phrase table1.
         </citsent>
         <aftsection>
            <nextsent>The phrases extracted do not necessarily correspond to the speakers?</nextsent>
            <nextsent>intuition.</nextsent>
            <nextsent>Rather, they are units whose boundaries are preserved during translation.</nextsent>
            <nextsent>However, the distinction does not affect our work.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>Human evaluation.</section>
      <citcontext>
         <prevsection>
            <prevsent>The results are listed in Table 1.</prevsent>
            <prevsent>The inter-judge correlation is between 0.60 and 0.67 at the sentence level and above 0.99 at the system level.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1046 ">
            These correlation scores can be considered very high when compared to similar results reported in MT evaluations, e.g., Blatz et al (2003)
            <papid>C04-1046</papid>
            .
         </citsent>
         <aftsection>
            <nextsent>The high correlation confirms that our evaluation task is well defined.Having confirmed that human judgments correlate strongly, we combine the scores of the three judges by taking their arithmetic mean.</nextsent>
            <nextsent>Together with the three artificial control paraphrase systems, they form the human reference evaluation which we use for the remainder of the experiments.</nextsent>
            <nextsent>5.3 Adequacy, fluency, and dissimilarity.</nextsent>
            <nextsent>In this section, we empirically validate the importance of our three proposed criteria: adequacy, fluency, and lexical dissimilarity.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>PEM vs. human evaluation.</section>
      <citcontext>
         <prevsection>
            <prevsent>Both FBIS and MTC are in the Chinese newswire domain.</prevsent>
            <prevsent>We stem all English words in both datasets withthe Porter stemmer (Porter, 1980).</prevsent>
         </prevsection>
         <citsent citstr=" I05-3025 ">
            We use the maximum entropy segmenter of (Low et al, 2005
            <papid>I05-3025</papid>
            ) to segment the Chinese part of the FBIS corpus.
         </citsent>
         <aftsection>
            <nextsent>
               Subsequently, word level Chinese-English alignments are generated using the Berkeley aligner (Liang et al, 2006
               <papid>N06-1014</papid>
               ; Haghighi et al, 2009
               <papid>P09-1104</papid>
               ) with five iterations oftraining.
            </nextsent>
            <nextsent>Phrases are then extracted with the widely used heuristic in Koehn et al (2003).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>PEM vs. human evaluation.</section>
      <citcontext>
         <prevsection>
            <prevsent>We stem all English words in both datasets withthe Porter stemmer (Porter, 1980).</prevsent>
            <prevsent>
               We use the maximum entropy segmenter of (Low et al, 2005
               <papid>I05-3025</papid>
               ) to segment the Chinese part of the FBIS corpus.
            </prevsent>
         </prevsection>
         <citsent citstr=" N06-1014 ">
            Subsequently, word level Chinese-English alignments are generated using the Berkeley aligner (Liang et al, 2006
            <papid>N06-1014</papid>
            ; Haghighi et al, 2009
            <papid>P09-1104</papid>
            ) with five iterations oftraining.
         </citsent>
         <aftsection>
            <nextsent>Phrases are then extracted with the widely used heuristic in Koehn et al (2003).</nextsent>
            <nextsent>We extract phrases of up to four words in length.Bags of Chinese pivot language N-grams are extracted for all paraphrase pairs as described in Section 4.3.</nextsent>
            <nextsent>For computational efficiency, we consider only edges of the confusion network with probabilities higher than 0.1, and only N-grams with probabilities higher than 0.01 in the bag of N-grams.</nextsent>
            <nextsent>We collect N-grams up to length four.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D10-1090.xml">pem a paraphrase evaluation metric exploiting parallel texts</title>
      <section>PEM vs. human evaluation.</section>
      <citcontext>
         <prevsection>
            <prevsent>We stem all English words in both datasets withthe Porter stemmer (Porter, 1980).</prevsent>
            <prevsent>
               We use the maximum entropy segmenter of (Low et al, 2005
               <papid>I05-3025</papid>
               ) to segment the Chinese part of the FBIS corpus.
            </prevsent>
         </prevsection>
         <citsent citstr=" P09-1104 ">
            Subsequently, word level Chinese-English alignments are generated using the Berkeley aligner (Liang et al, 2006
            <papid>N06-1014</papid>
            ; Haghighi et al, 2009
            <papid>P09-1104</papid>
            ) with five iterations oftraining.
         </citsent>
         <aftsection>
            <nextsent>Phrases are then extracted with the widely used heuristic in Koehn et al (2003).</nextsent>
            <nextsent>We extract phrases of up to four words in length.Bags of Chinese pivot language N-grams are extracted for all paraphrase pairs as described in Section 4.3.</nextsent>
            <nextsent>For computational efficiency, we consider only edges of the confusion network with probabilities higher than 0.1, and only N-grams with probabilities higher than 0.01 in the bag of N-grams.</nextsent>
            <nextsent>We collect N-grams up to length four.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>Latent Variable Grammars.</section>
      <citcontext>
         <prevsection>
            <prevsent>20 10 20 30 40 50 60 N P V P PP A D V P A D JP S SB A R QP N N P JJ N N S N N RB V BN V BG V B IN CD V BD V BZ D T V BP Automatically determined number of subcategories figure 1: There is large variance in the number of subcategories (error bars correspond to one standard deviation).</prevsent>
            <prevsent>is only a weak correlation between the accuracies on the two evaluation sets (Pearson coefficient 0.34).This suggests that no single grammar should be preferred over the others.</prevsent>
         </prevsection>
         <citsent citstr=" N07-1051 ">
            In previous work (Petrov et al., 2006; Petrov and Klein, 2007
            <papid>N07-1051</papid>
            ) the final grammar was chosen based on its performance on a held-out set (section 22), and corresponds to the second best grammar in Figure 3 (because only 8 different grammars were trained).A more detailed error analysis is given in Figure 4, where we show a breakdown of F1 scores for selected phrasal categories in addition to the overall F1 score and exact match (on the WSJ development set).
         </citsent>
         <aftsection>
            <nextsent>While grammar G2 has the highest overall F1 score, its exact match is not particularly high, andit turns out to be the weakest at predicting quantifier phrases (QP).</nextsent>
            <nextsent>Similarly, the performance of the other grammars varies between the different error measures, indicating again that no single grammar dominates the others.</nextsent>
            <nextsent>It should be clear by now that simply varying the random seed used for initialization causes EM to discover very different latent variable grammars.</nextsent>
            <nextsent>While this behavior is worrisome in general, it turns out that we can use it to our advantage in this particular case.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>A Simple Product Model</section>
      <citcontext>
         <prevsection>
            <prevsent>To emphasize, we do not introduce any systematic bias (but see Section 4.3 for some experiments), or attempt to train the models to be maximally different (Hinton, 2002) ? we simply train a random collection of grammars by varying the random seed used for initialization.</prevsent>
            <prevsent>We found in our experiments that the randomness provided by EM is sufficient to achieve diversity among the individual grammars, and gives results that are as good as more involved training procedures.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3242 ">
            Xu and Jelinek (2004)
            <papid>W04-3242</papid>
            made a similar observation when learning random forests for language modeling.
         </citsent>
         <aftsection>
            <nextsent>Our model is reminiscent of Logarithmic Opinion pools (Bordley, 1982) and Products of Experts (Hin ton, 2001).3 However, because we believe that none of the underlying grammars should be favored, we deliberately do not use any combination weights.</nextsent>
            <nextsent>3.2 Inference.</nextsent>
            <nextsent>Computing the most likely parse tree is intractable for latent variable grammars (Simaan, 2002), and therefore also for our product model.</nextsent>
            <nextsent>This is because there are exponentially many derivations over split subcategories that correspond to a single parse tree over unsplit categories, and there is no dynamic program to efficiently marginalize out the latent variables.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>A Simple Product Model</section>
      <citcontext>
         <prevsection>
            <prevsent>Previous work on parse risk minimization has addressed this problem in two different ways: by changing the objective function, or by constraining3As a matter of fact, Hinton (2001) mentions syntactic parsing as one of the motivating examples for Products of Experts.</prevsent>
            <prevsent>G1 G2 G3 G4 P 90% 91.5% 93% F1 Score G1 G2 G3 G4 P 40% 45% 50% Exact Match G1 G2 G3 G4 P 91% 93% 95% NP G1 G2 G3 G4 P 90% 92% 94% VP G1 G2 G3 G4 P 85% 88% 91% PP G1 G2 G3 G4 P 90% 92.5% 95% QP Figure 4: Breakdown of different accuracy measures for four randomly selected grammars (G1-G4), as well as a product model (P) that uses those four grammars.</prevsent>
         </prevsection>
         <citsent citstr=" P96-1024 ">
            Note that no single grammar does well on all measures, while the product model does significantly better on all.the search space (Goodman, 1996
            <papid>P96-1024</papid>
            ; Titov and Henderson, 2006; Petrov and Klein, 2007
            <papid>N07-1051</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The simplest approach is to stick to likelihood as the objective function, but to limit the search space to a set of high quality candidates T : T ? = argmax TT P(T |w) (3) Because the likelihood of a given parse tree can be computed exactly for our product model (Eq.</nextsent>
            <nextsent>2), the quality of this approximation is only limited by thequality of the candidate list.</nextsent>
            <nextsent>To generate the candidate list, we produce k-best lists of Viterbi derivations with the efficient algorithm of Huang and Chiang (2005), and erase the subcategory information to obtain parse trees over unsplit categories.</nextsent>
            <nextsent>We refer to this approximation as TREE-LEVEL inference, because it considers a list of complete trees from the underlying grammars, and selects the tree that has the highest likelihood under the product model.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>A Simple Product Model</section>
      <citcontext>
         <prevsection>
            <prevsent>Previous work on parse risk minimization has addressed this problem in two different ways: by changing the objective function, or by constraining3As a matter of fact, Hinton (2001) mentions syntactic parsing as one of the motivating examples for Products of Experts.</prevsent>
            <prevsent>G1 G2 G3 G4 P 90% 91.5% 93% F1 Score G1 G2 G3 G4 P 40% 45% 50% Exact Match G1 G2 G3 G4 P 91% 93% 95% NP G1 G2 G3 G4 P 90% 92% 94% VP G1 G2 G3 G4 P 85% 88% 91% PP G1 G2 G3 G4 P 90% 92.5% 95% QP Figure 4: Breakdown of different accuracy measures for four randomly selected grammars (G1-G4), as well as a product model (P) that uses those four grammars.</prevsent>
         </prevsection>
         <citsent citstr=" N07-1051 ">
            Note that no single grammar does well on all measures, while the product model does significantly better on all.the search space (Goodman, 1996
            <papid>P96-1024</papid>
            ; Titov and Henderson, 2006; Petrov and Klein, 2007
            <papid>N07-1051</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The simplest approach is to stick to likelihood as the objective function, but to limit the search space to a set of high quality candidates T : T ? = argmax TT P(T |w) (3) Because the likelihood of a given parse tree can be computed exactly for our product model (Eq.</nextsent>
            <nextsent>2), the quality of this approximation is only limited by thequality of the candidate list.</nextsent>
            <nextsent>To generate the candidate list, we produce k-best lists of Viterbi derivations with the efficient algorithm of Huang and Chiang (2005), and erase the subcategory information to obtain parse trees over unsplit categories.</nextsent>
            <nextsent>We refer to this approximation as TREE-LEVEL inference, because it considers a list of complete trees from the underlying grammars, and selects the tree that has the highest likelihood under the product model.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>Unless noted otherwise, we use CONSTITUENT-LEVEL inference.</prevsent>
            <prevsent>All our experiments are based on the publicly available BerkeleyParser.4 4http://code.google.com/p/berkeleyparser 23 Training Set Dev.</prevsent>
         </prevsection>
         <citsent citstr=" A97-1014 ">
            Set Test Set ENGLISH-WSJ Sections Section 22 Section 23(Marcus et al, 1993) 2-21 ENGLISH-BROWN see 10% of 10% of the (Francis et al 1979) ENGLISH-WSJ the data5 the data5 GERMAN Sentences Sentences Sentences (Skut et al, 1997
            <papid>A97-1014</papid>
            ) 1-18,602 18,603-19,602 19,603-20,602 Table 1: Corpora and standard experimental setups.
         </citsent>
         <aftsection>
            <nextsent>4.1 (Weighted) Product vs.</nextsent>
            <nextsent>(Weighted) SumA great deal has been written on the topic of products versus sums of probability distributions for joint prediction (Genest and Zidek, 1986; Tax et al, 2000).</nextsent>
            <nextsent>However, those theoretical results do not apply directly here, because we are using multiple randomly permuted models from the same class, rather models from different classes.</nextsent>
            <nextsent>To shed some light on this issue, we addressed the question empirically, and combined two grammars into an unweighted product model, and also an unweighted sum model.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>Using weights learned on a held-out set and rescoring 50 best lists from Charniak (2000) and Petrov et al (2006), they obtain an F1 score of 91.0 (which they further improve to 91.4 using a voting scheme).</prevsent>
            <prevsent>We replicated their experiment, but used an unweighted product of the two model scores.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1003 ">
            Using TREE LEVEL inference, we obtained an F1 score of 91.6, suggesting that weighting is not so important in the product case, as long as the classifiers are of comparable quality.6 This is in line with previous work on product models, where weighting has been important when combining heterogenous classifiers (Hes kes, 1998), and less important when the classifiers are of similar accuracy (Smith et al, 2005
            <papid>P05-1003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               5See Gildea (2001)
               <papid>W01-0521</papid>
               for the exact setup.
            </nextsent>
            <nextsent>6The unweighted sum model, however, underperforms the individual models with an F1 score of only 90.3.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>We replicated their experiment, but used an unweighted product of the two model scores.</prevsent>
            <prevsent>
               Using TREE LEVEL inference, we obtained an F1 score of 91.6, suggesting that weighting is not so important in the product case, as long as the classifiers are of comparable quality.6 This is in line with previous work on product models, where weighting has been important when combining heterogenous classifiers (Hes kes, 1998), and less important when the classifiers are of similar accuracy (Smith et al, 2005
               <papid>P05-1003</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" W01-0521 ">
            5See Gildea (2001)
            <papid>W01-0521</papid>
            for the exact setup.
         </citsent>
         <aftsection>
            <nextsent>6The unweighted sum model, however, underperforms the individual models with an F1 score of only 90.3.</nextsent>
            <nextsent>90.5 91 91.5 92 92.5 1 2 4 8 16 Number of grammars in product model Parsing accuracy on the WSJ development set Constituent-Level Inference Tree-Level Inference Figure 6: Adding more grammars to the product model improves parsing accuracy, while CONSTITUENT-LEVEL inference gives consistently better results.</nextsent>
            <nextsent>4.2 Tree-Level vs. Constituent-Level Inference.</nextsent>
            <nextsent>Figure 6 shows that accuracy increases when more grammars are added to the product model, but levels off after eight grammars.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>How ever, this is not the case, as the results hold even when the candidate set for CONSTITUENT-LEVEL inference is constrained to trees from the k-best lists.While the packed forrest representation can very efficiently encode an exponential set of parse trees, inour case the k-best lists appear to be already very diverse because they are generated by multiple grammars.</prevsent>
            <prevsent>Starting at 96.1 for a single latent variable grammar, merging two 50-best lists from different grammars gives an oracle score of 97.4, and adding more k-best lists further improves the oracle score to 98.6 for 16 grammars.</prevsent>
         </prevsection>
         <citsent citstr=" P08-1067 ">
            This compares favorably to the results of Huang (2008)
            <papid>P08-1067</papid>
            , where the oracle score over a pruned forest is shown to be 97.8 (compared to 96.7 for a 50-best list).The accuracy improvement can instead be explained by the change in the objective function.
         </citsent>
         <aftsection>
            <nextsent>Recall from section Section 3.2, that CONSTITUENT LEVEL inference maximizes the expected number of correct productions, while TREE-LEVEL inference maximizes tree-likelihood.</nextsent>
            <nextsent>It is therefore nottoo surprising that the two objective functions select the same tree only 41% of the time, even when limited to the same candidate set.</nextsent>
            <nextsent>Maximizing the 24 expected number of correct productions is superior for F1 score (see the one grammar case in Figure 6).</nextsent>
            <nextsent>However, as to be expected, likelihood is better for exact match, giving a score of 47.6% vs. 46.8%.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>This approach has the disadvantage that it reduces the performance of the individual models, and is not directly applicable for latent variable grammars because the features are automatically learned.</prevsent>
            <prevsent>Alternatively, onecan introduce diversity by changing the training distribution.</prevsent>
         </prevsection>
         <citsent citstr=" A00-2005 ">
            Bagging (Breiman, 1996) and Boosting (Freund and Shapire, 1996) fall into this category,but have had limited success for parsing (Henderson and Brill, 2000
            <papid>A00-2005</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Furthermore boosting is im practical here, because it requires training dozens of grammars in sequence.</nextsent>
            <nextsent>Since training a single grammar takes roughly one day, we opted for a different, parallelizable way ofchanging the training distribution.</nextsent>
            <nextsent>In a first experiment, we divided the training set into two disjoint sets, and trained separate grammars on each half.</nextsent>
            <nextsent>These truly disjoint grammars had low F1 scores of 89.4 and 89.6 respectively (because they weretrained on less data).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>While we use multiple grammars in our work, all grammars are from the same model class for us.</prevsent>
            <prevsent>In contrast, those methods relyon a diverse set of individual parsers, each of which requires a significant effort to build.</prevsent>
         </prevsection>
         <citsent citstr=" W99-0623 ">
            Furthermore, those techniques have largely relied on different voting schemes in the past (Henderson and Brill, 1999
            <papid>W99-0623</papid>
            ; Sagae and Lavie,2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009
            <papid>N09-2064</papid>
            ; Zhang et al, 2009
            <papid>D09-1161</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars.</nextsent>
            <nextsent>
               It is also interesting to note that the best resultsin Zhang et al (2009) are achieved by combining k best lists from a latent variable grammar of Petrov et al (2006) with the self-trained reranking parser ofMcClosky et al (2006)
               <papid>N06-1020</papid>
               .
            </nextsent>
            <nextsent>Clearly, replacing the single latent variable grammar with a product of latent variable grammars ought to improve performance.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>While we use multiple grammars in our work, all grammars are from the same model class for us.</prevsent>
            <prevsent>In contrast, those methods relyon a diverse set of individual parsers, each of which requires a significant effort to build.</prevsent>
         </prevsection>
         <citsent citstr=" N09-2064 ">
            Furthermore, those techniques have largely relied on different voting schemes in the past (Henderson and Brill, 1999
            <papid>W99-0623</papid>
            ; Sagae and Lavie,2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009
            <papid>N09-2064</papid>
            ; Zhang et al, 2009
            <papid>D09-1161</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars.</nextsent>
            <nextsent>
               It is also interesting to note that the best resultsin Zhang et al (2009) are achieved by combining k best lists from a latent variable grammar of Petrov et al (2006) with the self-trained reranking parser ofMcClosky et al (2006)
               <papid>N06-1020</papid>
               .
            </nextsent>
            <nextsent>Clearly, replacing the single latent variable grammar with a product of latent variable grammars ought to improve performance.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>While we use multiple grammars in our work, all grammars are from the same model class for us.</prevsent>
            <prevsent>In contrast, those methods relyon a diverse set of individual parsers, each of which requires a significant effort to build.</prevsent>
         </prevsection>
         <citsent citstr=" D09-1161 ">
            Furthermore, those techniques have largely relied on different voting schemes in the past (Henderson and Brill, 1999
            <papid>W99-0623</papid>
            ; Sagae and Lavie,2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009
            <papid>N09-2064</papid>
            ; Zhang et al, 2009
            <papid>D09-1161</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars.</nextsent>
            <nextsent>
               It is also interesting to note that the best resultsin Zhang et al (2009) are achieved by combining k best lists from a latent variable grammar of Petrov et al (2006) with the self-trained reranking parser ofMcClosky et al (2006)
               <papid>N06-1020</papid>
               .
            </nextsent>
            <nextsent>Clearly, replacing the single latent variable grammar with a product of latent variable grammars ought to improve performance.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" N10-1003.xml">products of random latent variable grammars</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Furthermore, those techniques have largely relied on different voting schemes in the past (Henderson and Brill, 1999
               <papid>W99-0623</papid>
               ; Sagae and Lavie,2006), and only more recently have started using actual posteriors from the underlying models (Fossum and Knight, 2009
               <papid>N09-2064</papid>
               ; Zhang et al, 2009
               <papid>D09-1161</papid>
               ).
            </prevsent>
            <prevsent>Even then, those methods operate only over k-best lists, and we are the first to work directly with parse forests from multiple grammars.</prevsent>
         </prevsection>
         <citsent citstr=" N06-1020 ">
            It is also interesting to note that the best resultsin Zhang et al (2009) are achieved by combining k best lists from a latent variable grammar of Petrov et al (2006) with the self-trained reranking parser ofMcClosky et al (2006)
            <papid>N06-1020</papid>
            .
         </citsent>
         <aftsection>
            <nextsent>Clearly, replacing the single latent variable grammar with a product of latent variable grammars ought to improve performance.</nextsent>
            <nextsent>The results on the other two corpora are similar.A product of latent variable grammars very significantly outperforms a single latent variable grammar and sets new standards for the state-of-the-art.We also analyzed the errors of the product models.</nextsent>
            <nextsent>In addition to the illustrative example in Figure 5, we computed detailed error metrics for different phrasal categories.</nextsent>
            <nextsent>Figure 4 shows that a product of four random grammars is always better than eventhe best underlying grammar.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Rather than filter out all non-syntactic phrases, we only apply syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words.</prevsent>
            <prevsent>Our method is very simple and yields a 24.38% phrase pair reduction and a 0.52 BLEU point improvement when compared to a baseline PBMT system with full-size tables.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            Both PBMT models (Koehn et al, 2003
            <papid>N03-1017</papid>
            ; Chiang, 2005
            <papid>P12-2057</papid>
            ) and syntax-based machine translation models (Yamada et al, 2000; Quirk et al, 2005
            <papid>P05-1034</papid>
            ; Galley et al, 2006
            <papid>P06-1121</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Marcu et al, 2006
            <papid>W06-1606</papid>
            ; and numerous others) are the state-of-the- art statistical machine translation (SMT) meth ods.
         </citsent>
         <aftsection>
            <nextsent>Over the last several years, an increasing amount of work has been done to combine the advantages of the two approaches.</nextsent>
            <nextsent>
               DeNeefe et al (2007)
               <papid>D07-1079</papid>
               made a quantitative comparison of the phrase pairs that each model has to work with and found it is useful to improve the phrasal coverage of their string-to-tree model.
            </nextsent>
            <nextsent>
               Liu et al (2007)
               <papid>P07-1089</papid>
               proposed forest-to-string rules to capture the non-syntactic phrases in their tree-to-string model.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Rather than filter out all non-syntactic phrases, we only apply syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words.</prevsent>
            <prevsent>Our method is very simple and yields a 24.38% phrase pair reduction and a 0.52 BLEU point improvement when compared to a baseline PBMT system with full-size tables.</prevsent>
         </prevsection>
         <citsent citstr=" P12-2057 ">
            Both PBMT models (Koehn et al, 2003
            <papid>N03-1017</papid>
            ; Chiang, 2005
            <papid>P12-2057</papid>
            ) and syntax-based machine translation models (Yamada et al, 2000; Quirk et al, 2005
            <papid>P05-1034</papid>
            ; Galley et al, 2006
            <papid>P06-1121</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Marcu et al, 2006
            <papid>W06-1606</papid>
            ; and numerous others) are the state-of-the- art statistical machine translation (SMT) meth ods.
         </citsent>
         <aftsection>
            <nextsent>Over the last several years, an increasing amount of work has been done to combine the advantages of the two approaches.</nextsent>
            <nextsent>
               DeNeefe et al (2007)
               <papid>D07-1079</papid>
               made a quantitative comparison of the phrase pairs that each model has to work with and found it is useful to improve the phrasal coverage of their string-to-tree model.
            </nextsent>
            <nextsent>
               Liu et al (2007)
               <papid>P07-1089</papid>
               proposed forest-to-string rules to capture the non-syntactic phrases in their tree-to-string model.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Rather than filter out all non-syntactic phrases, we only apply syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words.</prevsent>
            <prevsent>Our method is very simple and yields a 24.38% phrase pair reduction and a 0.52 BLEU point improvement when compared to a baseline PBMT system with full-size tables.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1034 ">
            Both PBMT models (Koehn et al, 2003
            <papid>N03-1017</papid>
            ; Chiang, 2005
            <papid>P12-2057</papid>
            ) and syntax-based machine translation models (Yamada et al, 2000; Quirk et al, 2005
            <papid>P05-1034</papid>
            ; Galley et al, 2006
            <papid>P06-1121</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Marcu et al, 2006
            <papid>W06-1606</papid>
            ; and numerous others) are the state-of-the- art statistical machine translation (SMT) meth ods.
         </citsent>
         <aftsection>
            <nextsent>Over the last several years, an increasing amount of work has been done to combine the advantages of the two approaches.</nextsent>
            <nextsent>
               DeNeefe et al (2007)
               <papid>D07-1079</papid>
               made a quantitative comparison of the phrase pairs that each model has to work with and found it is useful to improve the phrasal coverage of their string-to-tree model.
            </nextsent>
            <nextsent>
               Liu et al (2007)
               <papid>P07-1089</papid>
               proposed forest-to-string rules to capture the non-syntactic phrases in their tree-to-string model.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Rather than filter out all non-syntactic phrases, we only apply syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words.</prevsent>
            <prevsent>Our method is very simple and yields a 24.38% phrase pair reduction and a 0.52 BLEU point improvement when compared to a baseline PBMT system with full-size tables.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1121 ">
            Both PBMT models (Koehn et al, 2003
            <papid>N03-1017</papid>
            ; Chiang, 2005
            <papid>P12-2057</papid>
            ) and syntax-based machine translation models (Yamada et al, 2000; Quirk et al, 2005
            <papid>P05-1034</papid>
            ; Galley et al, 2006
            <papid>P06-1121</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Marcu et al, 2006
            <papid>W06-1606</papid>
            ; and numerous others) are the state-of-the- art statistical machine translation (SMT) meth ods.
         </citsent>
         <aftsection>
            <nextsent>Over the last several years, an increasing amount of work has been done to combine the advantages of the two approaches.</nextsent>
            <nextsent>
               DeNeefe et al (2007)
               <papid>D07-1079</papid>
               made a quantitative comparison of the phrase pairs that each model has to work with and found it is useful to improve the phrasal coverage of their string-to-tree model.
            </nextsent>
            <nextsent>
               Liu et al (2007)
               <papid>P07-1089</papid>
               proposed forest-to-string rules to capture the non-syntactic phrases in their tree-to-string model.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Rather than filter out all non-syntactic phrases, we only apply syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words.</prevsent>
            <prevsent>Our method is very simple and yields a 24.38% phrase pair reduction and a 0.52 BLEU point improvement when compared to a baseline PBMT system with full-size tables.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1077 ">
            Both PBMT models (Koehn et al, 2003
            <papid>N03-1017</papid>
            ; Chiang, 2005
            <papid>P12-2057</papid>
            ) and syntax-based machine translation models (Yamada et al, 2000; Quirk et al, 2005
            <papid>P05-1034</papid>
            ; Galley et al, 2006
            <papid>P06-1121</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Marcu et al, 2006
            <papid>W06-1606</papid>
            ; and numerous others) are the state-of-the- art statistical machine translation (SMT) meth ods.
         </citsent>
         <aftsection>
            <nextsent>Over the last several years, an increasing amount of work has been done to combine the advantages of the two approaches.</nextsent>
            <nextsent>
               DeNeefe et al (2007)
               <papid>D07-1079</papid>
               made a quantitative comparison of the phrase pairs that each model has to work with and found it is useful to improve the phrasal coverage of their string-to-tree model.
            </nextsent>
            <nextsent>
               Liu et al (2007)
               <papid>P07-1089</papid>
               proposed forest-to-string rules to capture the non-syntactic phrases in their tree-to-string model.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Rather than filter out all non-syntactic phrases, we only apply syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words.</prevsent>
            <prevsent>Our method is very simple and yields a 24.38% phrase pair reduction and a 0.52 BLEU point improvement when compared to a baseline PBMT system with full-size tables.</prevsent>
         </prevsection>
         <citsent citstr=" W06-1606 ">
            Both PBMT models (Koehn et al, 2003
            <papid>N03-1017</papid>
            ; Chiang, 2005
            <papid>P12-2057</papid>
            ) and syntax-based machine translation models (Yamada et al, 2000; Quirk et al, 2005
            <papid>P05-1034</papid>
            ; Galley et al, 2006
            <papid>P06-1121</papid>
            ; Liu et al, 2006
            <papid>P06-1077</papid>
            ; Marcu et al, 2006
            <papid>W06-1606</papid>
            ; and numerous others) are the state-of-the- art statistical machine translation (SMT) meth ods.
         </citsent>
         <aftsection>
            <nextsent>Over the last several years, an increasing amount of work has been done to combine the advantages of the two approaches.</nextsent>
            <nextsent>
               DeNeefe et al (2007)
               <papid>D07-1079</papid>
               made a quantitative comparison of the phrase pairs that each model has to work with and found it is useful to improve the phrasal coverage of their string-to-tree model.
            </nextsent>
            <nextsent>
               Liu et al (2007)
               <papid>P07-1089</papid>
               proposed forest-to-string rules to capture the non-syntactic phrases in their tree-to-string model.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Both PBMT models (Koehn et al, 2003
               <papid>N03-1017</papid>
               ; Chiang, 2005
               <papid>P12-2057</papid>
               ) and syntax-based machine translation models (Yamada et al, 2000; Quirk et al, 2005
               <papid>P05-1034</papid>
               ; Galley et al, 2006
               <papid>P06-1121</papid>
               ; Liu et al, 2006
               <papid>P06-1077</papid>
               ; Marcu et al, 2006
               <papid>W06-1606</papid>
               ; and numerous others) are the state-of-the- art statistical machine translation (SMT) meth ods.
            </prevsent>
            <prevsent>Over the last several years, an increasing amount of work has been done to combine the advantages of the two approaches.</prevsent>
         </prevsection>
         <citsent citstr=" D07-1079 ">
            DeNeefe et al (2007)
            <papid>D07-1079</papid>
            made a quantitative comparison of the phrase pairs that each model has to work with and found it is useful to improve the phrasal coverage of their string-to-tree model.
         </citsent>
         <aftsection>
            <nextsent>
               Liu et al (2007)
               <papid>P07-1089</papid>
               proposed forest-to-string rules to capture the non-syntactic phrases in their tree-to-string model.
            </nextsent>
            <nextsent>
               Zhang et al (2008)
               <papid>P08-1064</papid>
               proposed a tree sequence based tree-to-tree model which can describe non-syntactic phrases with syntactic structure information.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Over the last several years, an increasing amount of work has been done to combine the advantages of the two approaches.</prevsent>
            <prevsent>
               DeNeefe et al (2007)
               <papid>D07-1079</papid>
               made a quantitative comparison of the phrase pairs that each model has to work with and found it is useful to improve the phrasal coverage of their string-to-tree model.
            </prevsent>
         </prevsection>
         <citsent citstr=" P07-1089 ">
            Liu et al (2007)
            <papid>P07-1089</papid>
            proposed forest-to-string rules to capture the non-syntactic phrases in their tree-to-string model.
         </citsent>
         <aftsection>
            <nextsent>
               Zhang et al (2008)
               <papid>P08-1064</papid>
               proposed a tree sequence based tree-to-tree model which can describe non-syntactic phrases with syntactic structure information.
            </nextsent>
            <nextsent>The converse of the above methods is to incorporate syntactic information into the PBMT model.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               DeNeefe et al (2007)
               <papid>D07-1079</papid>
               made a quantitative comparison of the phrase pairs that each model has to work with and found it is useful to improve the phrasal coverage of their string-to-tree model.
            </prevsent>
            <prevsent>
               Liu et al (2007)
               <papid>P07-1089</papid>
               proposed forest-to-string rules to capture the non-syntactic phrases in their tree-to-string model.
            </prevsent>
         </prevsection>
         <citsent citstr=" P08-1064 ">
            Zhang et al (2008)
            <papid>P08-1064</papid>
            proposed a tree sequence based tree-to-tree model which can describe non-syntactic phrases with syntactic structure information.
         </citsent>
         <aftsection>
            <nextsent>The converse of the above methods is to incorporate syntactic information into the PBMT model.</nextsent>
            <nextsent>
               Zollmann and Venugopal (2006)
               <papid>W06-3119</papid>
               started with a complete set of phrases as extracted by traditional PBMT heuristics, and then annotated the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span.
            </nextsent>
            <nextsent>
               Marton and Resnik (2008)
               <papid>D10-1014</papid>
               and Cherry (2008)
               <papid>P08-1009</papid>
               imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Zhang et al (2008)
               <papid>P08-1064</papid>
               proposed a tree sequence based tree-to-tree model which can describe non-syntactic phrases with syntactic structure information.
            </prevsent>
            <prevsent>The converse of the above methods is to incorporate syntactic information into the PBMT model.</prevsent>
         </prevsection>
         <citsent citstr=" W06-3119 ">
            Zollmann and Venugopal (2006)
            <papid>W06-3119</papid>
            started with a complete set of phrases as extracted by traditional PBMT heuristics, and then annotated the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span.
         </citsent>
         <aftsection>
            <nextsent>
               Marton and Resnik (2008)
               <papid>D10-1014</papid>
               and Cherry (2008)
               <papid>P08-1009</papid>
               imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis.
            </nextsent>
            <nextsent>In their PBMT decoders, a candidate translation gets an extra credit if it respects the source side syntactic parse tree but may incur a cost if it violates a constituent boundary.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The converse of the above methods is to incorporate syntactic information into the PBMT model.</prevsent>
            <prevsent>
               Zollmann and Venugopal (2006)
               <papid>W06-3119</papid>
               started with a complete set of phrases as extracted by traditional PBMT heuristics, and then annotated the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span.
            </prevsent>
         </prevsection>
         <citsent citstr=" D10-1014 ">
            Marton and Resnik (2008)
            <papid>D10-1014</papid>
            and Cherry (2008)
            <papid>P08-1009</papid>
            imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis.
         </citsent>
         <aftsection>
            <nextsent>In their PBMT decoders, a candidate translation gets an extra credit if it respects the source side syntactic parse tree but may incur a cost if it violates a constituent boundary.</nextsent>
            <nextsent>
               Xiong et al (2009)
               <papid>P09-1036</papid>
               proposed a syn tax-driven bracketing model to predict whether a phrase (a sequence of contiguous words) is bracketable or not using rich syntactic con straints.
            </nextsent>
            <nextsent>In this paper, we try to utilize syntactic knowledge to constrain the phrase extraction from word-based alignments for PBMT system.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The converse of the above methods is to incorporate syntactic information into the PBMT model.</prevsent>
            <prevsent>
               Zollmann and Venugopal (2006)
               <papid>W06-3119</papid>
               started with a complete set of phrases as extracted by traditional PBMT heuristics, and then annotated the target side of each phrasal entry with the label of the constituent node in the target-side parse tree that subsumes the span.
            </prevsent>
         </prevsection>
         <citsent citstr=" P08-1009 ">
            Marton and Resnik (2008)
            <papid>D10-1014</papid>
            and Cherry (2008)
            <papid>P08-1009</papid>
            imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis.
         </citsent>
         <aftsection>
            <nextsent>In their PBMT decoders, a candidate translation gets an extra credit if it respects the source side syntactic parse tree but may incur a cost if it violates a constituent boundary.</nextsent>
            <nextsent>
               Xiong et al (2009)
               <papid>P09-1036</papid>
               proposed a syn tax-driven bracketing model to predict whether a phrase (a sequence of contiguous words) is bracketable or not using rich syntactic con straints.
            </nextsent>
            <nextsent>In this paper, we try to utilize syntactic knowledge to constrain the phrase extraction from word-based alignments for PBMT system.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Marton and Resnik (2008)
               <papid>D10-1014</papid>
               and Cherry (2008)
               <papid>P08-1009</papid>
               imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis.
            </prevsent>
            <prevsent>In their PBMT decoders, a candidate translation gets an extra credit if it respects the source side syntactic parse tree but may incur a cost if it violates a constituent boundary.</prevsent>
         </prevsection>
         <citsent citstr=" P09-1036 ">
            Xiong et al (2009)
            <papid>P09-1036</papid>
            proposed a syn tax-driven bracketing model to predict whether a phrase (a sequence of contiguous words) is bracketable or not using rich syntactic con straints.
         </citsent>
         <aftsection>
            <nextsent>In this paper, we try to utilize syntactic knowledge to constrain the phrase extraction from word-based alignments for PBMT system.</nextsent>
            <nextsent>Rather than filter out all non-syntactic phrases, we only apply syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words.</nextsent>
            <nextsent>Our method is very simple and yields a 24.38% phrase pair reduction and a 0.52 BLEU point improvement when compared to the baseline PBMT system with full-size tables.</nextsent>
            <nextsent>based Alignments In this section, we briefly review a simple and effective phrase pair extraction algorithm upon which this work builds.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Extracting Phrase Pairs from Word-.</section>
      <citcontext>
         <prevsection>
            <prevsent>Figure 1: An example parallel sentence pair and word alignment Since there is no phrase segmentation information in the word-aligned sentence pair, in practice all pairs of source word sequence ||| target word sequence?</prevsent>
            <prevsent>that are consistent with word alignments are collected.</prevsent>
         </prevsection>
         <citsent citstr=" W99-0604 ">
            The words in a legal phrase pair are only aligned to each other, and not to words outside (Och et al, 1999
            <papid>W99-0604</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>For example, given a sentence pair and its word alignments shown in Figure1, the following nine phrase pairs will be extracted: Source phrase ||| Target phrase f1 ||| e1 f2 ||| e2 f4 ||| e3 f1 f2 ||| e1 e2 f2 f3 ||| e2 f3 f4 ||| e3 f1 f2 f3 ||| e1 e2 f2 f3 f4 ||| e2 e3 f1 f2 f3 f4 ||| e1 e2 e3 Table 1: Phrase pairs extracted from the example in Figure 1 Note that neither the source phrase nor the target phrase can be empty.</nextsent>
            <nextsent>So f3 ||| EMPTY?</nextsent>
            <nextsent>is not a legal phrase pair.</nextsent>
            <nextsent>Phrase pairs are extracted over the entire training corpus.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Extracting Phrase Pairs from Word-.</section>
      <citcontext>
         <prevsection>
            <prevsent>This is not desirable in real application where speed and memory consumption are often critical concerns.</prevsent>
            <prevsent>In addition, some phrase translation pairs are generated from training data errors and word alignment noise.</prevsent>
         </prevsection>
         <citsent citstr=" D07-1103 ">
            Therefore, we need to filter the phrase table in an appropriate way for both efficiency and translation quality (Johnson et al, 2007
            <papid>D07-1103</papid>
            ; Yang and Zheng, 2009
            <papid>P09-2060</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>f1 f2 f3 f4 | | | e1 e2 e3</nextsent>
            <nextsent>Extraction We can divide all the possible phrases into two types: syntactic phrases and non-syntactic phrases.</nextsent>
            <nextsent>A syntactic phrase?</nextsent>
            <nextsent>is defined as a word sequence that is covered by a single sub tree in a syntactic parse tree (Imamura, 2002).</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Extracting Phrase Pairs from Word-.</section>
      <citcontext>
         <prevsection>
            <prevsent>This is not desirable in real application where speed and memory consumption are often critical concerns.</prevsent>
            <prevsent>In addition, some phrase translation pairs are generated from training data errors and word alignment noise.</prevsent>
         </prevsection>
         <citsent citstr=" P09-2060 ">
            Therefore, we need to filter the phrase table in an appropriate way for both efficiency and translation quality (Johnson et al, 2007
            <papid>D07-1103</papid>
            ; Yang and Zheng, 2009
            <papid>P09-2060</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>f1 f2 f3 f4 | | | e1 e2 e3</nextsent>
            <nextsent>Extraction We can divide all the possible phrases into two types: syntactic phrases and non-syntactic phrases.</nextsent>
            <nextsent>A syntactic phrase?</nextsent>
            <nextsent>is defined as a word sequence that is covered by a single sub tree in a syntactic parse tree (Imamura, 2002).</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Syntactic Constraints on Phrase Pair.</section>
      <citcontext>
         <prevsection>
            <prevsent>is defined as a word sequence that is covered by a single sub tree in a syntactic parse tree (Imamura, 2002).</prevsent>
            <prevsent>Intuitively, we would think syntactic phrases are much more reliable while the non-syntactic phrases are useless.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            However, (Koehn et al, 2003
            <papid>N03-1017</papid>
            ) showed that restricting phrasal translation to only syntactic phrases yields poor translation performance ? the ability to translate non syntactic phrases (such as there are?, note that?, and according to?)
         </citsent>
         <aftsection>
            <nextsent>turns out to be critical and pervasive.</nextsent>
            <nextsent>
               (Koehn et al, 2003
               <papid>N03-1017</papid>
               ) uses syntactic constraints from both the source and target languages, and over 80% of all phrase pairs are eliminated.
            </nextsent>
            <nextsent>In this section, we try to use syntactic knowledge in a less restrictive way.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>So our algorithm uses prior syntactic knowledge to keep f3 f4 ||| e3?</prevsent>
            <prevsent>and exclude f2 f3 ||| e2?.</prevsent>
         </prevsection>
         <citsent citstr=" W08-0334 ">
            Our SMT system is based on a fairly typical phrase-based model (Finch and Sumita, 2008
            <papid>W08-0334</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>For the training of our SMT model, we use a modified training toolkit adapted from the MOSES decoder.</nextsent>
            <nextsent>Our decoder can operate on the same principles as the MOSES decoder.</nextsent>
            <nextsent>
               Minimum error rate training (MERT) with respect to BLEU score is used to tune the decoders parameters, and it is performed using the standard technique of Och (2003)
               <papid>P03-1021</papid>
               .
            </nextsent>
            <nextsent>A lexicalized reordering model was built by using the msdbidirectional-fe?</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>For the training of our SMT model, we use a modified training toolkit adapted from the MOSES decoder.</prevsent>
            <prevsent>Our decoder can operate on the same principles as the MOSES decoder.</prevsent>
         </prevsection>
         <citsent citstr=" P03-1021 ">
            Minimum error rate training (MERT) with respect to BLEU score is used to tune the decoders parameters, and it is performed using the standard technique of Och (2003)
            <papid>P03-1021</papid>
            .
         </citsent>
         <aftsection>
            <nextsent>A lexicalized reordering model was built by using the msdbidirectional-fe?</nextsent>
            <nextsent>configuration in our experiments.</nextsent>
            <nextsent>The translation model was created from the FBIS parallel corpus.</nextsent>
            <nextsent>We used a 5-gram language model trained with modified Kneser-Ney smoothing.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>With the exception of the above differences in phrase translation pair extraction, all the other settings were the identical in the three experiments.</prevsent>
            <prevsent>Table 5 summarizes the SMT performance.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1072 ">
            The evaluation metric is case sensitive BLEU-4 (Papineni et al, 2002
            <papid>C04-1072</papid>
            ) which estimates the accuracy of translation output with respect to a set of reference translations.
         </citsent>
         <aftsection>
            <nextsent>Syntactic Constraints Number of distinct phrase pairs BLEU None 14,195,686 17.26 Full constraint 4,855,108 16.51 Selectively constraint 10,733,731 17.78 Table 5: Comparison of different constraints on phrase pair extraction by translation quality As shown in the table, it is harmful to fully apply syntactic constraints on phrase extraction, even just on the source language side.</nextsent>
            <nextsent>
               This is consistent with the observation of (Koehn et al, 2003
               <papid>N03-1017</papid>
               ) who applied both source and target constraints in German to English translation ex periments.
            </nextsent>
            <nextsent>Clearly, we obtained the best performance if we use source language syntactic constraints only on phrases whose first or last source word is unaligned.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The evaluation metric is case sensitive BLEU-4 (Papineni et al, 2002
               <papid>C04-1072</papid>
               ) which estimates the accuracy of translation output with respect to a set of reference translations.
            </prevsent>
            <prevsent>Syntactic Constraints Number of distinct phrase pairs BLEU None 14,195,686 17.26 Full constraint 4,855,108 16.51 Selectively constraint 10,733,731 17.78 Table 5: Comparison of different constraints on phrase pair extraction by translation quality As shown in the table, it is harmful to fully apply syntactic constraints on phrase extraction, even just on the source language side.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            This is consistent with the observation of (Koehn et al, 2003
            <papid>N03-1017</papid>
            ) who applied both source and target constraints in German to English translation ex periments.
         </citsent>
         <aftsection>
            <nextsent>Clearly, we obtained the best performance if we use source language syntactic constraints only on phrases whose first or last source word is unaligned.</nextsent>
            <nextsent>In addition, we reduced the number of distinct phrase pairs by 24.38% over the base line full-size phrase table.</nextsent>
            <nextsent>The results in table 5 show that while some non-syntactic phrases are very important to maintain the performance of a PBMT system, not all of them are necessary.</nextsent>
            <nextsent>We can achieve better performance and a smaller phrase table by applying syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>The results in table 5 show that while some non-syntactic phrases are very important to maintain the performance of a PBMT system, not all of them are necessary.</prevsent>
            <prevsent>We can achieve better performance and a smaller phrase table by applying syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words.</prevsent>
         </prevsection>
         <citsent citstr=" W08-0409 ">
            To some extent, our idea is similar to Ma et al (2008)
            <papid>W08-0409</papid>
            , who used an anchor word alignment model to find a set of high-precision anchor links and then aligned the remaining words relying on dependency information invoked by the acquired anchor links.
         </citsent>
         <aftsection>
            <nextsent>
               The similarity is that both Ma et al (2008)
               <papid>W08-0409</papid>
               and this work utilize structure information to find appropriate translations for words which are difficult to align.
            </nextsent>
            <nextsent>The differ 31 ence is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>We can achieve better performance and a smaller phrase table by applying syntactic constraints when there is phrase segmentation ambiguity arising from unaligned words.</prevsent>
            <prevsent>
               To some extent, our idea is similar to Ma et al (2008)
               <papid>W08-0409</papid>
               , who used an anchor word alignment model to find a set of high-precision anchor links and then aligned the remaining words relying on dependency information invoked by the acquired anchor links.
            </prevsent>
         </prevsection>
         <citsent citstr=" W08-0409 ">
            The similarity is that both Ma et al (2008)
            <papid>W08-0409</papid>
            and this work utilize structure information to find appropriate translations for words which are difficult to align.
         </citsent>
         <aftsection>
            <nextsent>The differ 31 ence is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage.</nextsent>
            <nextsent>
               There are also many works which leverage syntax information to improve word alignments (e.g., Cherry and Lin, 2006
               <papid>P06-2014</papid>
               ; DeNero and Klein, 2007
               <papid>P07-1003</papid>
               ; Fossum et al, 2008
               <papid>W08-0306</papid>
               ; Hermja kob, 2009).
            </nextsent>
            <nextsent>Johnson et al, (2007) presented a technique for pruning the phrase table in a PBMT system using Fishers exact test.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The similarity is that both Ma et al (2008)
               <papid>W08-0409</papid>
               and this work utilize structure information to find appropriate translations for words which are difficult to align.
            </prevsent>
            <prevsent>The differ 31 ence is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage.</prevsent>
         </prevsection>
         <citsent citstr=" P06-2014 ">
            There are also many works which leverage syntax information to improve word alignments (e.g., Cherry and Lin, 2006
            <papid>P06-2014</papid>
            ; DeNero and Klein, 2007
            <papid>P07-1003</papid>
            ; Fossum et al, 2008
            <papid>W08-0306</papid>
            ; Hermja kob, 2009).
         </citsent>
         <aftsection>
            <nextsent>Johnson et al, (2007) presented a technique for pruning the phrase table in a PBMT system using Fishers exact test.</nextsent>
            <nextsent>They compute the significance value of each phrase pair and prune the table by deleting phrase pairs with significance values smaller than a certain threshold.</nextsent>
            <nextsent>Yang and Zheng (2008) extended the work in Johnson et al, (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG).</nextsent>
            <nextsent>
               Tomeh et al, (2009) described an approach for filtering phrase tables in a statistical machine translation system, which relies on a statistical independence measure called Noise, first introduced in (Moore, 2004
               <papid>W04-3243</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The similarity is that both Ma et al (2008)
               <papid>W08-0409</papid>
               and this work utilize structure information to find appropriate translations for words which are difficult to align.
            </prevsent>
            <prevsent>The differ 31 ence is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage.</prevsent>
         </prevsection>
         <citsent citstr=" P07-1003 ">
            There are also many works which leverage syntax information to improve word alignments (e.g., Cherry and Lin, 2006
            <papid>P06-2014</papid>
            ; DeNero and Klein, 2007
            <papid>P07-1003</papid>
            ; Fossum et al, 2008
            <papid>W08-0306</papid>
            ; Hermja kob, 2009).
         </citsent>
         <aftsection>
            <nextsent>Johnson et al, (2007) presented a technique for pruning the phrase table in a PBMT system using Fishers exact test.</nextsent>
            <nextsent>They compute the significance value of each phrase pair and prune the table by deleting phrase pairs with significance values smaller than a certain threshold.</nextsent>
            <nextsent>Yang and Zheng (2008) extended the work in Johnson et al, (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG).</nextsent>
            <nextsent>
               Tomeh et al, (2009) described an approach for filtering phrase tables in a statistical machine translation system, which relies on a statistical independence measure called Noise, first introduced in (Moore, 2004
               <papid>W04-3243</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The similarity is that both Ma et al (2008)
               <papid>W08-0409</papid>
               and this work utilize structure information to find appropriate translations for words which are difficult to align.
            </prevsent>
            <prevsent>The differ 31 ence is that they used dependency information in the word alignment stage while our method uses syntactic information during the phrase pair extraction stage.</prevsent>
         </prevsection>
         <citsent citstr=" W08-0306 ">
            There are also many works which leverage syntax information to improve word alignments (e.g., Cherry and Lin, 2006
            <papid>P06-2014</papid>
            ; DeNero and Klein, 2007
            <papid>P07-1003</papid>
            ; Fossum et al, 2008
            <papid>W08-0306</papid>
            ; Hermja kob, 2009).
         </citsent>
         <aftsection>
            <nextsent>Johnson et al, (2007) presented a technique for pruning the phrase table in a PBMT system using Fishers exact test.</nextsent>
            <nextsent>They compute the significance value of each phrase pair and prune the table by deleting phrase pairs with significance values smaller than a certain threshold.</nextsent>
            <nextsent>Yang and Zheng (2008) extended the work in Johnson et al, (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG).</nextsent>
            <nextsent>
               Tomeh et al, (2009) described an approach for filtering phrase tables in a statistical machine translation system, which relies on a statistical independence measure called Noise, first introduced in (Moore, 2004
               <papid>W04-3243</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W10-3804.xml">syntactic constraints on phrase extraction for phrase based machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>They compute the significance value of each phrase pair and prune the table by deleting phrase pairs with significance values smaller than a certain threshold.</prevsent>
            <prevsent>Yang and Zheng (2008) extended the work in Johnson et al, (2007) to a hierarchical PBMT model, which is built on synchronous context free grammars (SCFG).</prevsent>
         </prevsection>
         <citsent citstr=" W04-3243 ">
            Tomeh et al, (2009) described an approach for filtering phrase tables in a statistical machine translation system, which relies on a statistical independence measure called Noise, first introduced in (Moore, 2004
            <papid>W04-3243</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>The difference between the above research and this work is they took advantage of some statistical measures while we use syntactic knowledge to filter phrase tables.</nextsent>
            <nextsent>Phrase pair extraction plays a very important role on the performance of PBMT systems.</nextsent>
            <nextsent>We utilize syntactic knowledge to constrain the phrase extraction from word-based alignments for a PBMT system.</nextsent>
            <nextsent>Rather than filter out all non-syntactic phrases, we only filter out non syntactic phrases whose first or last source word is unaligned.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We deploy local features for SCFG-based SMT that can be readoff from rules at runtime, and present a learning algorithm that applies `1/`2 regularization for joint feature selection over distributed stochastic learning processes.</prevsent>
            <prevsent>We present experiments on learning on 1.5 million training sentences, and show significant improvements over tuning discriminative models on small development sets.</prevsent>
         </prevsection>
         <citsent citstr=" P03-1021 ">
            The standard SMT training pipeline combines scores from large count-based translation models and language models with a few other features and tunes these using the well-understood line-search technique for error minimization of Och (2003)
            <papid>P03-1021</papid>
            .
         </citsent>
         <aftsection>
            <nextsent>If only a handful of dense features need to be tuned, minimum error rate training can be done on small tuning sets and is hard to beat in terms of accuracy and efficiency.</nextsent>
            <nextsent>In contrast, the promise of large scale discriminative training for SMT is to scale to arbitrary types and numbers of features and to provide sufficient statistical support by parameter estimation on large sample sizes.</nextsent>
            <nextsent>Features may be lexicalized and sparse, non-local and overlapping, or be designed to generalize beyond surface statistics by incorporating part-of-speech or syntactic labels.</nextsent>
            <nextsent>The modelers goals might be to identify complex properties of translations, or to counter errors of pre trained translation models and language models by explicitly down-weighting translations that exhibit certain undesired properties.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Why is this?</prevsent>
            <prevsent>One possible reason why discriminative SMT has mostly been content with small tuning sets lies in the particular design of the features themselves.</prevsent>
         </prevsection>
         <citsent citstr=" N09-1025 ">
            For example, the features introduced by Chiang et al (2008) and Chiang et al (2009)
            <papid>N09-1025</papid>
            for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-sideterminals.
         </citsent>
         <aftsection>
            <nextsent>These features are specified in handcrafted lists based on a thorough analysis of a tuningset.</nextsent>
            <nextsent>Such finely hand-crafted features will find sufficient statistical support on a few thousand examples and thus do not benefit from larger training sets.The second type of features deploys external information such as syntactic parses or word alignments to penalize bad reorderings or undesired translations of phrases that cross syntactic constraints.</nextsent>
            <nextsent>At large scale, extraction of such features quickly becomes 11 (1) X ? X1 hat X2 versprochen, X1 promised X2 (2) X ? X1 hat mir X2 versprochen, X1 promised me X2 (3) X ? X1 versprach X2, X1 promised X2 Figure 1: SCFG rules for translation.</nextsent>
            <nextsent>infeasible because of costly generation and storage of linguistic annotations.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Exceptions where discriminative SMT has been used on large training data are Liang et al (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al.</prevsent>
            <prevsent>(2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training.</prevsent>
         </prevsection>
         <citsent citstr=" W10-1757 ">
            Our approach is inspired by Duh et al (2010)
            <papid>W10-1757</papid>
            who applied multi-task learning for improved generalization in n-best reranking.
         </citsent>
         <aftsection>
            <nextsent>
               In contrast to ourwork, Duh et al (2010)
               <papid>W10-1757</papid>
               did not incorporate multi task learning into distributed learning, but defined tasks as n-best lists, nor did they develop new algorithms, but used off-the-shelf multi-task tools.
            </nextsent>
            <nextsent>
               The work described in this paper is based on theSMT framework of hierarchical phrase-based translation (Chiang, 2005
               <papid>P12-2057</papid>
               ; Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>(2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training.</prevsent>
            <prevsent>
               Our approach is inspired by Duh et al (2010)
               <papid>W10-1757</papid>
               who applied multi-task learning for improved generalization in n-best reranking.
            </prevsent>
         </prevsection>
         <citsent citstr=" W10-1757 ">
            In contrast to ourwork, Duh et al (2010)
            <papid>W10-1757</papid>
            did not incorporate multi task learning into distributed learning, but defined tasks as n-best lists, nor did they develop new algorithms, but used off-the-shelf multi-task tools.
         </citsent>
         <aftsection>
            <nextsent>
               The work described in this paper is based on theSMT framework of hierarchical phrase-based translation (Chiang, 2005
               <papid>P12-2057</papid>
               ; Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </nextsent>
            <nextsent>Translation rules are extracted from word-aligned parallel sentences and can be seen as productions of a synchronous CFG.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Local Features for Synchronous CFGs.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Our approach is inspired by Duh et al (2010)
               <papid>W10-1757</papid>
               who applied multi-task learning for improved generalization in n-best reranking.
            </prevsent>
            <prevsent>
               In contrast to ourwork, Duh et al (2010)
               <papid>W10-1757</papid>
               did not incorporate multi task learning into distributed learning, but defined tasks as n-best lists, nor did they develop new algorithms, but used off-the-shelf multi-task tools.
            </prevsent>
         </prevsection>
         <citsent citstr=" P12-2057 ">
            The work described in this paper is based on theSMT framework of hierarchical phrase-based translation (Chiang, 2005
            <papid>P12-2057</papid>
            ; Chiang, 2007
            <papid>J07-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Translation rules are extracted from word-aligned parallel sentences and can be seen as productions of a synchronous CFG.</nextsent>
            <nextsent>Examples are rules like (1)-(3) 12 shown in Figure 1.</nextsent>
            <nextsent>Local features are designed to be readable directly off the rule at decoding time.</nextsent>
            <nextsent>We use three rule templates in our work: Rule identifiers: These features identify each ruleby a unique identifier.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Local Features for Synchronous CFGs.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Our approach is inspired by Duh et al (2010)
               <papid>W10-1757</papid>
               who applied multi-task learning for improved generalization in n-best reranking.
            </prevsent>
            <prevsent>
               In contrast to ourwork, Duh et al (2010)
               <papid>W10-1757</papid>
               did not incorporate multi task learning into distributed learning, but defined tasks as n-best lists, nor did they develop new algorithms, but used off-the-shelf multi-task tools.
            </prevsent>
         </prevsection>
         <citsent citstr=" J07-2003 ">
            The work described in this paper is based on theSMT framework of hierarchical phrase-based translation (Chiang, 2005
            <papid>P12-2057</papid>
            ; Chiang, 2007
            <papid>J07-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Translation rules are extracted from word-aligned parallel sentences and can be seen as productions of a synchronous CFG.</nextsent>
            <nextsent>Examples are rules like (1)-(3) 12 shown in Figure 1.</nextsent>
            <nextsent>Local features are designed to be readable directly off the rule at decoding time.</nextsent>
            <nextsent>We use three rule templates in our work: Rule identifiers: These features identify each ruleby a unique identifier.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Joint Feature Selection in Distributed.</section>
      <citcontext>
         <prevsection>
            <prevsent>denotes the standard vector dot product.</prevsent>
            <prevsent>Instantiating SGD to the following stochastic 2Similar monolingual parse features?</prevsent>
         </prevsection>
         <citsent citstr=" W11-2139 ">
            have been used in Dyer et al (2011)
            <papid>W11-2139</papid>
            .
         </citsent>
         <aftsection>
            <nextsent>sub gradient leads to the perceptron algorithm for pairwise ranking3 (Shen and Joshi, 2005): lj(w) = { xj if w, xj? ? 0, 0 else.</nextsent>
            <nextsent>Our baseline algorithm 1 (SDG) scales pairwise ranking to large scale scenarios.</nextsent>
            <nextsent>
               The algorithm takes an average over the final weight updates of each epoch instead of keeping a record of all weight updates for final averaging (Collins, 2002
               <papid>W02-1001</papid>
               ) or for voting (Freund and Schapire, 1999).
            </nextsent>
            <nextsent>Algorithm 1 SGD: int I, T , float ? Initialize w0,0,0 ? 0.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Joint Feature Selection in Distributed.</section>
      <citcontext>
         <prevsection>
            <prevsent>sub gradient leads to the perceptron algorithm for pairwise ranking3 (Shen and Joshi, 2005): lj(w) = { xj if w, xj? ? 0, 0 else.</prevsent>
            <prevsent>Our baseline algorithm 1 (SDG) scales pairwise ranking to large scale scenarios.</prevsent>
         </prevsection>
         <citsent citstr=" W02-1001 ">
            The algorithm takes an average over the final weight updates of each epoch instead of keeping a record of all weight updates for final averaging (Collins, 2002
            <papid>W02-1001</papid>
            ) or for voting (Freund and Schapire, 1999).
         </citsent>
         <aftsection>
            <nextsent>Algorithm 1 SGD: int I, T , float ? Initialize w0,0,0 ? 0.</nextsent>
            <nextsent>for epochs t?</nextsent>
            <nextsent>0 . . .</nextsent>
            <nextsent>T ? 1: do for all i ? {0 . . .</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>The datasets used in our experiments are versions of the News Commentary (nc), News Crawl (crawl) and Europarl (ep) corpora described in Table 1.</prevsent>
            <prevsent>The translation direction is German-to-English.</prevsent>
         </prevsection>
         <citsent citstr=" J07-2003 ">
            The SMT framework used in our experiments is hierarchical phrase-based translation (Chiang, 2007
            <papid>J07-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               We use the cdec decoder5 (Dyer et al, 2010
               <papid>P10-4002</papid>
               ) and induce SCFG grammars from two sets of symmetrized alignments using the method described by Chiang (2007).
            </nextsent>
            <nextsent>
               All data was tokenized and lowercased; German compounds were split (Dyer, 2009
               <papid>N09-1046</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>Before training, we collect all the grammar rules necessary to4Note that by definition of ||W||1,2, standard `1 regularization is a special case of `1/`2 regularization for a single task.</prevsent>
            <prevsent>5cdec meta parameters were set to a non-terminal span limit of 15 and standard cube pruning with a pop limit of 200.</prevsent>
         </prevsection>
         <citsent citstr=" D07-1104 ">
            translate each individual sentence into separate files (so-called per-sentence grammars) (Lopez, 2007
            <papid>D07-1104</papid>
            ).When decoding, cdec loads the appropriate file immediately prior to translation of the sentence.
         </citsent>
         <aftsection>
            <nextsent>The computational overhead is minimal compared to the expense of decoding.</nextsent>
            <nextsent>Also, deploying disk space instead of memory fits perfectly into the MapRe duce framework we are working in.</nextsent>
            <nextsent>
               Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stol cke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011
               <papid>W11-2123</papid>
               ).
            </nextsent>
            <nextsent>For the 5-gram language models, we replaced every word in the lm training data with &lt;unk&gt; that did not appear in the English part of the parallel training data to build an open vocabulary language model.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>The intuition is to ensure that good translations are preferred over bad translations without teasing apart small differences.</prevsent>
            <prevsent>For evaluation, we used the mteval-v11b.plscript to compute lower cased BLEU-4 scores (Pa pineni et al, 2001).</prevsent>
         </prevsection>
         <citsent citstr=" W05-0908 ">
            Statistical significance was measured using an Approximate Randomization test (Noreen, 1989; Riezler and Maxwell, 2005
            <papid>W05-0908</papid>
            ).All experiments for training on dev sets were carried out on a single computer.
         </citsent>
         <aftsection>
            <nextsent>For grammar extraction and training of the full dataset we used a 30 node hadoop Map/Reduce cluster that can handle 300 jobs at once.</nextsent>
            <nextsent>We split the data into 2290 shards for the ep runs and 141 shards for the nc runs, each shard holding about 1,000 sentences, which corresponds to the dev set size of the nc dataset.</nextsent>
            <nextsent>5.2 Experimental Results.</nextsent>
            <nextsent>The baseline learner in our experiments is a pairwise ranking perceptron that is used on various feature sand training data and plugged into various meta M x?</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>@ indicates the optimal number of epochs chosen on the devtest set.</prevsent>
            <prevsent>per graph as is done in the cdec implementation of MIRA.</prevsent>
         </prevsection>
         <citsent citstr=" D11-1125 ">
            We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011
            <papid>D11-1125</papid>
            ) or hypergraph-MERT (Kumar et al, 2009
            <papid>P09-1019</papid>
            ) both of which depend on hypergraph sampling.
         </citsent>
         <aftsection>
            <nextsent>In contrast, the perceptron is deterministic when started from a zero-vector of weights and achieves favorable 28.0 BLEU on the news-commentary test set.</nextsent>
            <nextsent>Since we are interested in relative improvements over a stable baseline, we restrict our attention in all following experiments to the perceptron.7 Table 2 shows the results of the experimental comparison of the 4 algorithms of Section 4.</nextsent>
            <nextsent>
               The 7Absolute improvements would be possible, e.g., by using larger language models or by adding news data to the ep training set when evaluating on crawl test sets (see, e.g., Dyer et al (2011)
               <papid>W11-2139</papid>
               ), however, this is not the focus of this paper.
            </nextsent>
            <nextsent>default features include 12 dense models defined on SCFG rules;8 The sparse features are the 3 templates described in Section 3.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>@ indicates the optimal number of epochs chosen on the devtest set.</prevsent>
            <prevsent>per graph as is done in the cdec implementation of MIRA.</prevsent>
         </prevsection>
         <citsent citstr=" P09-1019 ">
            We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011
            <papid>D11-1125</papid>
            ) or hypergraph-MERT (Kumar et al, 2009
            <papid>P09-1019</papid>
            ) both of which depend on hypergraph sampling.
         </citsent>
         <aftsection>
            <nextsent>In contrast, the perceptron is deterministic when started from a zero-vector of weights and achieves favorable 28.0 BLEU on the news-commentary test set.</nextsent>
            <nextsent>Since we are interested in relative improvements over a stable baseline, we restrict our attention in all following experiments to the perceptron.7 Table 2 shows the results of the experimental comparison of the 4 algorithms of Section 4.</nextsent>
            <nextsent>
               The 7Absolute improvements would be possible, e.g., by using larger language models or by adding news data to the ep training set when evaluating on crawl test sets (see, e.g., Dyer et al (2011)
               <papid>W11-2139</papid>
               ), however, this is not the focus of this paper.
            </nextsent>
            <nextsent>default features include 12 dense models defined on SCFG rules;8 The sparse features are the 3 templates described in Section 3.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>In contrast, the perceptron is deterministic when started from a zero-vector of weights and achieves favorable 28.0 BLEU on the news-commentary test set.</prevsent>
            <prevsent>Since we are interested in relative improvements over a stable baseline, we restrict our attention in all following experiments to the perceptron.7 Table 2 shows the results of the experimental comparison of the 4 algorithms of Section 4.</prevsent>
         </prevsection>
         <citsent citstr=" W11-2139 ">
            The 7Absolute improvements would be possible, e.g., by using larger language models or by adding news data to the ep training set when evaluating on crawl test sets (see, e.g., Dyer et al (2011)
            <papid>W11-2139</papid>
            ), however, this is not the focus of this paper.
         </citsent>
         <aftsection>
            <nextsent>default features include 12 dense models defined on SCFG rules;8 The sparse features are the 3 templates described in Section 3.</nextsent>
            <nextsent>All feature weights were tuned together using algorithms 1-4.</nextsent>
            <nextsent>If not indicated otherwise, the perceptron was run for 10 epochs with learning rate ? = 0.0001, started at zero weight vector, using deduplicated 100-best lists.</nextsent>
            <nextsent>
               The results on the news-commentary (nc) data show that training on the development set does not benefit from adding large feature sets ? BLEU result differences between tuning 12 default features 8negative log relative frequency p(e|f); log count(f ); log count(e, f ); lexical translation probability p(f |e) and p(e|f) (Koehn et al, 2003
               <papid>N03-1017</papid>
               ); indicator variable on singleton phrase e; indicator variable on singleton phrase pair f, e; word penalty;language model weight; OOV count of language model; number of untranslated words; Hiero glue rules (Chiang, 2007
               <papid>J07-2003</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>All feature weights were tuned together using algorithms 1-4.</prevsent>
            <prevsent>If not indicated otherwise, the perceptron was run for 10 epochs with learning rate ? = 0.0001, started at zero weight vector, using deduplicated 100-best lists.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1017 ">
            The results on the news-commentary (nc) data show that training on the development set does not benefit from adding large feature sets ? BLEU result differences between tuning 12 default features 8negative log relative frequency p(e|f); log count(f ); log count(e, f ); lexical translation probability p(f |e) and p(e|f) (Koehn et al, 2003
            <papid>N03-1017</papid>
            ); indicator variable on singleton phrase e; indicator variable on singleton phrase pair f, e; word penalty;language model weight; OOV count of language model; number of untranslated words; Hiero glue rules (Chiang, 2007
            <papid>J07-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>17 Alg.</nextsent>
            <nextsent>Tuning set Features #Feats devtest-ep test-ep Tuning set test-crawl10 test-crawl11 1 dev-ep default 12 25.62 26.42?</nextsent>
            <nextsent>dev-crawl 15.39?</nextsent>
            <nextsent>14.43?</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" P12-1002.xml">joint feature selection in distributed stochastic learning for largescale discriminative training in smt</title>
      <section>Experiments.</section>
      <citcontext>
         <prevsection>
            <prevsent>All feature weights were tuned together using algorithms 1-4.</prevsent>
            <prevsent>If not indicated otherwise, the perceptron was run for 10 epochs with learning rate ? = 0.0001, started at zero weight vector, using deduplicated 100-best lists.</prevsent>
         </prevsection>
         <citsent citstr=" J07-2003 ">
            The results on the news-commentary (nc) data show that training on the development set does not benefit from adding large feature sets ? BLEU result differences between tuning 12 default features 8negative log relative frequency p(e|f); log count(f ); log count(e, f ); lexical translation probability p(f |e) and p(e|f) (Koehn et al, 2003
            <papid>N03-1017</papid>
            ); indicator variable on singleton phrase e; indicator variable on singleton phrase pair f, e; word penalty;language model weight; OOV count of language model; number of untranslated words; Hiero glue rules (Chiang, 2007
            <papid>J07-2003</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>17 Alg.</nextsent>
            <nextsent>Tuning set Features #Feats devtest-ep test-ep Tuning set test-crawl10 test-crawl11 1 dev-ep default 12 25.62 26.42?</nextsent>
            <nextsent>dev-crawl 15.39?</nextsent>
            <nextsent>14.43?</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Two common approaches are pivot?</prevsent>
            <prevsent>and distributional paraphrasing.</prevsent>
         </prevsection>
         <citsent citstr=" N06-1003 ">
            Pivot paraphrasing translates phrases of interest to other languages and back (Callison-Burch et al, 2006
            <papid>N06-1003</papid>
            ; Callison-Burch, 2008
            <papid>D08-1021</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>It relies on parallel texts (or translation phrase tables) in various languages, which are typically scarce, and hence limit its applicability.</nextsent>
            <nextsent>
               Distributional paraphrasing (Marton et al, 2009
               <papid>D09-1040</papid>
               ) generates paraphrases using a distributional semantic distance measure computed over a large monolingual corpus.1 Monolingual corpora are relatively easyand inexpensive to collect, but distributional semantic distance measures are known to rank antonymous and polarity-dissimilar phrasal candidates high.
            </nextsent>
            <nextsent>We therefore attempt to identify and filter out such ill suited paraphrase candidates.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>Two common approaches are pivot?</prevsent>
            <prevsent>and distributional paraphrasing.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1021 ">
            Pivot paraphrasing translates phrases of interest to other languages and back (Callison-Burch et al, 2006
            <papid>N06-1003</papid>
            ; Callison-Burch, 2008
            <papid>D08-1021</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>It relies on parallel texts (or translation phrase tables) in various languages, which are typically scarce, and hence limit its applicability.</nextsent>
            <nextsent>
               Distributional paraphrasing (Marton et al, 2009
               <papid>D09-1040</papid>
               ) generates paraphrases using a distributional semantic distance measure computed over a large monolingual corpus.1 Monolingual corpora are relatively easyand inexpensive to collect, but distributional semantic distance measures are known to rank antonymous and polarity-dissimilar phrasal candidates high.
            </nextsent>
            <nextsent>We therefore attempt to identify and filter out such ill suited paraphrase candidates.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Pivot paraphrasing translates phrases of interest to other languages and back (Callison-Burch et al, 2006
               <papid>N06-1003</papid>
               ; Callison-Burch, 2008
               <papid>D08-1021</papid>
               ).
            </prevsent>
            <prevsent>It relies on parallel texts (or translation phrase tables) in various languages, which are typically scarce, and hence limit its applicability.</prevsent>
         </prevsection>
         <citsent citstr=" D09-1040 ">
            Distributional paraphrasing (Marton et al, 2009
            <papid>D09-1040</papid>
            ) generates paraphrases using a distributional semantic distance measure computed over a large monolingual corpus.1 Monolingual corpora are relatively easyand inexpensive to collect, but distributional semantic distance measures are known to rank antonymous and polarity-dissimilar phrasal candidates high.
         </citsent>
         <aftsection>
            <nextsent>We therefore attempt to identify and filter out such ill suited paraphrase candidates.</nextsent>
            <nextsent>A phrase pair may have a varying degree of antonymy, beyond the better-known complete opposites (hot / cold) and contradictions (did / didnot), e.g., weaker contrasts (hot / cool), contrasting trends (covered / reduced coverage), or sentiment polarity (happy / sad).</nextsent>
            <nextsent>
               Information extraction, opinion mining and sentiment analysis literature has been grappling with identifying such pairs (Pang and Lee, 2008), e.g., in order to distinguish positive and negative reviews or comments, or to detect contradictions (Marneffe et al, 2008; Voorhees, 2008
               <papid>P08-1008</papid>
               ).
            </nextsent>
            <nextsent>We transfer some of the insights, data and techniques to the area of paraphrasing and SMT.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We therefore attempt to identify and filter out such ill suited paraphrase candidates.</prevsent>
            <prevsent>A phrase pair may have a varying degree of antonymy, beyond the better-known complete opposites (hot / cold) and contradictions (did / didnot), e.g., weaker contrasts (hot / cool), contrasting trends (covered / reduced coverage), or sentiment polarity (happy / sad).</prevsent>
         </prevsection>
         <citsent citstr=" P08-1008 ">
            Information extraction, opinion mining and sentiment analysis literature has been grappling with identifying such pairs (Pang and Lee, 2008), e.g., in order to distinguish positive and negative reviews or comments, or to detect contradictions (Marneffe et al, 2008; Voorhees, 2008
            <papid>P08-1008</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We transfer some of the insights, data and techniques to the area of paraphrasing and SMT.</nextsent>
            <nextsent>We distributionally expand a small seed set of antonyms in an unsupervised manner, following Mohammad et al (2008).</nextsent>
            <nextsent>We then present a method for filtering antonymous and polarity-dissimilar distributional paraphrases using the expanded antonymous list and a list of negators (e.g., cannot) and trend decreasing words (reduced).</nextsent>
            <nextsent>
               We evaluate the impact of our approach in a SMT setting, where non 1Other variants use a lexical resource in conjunction with the monolingual corpus (Mirkin et al, 2009
               <papid>P09-1089</papid>
               ; Marton, 2010).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>We distributionally expand a small seed set of antonyms in an unsupervised manner, following Mohammad et al (2008).</prevsent>
            <prevsent>We then present a method for filtering antonymous and polarity-dissimilar distributional paraphrases using the expanded antonymous list and a list of negators (e.g., cannot) and trend decreasing words (reduced).</prevsent>
         </prevsection>
         <citsent citstr=" P09-1089 ">
            We evaluate the impact of our approach in a SMT setting, where non 1Other variants use a lexical resource in conjunction with the monolingual corpus (Mirkin et al, 2009
            <papid>P09-1089</papid>
            ; Marton, 2010).
         </citsent>
         <aftsection>
            <nextsent>237baseline translation models are augmented with distributional paraphrases.</nextsent>
            <nextsent>We show gains of up to 1 BLEU relative to non-filtered models (1.6 BLEU.</nextsent>
            <nextsent>from non-augmented baselines) in English-Chinese models trained on small and medium-large size data, but lower to no gains in English-Arabic.</nextsent>
            <nextsent>The small training size simulates resource-poor languages.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Distributional Paraphrases.</section>
      <citcontext>
         <prevsection>
            <prevsent>Each DPphr is a vector containing log likelihood ratios of the focal phrase phr and each word w in the corpus.</prevsent>
            <prevsent>Given a paraphrase candidate phrase cand, the semantic distance between phr andcand is calculated using the cosine of their respective DPs (McDonald, 2000).</prevsent>
         </prevsection>
         <citsent citstr=" C04-1146 ">
            For details on DPs and distributional measures, see Weeds et al (2004)
            <papid>C04-1146</papid>
            and Turney and Pantel (2010).The search of the corpus for paraphrase candidates is performed in the following manner: 1.
         </citsent>
         <aftsection>
            <nextsent>For each focal phrase phr, build distributional.</nextsent>
            <nextsent>profile DPphr.</nextsent>
            <nextsent>2.</nextsent>
            <nextsent>Gather contexts: for each occurrence of phr,.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Antonyms, Trends, Sentiment Polarity.</section>
      <citcontext>
         <prevsection>
            <prevsent>Strict antonyms apart,there are also many word pairs that exhibit some degree of contrast in meaning, for example, lukewarm?</prevsent>
            <prevsent>cold, ascend slip, and fan enemy (Mohammad etal., 2008).</prevsent>
         </prevsection>
         <citsent citstr=" P08-1008 ">
            Automatically identifying such contrasting word pairs has many uses including detecting and generating paraphrases (The lion caught the gazel / The gazel could not escape the lion) 238 and detecting contradictions (Marneffe et al, 2008; Voorhees, 2008
            <papid>P08-1008</papid>
            ) (The inhabitants of Peru are well off / the inhabitants of Peru are poor).
         </citsent>
         <aftsection>
            <nextsent>Of course, such contradictions?</nextsent>
            <nextsent>may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements.</nextsent>
            <nextsent>Identifying paraphrases and contradictions are in turn useful ineffectively re-ranking target language hypotheses in machine translation, and for re-ranking query responses in information retrieval.</nextsent>
            <nextsent>
               Identifying contrasting word pairs (or short phrase pairs) is also useful for detecting humor (Mihalcea and Strapparava, 2005
               <papid>H05-1067</papid>
               ), as satire and jokes tend to have contradictions and oxymorons.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Antonyms, Trends, Sentiment Polarity.</section>
      <citcontext>
         <prevsection>
            <prevsent>may be a result of differing sentiment, new information, non-coreferent mentions, or genuinely contradictory statements.</prevsent>
            <prevsent>Identifying paraphrases and contradictions are in turn useful ineffectively re-ranking target language hypotheses in machine translation, and for re-ranking query responses in information retrieval.</prevsent>
         </prevsection>
         <citsent citstr=" H05-1067 ">
            Identifying contrasting word pairs (or short phrase pairs) is also useful for detecting humor (Mihalcea and Strapparava, 2005
            <papid>H05-1067</papid>
            ), as satire and jokes tend to have contradictions and oxymorons.
         </citsent>
         <aftsection>
            <nextsent>Lastly, it is useful to know which words contrast a focal word, even if only to filter them out.</nextsent>
            <nextsent>For example, in the automatic creation of a thesaurus it is necessary to distinguishnear-synonyms from contrasting word pairs.</nextsent>
            <nextsent>Distributional similarity measures typically fail to do so.</nextsent>
            <nextsent>Instances of strong contrast are recorded to some extent in manually created dictionaries, but hundreds of thousands of other contrasting pairs are not.</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Antonyms, Trends, Sentiment Polarity.</section>
      <citcontext>
         <prevsection>
            <prevsent>Instances of strong contrast are recorded to some extent in manually created dictionaries, but hundreds of thousands of other contrasting pairs are not.</prevsent>
            <prevsent>Further, antonyms can be of many kinds such as those described in Section 3.1 below.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1103 ">
            We use the Mohammad et al (2008)
            <papid>D08-1103</papid>
            method to automatically generate a large list of contrasting word pairs, which are used to identify false paraphrases.
         </citsent>
         <aftsection>
            <nextsent>Their method is briefly described in Section 3.2.</nextsent>
            <nextsent>3.1 Kinds of antonyms.</nextsent>
            <nextsent>Antonyms can be classified into different kinds.</nextsent>
            <nextsent>A detailed description of one such classification can be found in Cruse (1986) (Chapters 9, 10, and 11), where the author describes complement aries (open?</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Antonyms, Trends, Sentiment Polarity.</section>
      <citcontext>
         <prevsection>
            <prevsent>The antonymous phrase pair generation algorithm that we use here does not employ any antonym-subclass-specific techniques.</prevsent>
            <prevsent>3.2 Detecting antonyms.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1103 ">
            Mohammad et al (2008)
            <papid>D08-1103</papid>
            used a Roget-like thesaurus, co-occurrence statistics, and a seed set of antonyms to identify the degree of antonymy between two words, and generate a list of antonymous words.
         </citsent>
         <aftsection>
            <nextsent>The thesaurus divides the vocabulary into about a thousand coarse categories.</nextsent>
            <nextsent>Each category has, on average, about a hundred closely related words.</nextsent>
            <nextsent>(A word with more than one sense, is listed in more than one category.)</nextsent>
            <nextsent>Mohammad et al first determine pairs of thesaurus categories that are contrasting in meaning.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Paraphrase-Augmented SMT.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The General Inquirer (GI) (Stone et al, 1966) has 11,788 words labeled with 182 categories of word tags, such as positive and negative semantic orientation, pleasure, pain, and so on.2 Two of the GI categories, NOTLW and DECREAS, contain terms that negate the meaning of what follows (Choi and Cardie, 2008
               <papid>D08-1083</papid>
               ; Kennedy and Inkpen, 2005).
            </prevsent>
            <prevsent>These terms (with limited added inflection variation) form our list of negators.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1021 ">
            Augmenting the source side of SMT phrase tables with paraphrases of out-of-vocabulary (OOV) items was introduced by Callison-Burch et al (2006), and was adopted practically as-is? in consequent work (Callison-Burch, 2008
            <papid>D08-1021</papid>
            ; Marton et al, 2009
            <papid>D09-1040</papid>
            ; Marton, 2010).
         </citsent>
         <aftsection>
            <nextsent>Given an OOV source-side phrase f , if the translation model has a rule f ?, e?</nextsent>
            <nextsent>whose source side is a paraphrase f ? of f , then a new rule f, e?</nextsent>
            <nextsent>is added, with an extra weighted log-linearfeature, whose value for the new rule is the similarity score between f and f ?</nextsent>
            <nextsent>(computed as a function of the pivot translation probabilities or the distributional semantic distance of the respective DPs).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Paraphrase-Augmented SMT.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               The General Inquirer (GI) (Stone et al, 1966) has 11,788 words labeled with 182 categories of word tags, such as positive and negative semantic orientation, pleasure, pain, and so on.2 Two of the GI categories, NOTLW and DECREAS, contain terms that negate the meaning of what follows (Choi and Cardie, 2008
               <papid>D08-1083</papid>
               ; Kennedy and Inkpen, 2005).
            </prevsent>
            <prevsent>These terms (with limited added inflection variation) form our list of negators.</prevsent>
         </prevsection>
         <citsent citstr=" D09-1040 ">
            Augmenting the source side of SMT phrase tables with paraphrases of out-of-vocabulary (OOV) items was introduced by Callison-Burch et al (2006), and was adopted practically as-is? in consequent work (Callison-Burch, 2008
            <papid>D08-1021</papid>
            ; Marton et al, 2009
            <papid>D09-1040</papid>
            ; Marton, 2010).
         </citsent>
         <aftsection>
            <nextsent>Given an OOV source-side phrase f , if the translation model has a rule f ?, e?</nextsent>
            <nextsent>whose source side is a paraphrase f ? of f , then a new rule f, e?</nextsent>
            <nextsent>is added, with an extra weighted log-linearfeature, whose value for the new rule is the similarity score between f and f ?</nextsent>
            <nextsent>(computed as a function of the pivot translation probabilities or the distributional semantic distance of the respective DPs).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Experiment.</section>
      <citcontext>
         <prevsection>
            <prevsent>distortion cost, and 9.</prevsent>
            <prevsent>language model (LM) probability.</prevsent>
         </prevsection>
         <citsent citstr=" P00-1056 ">
            We used Giza++ (Och and Ney, 2000
            <papid>P00-1056</papid>
            ) for word alignment.All features were weighted in a log-linear framework (Och and Ney, 2002
            <papid>P02-1038</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Feature weights were set with minimum error rate training (Och, 2003
               <papid>P03-1021</papid>
               ) on a tuning set using BLEU (Papineni et al, 2002) as the objective function.
            </nextsent>
            <nextsent>Test results were evaluated using BLEU and TER (Snover et al, 2006): The higher the BLEU score, the better the result; the lower the TER score, the better the result.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Experiment.</section>
      <citcontext>
         <prevsection>
            <prevsent>distortion cost, and 9.</prevsent>
            <prevsent>language model (LM) probability.</prevsent>
         </prevsection>
         <citsent citstr=" P02-1038 ">
            We used Giza++ (Och and Ney, 2000
            <papid>P00-1056</papid>
            ) for word alignment.All features were weighted in a log-linear framework (Och and Ney, 2002
            <papid>P02-1038</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Feature weights were set with minimum error rate training (Och, 2003
               <papid>P03-1021</papid>
               ) on a tuning set using BLEU (Papineni et al, 2002) as the objective function.
            </nextsent>
            <nextsent>Test results were evaluated using BLEU and TER (Snover et al, 2006): The higher the BLEU score, the better the result; the lower the TER score, the better the result.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Experiment.</section>
      <citcontext>
         <prevsection>
            <prevsent>language model (LM) probability.</prevsent>
            <prevsent>
               We used Giza++ (Och and Ney, 2000
               <papid>P00-1056</papid>
               ) for word alignment.All features were weighted in a log-linear framework (Och and Ney, 2002
               <papid>P02-1038</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" P03-1021 ">
            Feature weights were set with minimum error rate training (Och, 2003
            <papid>P03-1021</papid>
            ) on a tuning set using BLEU (Papineni et al, 2002) as the objective function.
         </citsent>
         <aftsection>
            <nextsent>Test results were evaluated using BLEU and TER (Snover et al, 2006): The higher the BLEU score, the better the result; the lower the TER score, the better the result.</nextsent>
            <nextsent>This is denoted with BLEU?</nextsent>
            <nextsent>and TER?</nextsent>
            <nextsent>in Table 1.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Experiment.</section>
      <citcontext>
         <prevsection>
            <prevsent>and TER?</prevsent>
            <prevsent>in Table 1.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3250 ">
            Statistical significance of model output differences was determined using Koehn (2004)
            <papid>W04-3250</papid>
            s test on the objective function (BLEU).
         </citsent>
         <aftsection>
            <nextsent>The paraphrase-augmented models were created as described in Section 4.</nextsent>
            <nextsent>We used the same data and parameter settings as in Marton (2010).3 Weused cosine distance over DPs of log-likelihood ratios (McDonald, 2000), built with a sliding win 3Data preprocessing and paraphrasing code slightly differ from those used in Marton et al (2009) and Marton (2010), and hence scores are not exactly the same across these publications.</nextsent>
            <nextsent>240dow of size 6, a sampling threshold of 10000 occurrences, and a maximal paraphrase length of 6 tokens.</nextsent>
            <nextsent>We applied a paraphrase score thresholdt = 0.05; a dynamic context length (the shortest non-stoplisted left context L occurring less than512 times in the corpus, and similarly for R); paraphrasing of OOV unigrams; filtering paraphrase candidates occurring less than 25 times in the corpus (inspired by McDonald, 2000); and allowing up to k = 100 best paraphrases per phrase.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Experiment.</section>
      <citcontext>
         <prevsection>
            <prevsent>We chose Chinese as the translation target language in order to compare with Marton (2010), and for the same reasons it was chosen there: It is quite different from English (e.g., in word order), and four reference translation were available from NIST.</prevsent>
            <prevsent>We chose Arabicas another target language, because it is different from both English and Chinese, and richer morphologically, which introduces additional challenges.</prevsent>
         </prevsection>
         <citsent citstr=" I08-4013 ">
            English-Chinese: For training we used the LDC Sino rama and FBIS tests (LDC2005T10 and LDC2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005
            <papid>I08-4013</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>After tokenization and filtering, this bitext contained 231,586 lines (6.4M + 5.1M tokens).</nextsent>
            <nextsent>We trained a trigram language model on the Chinese side, withthe SRILM toolkit (Stolcke, 2002), using the modified Kneser-Ney smoothing option.</nextsent>
            <nextsent>We followed the split in Marton (2010), and constructed the reduced set of about 29,000 sentence pairs.</nextsent>
            <nextsent>The purpose of creating this subset model was to simulate aresource-poor language.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Experiment.</section>
      <citcontext>
         <prevsection>
            <prevsent>training data.</prevsent>
            <prevsent>The sentences were extracted from Arabic News (LDC2004T17), eTIRR (LDC2004E72), English translation of Arabic Treebank (LDC2005E46),and Ummah (LDC2004T18).4 For Arabic preprocessing, we follow previously reported best tokenization scheme (TB)5 and orthographic word normalization condition (Reduced) when translating from English to Arabic (El Kholy and Habash, 2010b).</prevsent>
         </prevsection>
         <citsent citstr=" P05-1071 ">
            MADA (Habash and Rambow, 2005
            <papid>P05-1071</papid>
            ) is used to pre-process the Arabic text for the translation model and 5-gram language model (LM).
         </citsent>
         <aftsection>
            <nextsent>As a postprocessing step, we jointly denormalize and deto kenize the text to produce the final Arabic output.</nextsent>
            <nextsent>Following El Kholy and Habash (2010a), we use their best detokenization technique, T+R+LM.</nextsent>
            <nextsent>The technique crucially utilizes a lookup table (T), mapping tokenized forms to detokenized forms, basedon our MADA-fied LM.</nextsent>
            <nextsent>Alternatives are given conditional probabilities, P (detokenized|tokenized).Tokenized words absent from the tables are deto kenized using deterministic rules (R), as a backoff strategy.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Experiment.</section>
      <citcontext>
         <prevsection>
            <prevsent>baseline 15.8 69.2 21.8 63.8 aug-1gram 16.4B 68.9 22.5B 64.4 aug-1gram-ant-filt 17.4BD 68.7 22.8BD 63.7 Table 1: English-Chinese scores.</prevsent>
            <prevsent>B/D = statistically significant w.r.t.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3250 ">
            (B)aseline or (D)istributional 1gram model, using Koehn (2004)
            <papid>W04-3250</papid>
            s statistical significance.
         </citsent>
         <aftsection>
            <nextsent>English-Arabic: Results are given in columns 1-7 of Table 2.</nextsent>
            <nextsent>On the MT05 test set, the 135k-sentence aug-1gram model outperformed its baseline in both BLEU and TER scores.</nextsent>
            <nextsent>The lemmatized variants of the scores showed higher or same gains.</nextsent>
            <nextsent>Since only one entry was antonym-filtered here, we do not provide separate scores for aug-1gram-ant-filt.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Discussion.</section>
      <citcontext>
         <prevsection>
            <prevsent>ratio ? TER Exact Match Lemma-only Unmatchable Total 30k-sentence (1M word) training dataset models MT05 baseline 23.6 31.3 99.2 1.008 57.6 47.3 15614 55.4% 4055 14.4% 8550 30.3% 28219 aug-1gram 23.2 30.8 99.9 1.001 58.8 48.4 15387 54.2% 4195 14.8% 8831 31.1% 28413 aug-1gram-ant-filt 23.2 30.8 99.9 1.001 58.8 48.3 15387 54.2% 4195 14.8% 8831 31.1% 28413 MEDAR baseline 13.6 18.7 93.6 1.066 67.6 61.3 4924 53.0% 1563 16.8% 2800 30.1% 9287 aug-1gram 12.9 18.3 94.2 1.060 68.9 62.3 4894 52.0% 1710 18.2% 2815 29.9% 9419 aug-1gram-ant-filt 12.9 18.3 94.2 1.060 69.0 62.3 4891 51.9% 1715 18.2% 2815 29.9% 9421 135k-sentence (4M word) training dataset models MT05 baseline 25.8 33.5 99.2 1.008 55.7 45.3 16115 57.1% 3999 14.2% 8128 28.8% 28242 aug-1gram 26.4 34.3B 99.5 1.005 55.1 44.7 16156 57.1% 4068 14.4% 8089 28.6% 28313 aug-1gram-ant-filt 26.4 34.3B 99.5 1.005 55.0 44.6 16153 57.1% 4090 14.5% 8068 28.5% 28311 MEDAR baseline 17.1 23.1 94.7 1.054 65.1 58.6 5483 57.7% 1577 16.6% 2438 25.7% 9498 aug-1gram 17.2 23.5 95.3 1.048 65.1 58.6 5586 58.1% 1606 16.7% 2424 25.2% 9616 aug-1gram-ant-filt 17.2 23.5 95.3 1.048 65.1 58.6 5586 58.1% 1606 16.7% 2424 25.2% 9616 Table 2: English-Arabic translation scores and analysis for NIST MT05 and MEDAR test sets.</prevsent>
            <prevsent>B = statistically significant w.r.t.</prevsent>
         </prevsection>
         <citsent citstr=" W04-3250 ">
            (B)aseline using Koehn (2004)
            <papid>W04-3250</papid>
            s statistical significance test.
         </citsent>
         <aftsection>
            <nextsent>bers from English are not directly comparable to the Chinese side: they relate to paraphrase candidates and not phrase table entries; they relate to types and not tokens; each OOV English word may translate to one or more Chinese words, each of which may comprise of one or more characters; and last but not least, the BLEU score we use is character-based.</nextsent>
            <nextsent>phrase ||| paraphrase ||| score comments absence ||| occupation ||| 0.06 mild absence ||| presence ||| 0.33 good backwards ||| forwards ||| 0.21 good wooden ||| plastic lawn ||| 0.12 sibling dump ||| dispose of ||| 0.41 bad cooler ||| warm ||| 0.45 mild diminished ||| increased ||| 0.23 good minor ||| serious ||| 0.42 good relic ||| youth activist in the ||| 0.12 harmless dive ||| rise ||| 0.15 good argue ||| also recognize ||| 0.05 mild bother ||| waste time ||| 0.79 bad dive ||| climb ||| 0.17 good moonlight ||| spring ||| 0.05 harmless sharply ||| slightly ||| 0.60 good substantial ||| meager ||| 0.14 good warmer ||| cooler ||| 0.72 good tough ||| delicate ||| 0.07 good tiny ||| mostly muslim ||| 0.06 mild softly ||| deep ||| 0.06 mild Table 3: Random filtering examples While individual unigram to 4gram scores for the augmented models were lower than the baselines, filtered models unigram and bigram scores were lower or similar to the baselines, and their trigram and 4gram scores were higher than the baselines. We intend to further investigate the cause for this pattern, and its effect on translation quality, with thehelp of a native Chinese speaker ? and on BLEU, together with the brevity penalty ? in the future.</nextsent>
            <nextsent>English-Arabic: The most striking fact is the set of differences between the language pairs: In English Chinese, we see gains with distributional paraphrase augmentation, and further gains when antonymous and contrasting paraphrase candidates are filtered out.</nextsent>
            <nextsent>But in the 30k-sentence English-Arabic models,paraphrase augmentation actually degrades performance, even in lemma scores.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>This paper brings together several sub-areas:SMT, paraphrase generation, distributional semantic distance measures, and antonym-related work.Therefore we can only briefly survey the most relevant work here.</prevsent>
            <prevsent>Our work can be viewed as an extension of the line of research that seeks to augment translation tables with automatically generated paraphrases of OOV words or phrases in a fashion similar to Section 4: Callison-Burch et al (2006) use pivoting technique (translating to other languages and back) in order to generate paraphrases, and the pivot translation probability as their similarity score; Callison-Burch (2008) filters such paraphrases using syntactic parsing information; Marton et al (2009)use distributional paraphrasing technique that applies distributional semantic distance measure forthe paraphrase score; Marton (2010) applies a lexical resource / corpus-based hybrid semantic distance measure for the paraphrase score instead, approximating word senses; here, we apply a distributional semantic distance measure that is similar to Marton et al (2009), with the main difference being the filtering of the resulting paraphrases for antonymity.</prevsent>
         </prevsection>
         <citsent citstr=" W09-0431 ">
            Other work on augmentating SMT: Habash and Hu (2009)
            <papid>W09-0431</papid>
            show, pivoting via a trilingual parallel text, that using English as a pivot language between Chinese and Arabic outperforms translation using a direct Chinese-Arabic bilingual parallel text.Other attempts to reduce the OOV rate by augmenting the phrase tables source side include Habash (2009), providing an online tool for paraphrasingOOV phrases by lexical and morphological expansion of known phrases and dictionary terms ? and transliteration of proper names.
         </citsent>
         <aftsection>
            <nextsent>Bond et al (2008) also pivot for paraphrasing.</nextsent>
            <nextsent>They improve SMT coverage by using a manually crafted monolingual HPSG grammar for generating meaning and grammar-preserving paraphrases.</nextsent>
            <nextsent>This grammar allows for certain word reordering, lexical substitutions, contractions, and typo?</nextsent>
            <nextsent>corrections.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>This grammar allows for certain word reordering, lexical substitutions, contractions, and typo?</prevsent>
            <prevsent>corrections.</prevsent>
         </prevsection>
         <citsent citstr=" P10-2001 ">
            Onishi et al (2010)
            <papid>P10-2001</papid>
            , Du et al (2010)
            <papid>D10-1041</papid>
            , and others,pivot-paraphrase the input, and represent the paraphrases in a lattice format, decoding it with Moses.
         </citsent>
         <aftsection>
            <nextsent>Work on paraphrase generation: Barzilay and mckeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source.</nextsent>
            <nextsent>However, monolingual parallel corpora are extremely rare and small.</nextsent>
            <nextsent>
               Dolan et al (2004)
               <papid>C04-1051</papid>
               use edit distance for paraphrasing.Max (2009)
               <papid>W09-2503</papid>
               and others take the context of the paraphrased words occurrence into account.
            </nextsent>
            <nextsent>
               Zhao et al (2008)
               <papid>P08-1116</papid>
               apply SMT-style decoding for paraphrasing, using several loglinear weighted resources while Zhao et al (2009)
               <papid>P09-1094</papid>
               filter out paraphrase candidatesand weight paraphrase features according to the desired NLP task.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>This grammar allows for certain word reordering, lexical substitutions, contractions, and typo?</prevsent>
            <prevsent>corrections.</prevsent>
         </prevsection>
         <citsent citstr=" D10-1041 ">
            Onishi et al (2010)
            <papid>P10-2001</papid>
            , Du et al (2010)
            <papid>D10-1041</papid>
            , and others,pivot-paraphrase the input, and represent the paraphrases in a lattice format, decoding it with Moses.
         </citsent>
         <aftsection>
            <nextsent>Work on paraphrase generation: Barzilay and mckeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source.</nextsent>
            <nextsent>However, monolingual parallel corpora are extremely rare and small.</nextsent>
            <nextsent>
               Dolan et al (2004)
               <papid>C04-1051</papid>
               use edit distance for paraphrasing.Max (2009)
               <papid>W09-2503</papid>
               and others take the context of the paraphrased words occurrence into account.
            </nextsent>
            <nextsent>
               Zhao et al (2008)
               <papid>P08-1116</papid>
               apply SMT-style decoding for paraphrasing, using several loglinear weighted resources while Zhao et al (2009)
               <papid>P09-1094</papid>
               filter out paraphrase candidatesand weight paraphrase features according to the desired NLP task.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Work on paraphrase generation: Barzilay and mckeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source.</prevsent>
            <prevsent>However, monolingual parallel corpora are extremely rare and small.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1051 ">
            Dolan et al (2004)
            <papid>C04-1051</papid>
            use edit distance for paraphrasing.Max (2009)
            <papid>W09-2503</papid>
            and others take the context of the paraphrased words occurrence into account.
         </citsent>
         <aftsection>
            <nextsent>
               Zhao et al (2008)
               <papid>P08-1116</papid>
               apply SMT-style decoding for paraphrasing, using several loglinear weighted resources while Zhao et al (2009)
               <papid>P09-1094</papid>
               filter out paraphrase candidatesand weight paraphrase features according to the desired NLP task.
            </nextsent>
            <nextsent>
               Chevelu et al (2009)
               <papid>P09-2063</papid>
               introducea new paraphrase generation tool based on Monte Carlo sampling.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Work on paraphrase generation: Barzilay and mckeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source.</prevsent>
            <prevsent>However, monolingual parallel corpora are extremely rare and small.</prevsent>
         </prevsection>
         <citsent citstr=" W09-2503 ">
            Dolan et al (2004)
            <papid>C04-1051</papid>
            use edit distance for paraphrasing.Max (2009)
            <papid>W09-2503</papid>
            and others take the context of the paraphrased words occurrence into account.
         </citsent>
         <aftsection>
            <nextsent>
               Zhao et al (2008)
               <papid>P08-1116</papid>
               apply SMT-style decoding for paraphrasing, using several loglinear weighted resources while Zhao et al (2009)
               <papid>P09-1094</papid>
               filter out paraphrase candidatesand weight paraphrase features according to the desired NLP task.
            </nextsent>
            <nextsent>
               Chevelu et al (2009)
               <papid>P09-2063</papid>
               introducea new paraphrase generation tool based on Monte Carlo sampling.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>However, monolingual parallel corpora are extremely rare and small.</prevsent>
            <prevsent>
               Dolan et al (2004)
               <papid>C04-1051</papid>
               use edit distance for paraphrasing.Max (2009)
               <papid>W09-2503</papid>
               and others take the context of the paraphrased words occurrence into account.
            </prevsent>
         </prevsection>
         <citsent citstr=" P08-1116 ">
            Zhao et al (2008)
            <papid>P08-1116</papid>
            apply SMT-style decoding for paraphrasing, using several loglinear weighted resources while Zhao et al (2009)
            <papid>P09-1094</papid>
            filter out paraphrase candidatesand weight paraphrase features according to the desired NLP task.
         </citsent>
         <aftsection>
            <nextsent>
               Chevelu et al (2009)
               <papid>P09-2063</papid>
               introducea new paraphrase generation tool based on Monte Carlo sampling.
            </nextsent>
            <nextsent>Mirkin et al (2009), inter alia, frame paraphrasing as a special, symmetrical case of (WordNet-based) textual entailment.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>However, monolingual parallel corpora are extremely rare and small.</prevsent>
            <prevsent>
               Dolan et al (2004)
               <papid>C04-1051</papid>
               use edit distance for paraphrasing.Max (2009)
               <papid>W09-2503</papid>
               and others take the context of the paraphrased words occurrence into account.
            </prevsent>
         </prevsection>
         <citsent citstr=" P09-1094 ">
            Zhao et al (2008)
            <papid>P08-1116</papid>
            apply SMT-style decoding for paraphrasing, using several loglinear weighted resources while Zhao et al (2009)
            <papid>P09-1094</papid>
            filter out paraphrase candidatesand weight paraphrase features according to the desired NLP task.
         </citsent>
         <aftsection>
            <nextsent>
               Chevelu et al (2009)
               <papid>P09-2063</papid>
               introducea new paraphrase generation tool based on Monte Carlo sampling.
            </nextsent>
            <nextsent>Mirkin et al (2009), inter alia, frame paraphrasing as a special, symmetrical case of (WordNet-based) textual entailment.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Dolan et al (2004)
               <papid>C04-1051</papid>
               use edit distance for paraphrasing.Max (2009)
               <papid>W09-2503</papid>
               and others take the context of the paraphrased words occurrence into account.
            </prevsent>
            <prevsent>
               Zhao et al (2008)
               <papid>P08-1116</papid>
               apply SMT-style decoding for paraphrasing, using several loglinear weighted resources while Zhao et al (2009)
               <papid>P09-1094</papid>
               filter out paraphrase candidatesand weight paraphrase features according to the desired NLP task.
            </prevsent>
         </prevsection>
         <citsent citstr=" P09-2063 ">
            Chevelu et al (2009)
            <papid>P09-2063</papid>
            introducea new paraphrase generation tool based on Monte Carlo sampling.
         </citsent>
         <aftsection>
            <nextsent>Mirkin et al (2009), inter alia, frame paraphrasing as a special, symmetrical case of (WordNet-based) textual entailment.</nextsent>
            <nextsent>
               See Madnani and Dorr (2010)
               <papid>J10-3003</papid>
               for a good paraphrasing survey.Work on measuring distributional semantic dis tance: For one survey of this rich topic, see Weeds et al (2004)
               <papid>C04-1146</papid>
               and Turney and Pantel (2010).
            </nextsent>
            <nextsent>We use here cosine of log-likelihood ratios (McDonald,2000).</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Chevelu et al (2009)
               <papid>P09-2063</papid>
               introducea new paraphrase generation tool based on Monte Carlo sampling.
            </prevsent>
            <prevsent>Mirkin et al (2009), inter alia, frame paraphrasing as a special, symmetrical case of (WordNet-based) textual entailment.</prevsent>
         </prevsection>
         <citsent citstr=" J10-3003 ">
            See Madnani and Dorr (2010)
            <papid>J10-3003</papid>
            for a good paraphrasing survey.Work on measuring distributional semantic dis tance: For one survey of this rich topic, see Weeds et al (2004)
            <papid>C04-1146</papid>
            and Turney and Pantel (2010).
         </citsent>
         <aftsection>
            <nextsent>We use here cosine of log-likelihood ratios (McDonald,2000).</nextsent>
            <nextsent>A recent paper (Kazama et al, 2010) advocates a Bayesian approach, making rare terms have lower strength of association, as a by-product of relying on their probabilistic Expectation.</nextsent>
            <nextsent>Work on detecting antonyms: Our work with antonyms can be thought of as an application-based extension of the (Mohammad et al, 2008) method.</nextsent>
            <nextsent>Some of the earliest computational work in this area is by Lin et al (2003) who used patterns 245 model e2z:29k e2z:232k e2a:30k e2a:135k phrase table baseline vocab.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Chevelu et al (2009)
               <papid>P09-2063</papid>
               introducea new paraphrase generation tool based on Monte Carlo sampling.
            </prevsent>
            <prevsent>Mirkin et al (2009), inter alia, frame paraphrasing as a special, symmetrical case of (WordNet-based) textual entailment.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1146 ">
            See Madnani and Dorr (2010)
            <papid>J10-3003</papid>
            for a good paraphrasing survey.Work on measuring distributional semantic dis tance: For one survey of this rich topic, see Weeds et al (2004)
            <papid>C04-1146</papid>
            and Turney and Pantel (2010).
         </citsent>
         <aftsection>
            <nextsent>We use here cosine of log-likelihood ratios (McDonald,2000).</nextsent>
            <nextsent>A recent paper (Kazama et al, 2010) advocates a Bayesian approach, making rare terms have lower strength of association, as a by-product of relying on their probabilistic Expectation.</nextsent>
            <nextsent>Work on detecting antonyms: Our work with antonyms can be thought of as an application-based extension of the (Mohammad et al, 2008) method.</nextsent>
            <nextsent>Some of the earliest computational work in this area is by Lin et al (2003) who used patterns 245 model e2z:29k e2z:232k e2a:30k e2a:135k phrase table baseline vocab.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2128.xml">filtering antonymous trend contrasting and polarity dissimilar distributional paraphrases for improving statistical machine translation</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>such as from X to Y ? and either X or Y ? to distinguish between antonymous and similar word pairs.</prevsent>
            <prevsent>Harabagiu et al (2006) detected antonyms by determining if their WordNet synsets are connected by the hypernymyhyponymy links and exactly one antonymy link.</prevsent>
         </prevsection>
         <citsent citstr=" C08-1114 ">
            Turney (2008)
            <papid>C08-1114</papid>
            proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hyper nyms, and other lexical-semantic relations between word pairs.
         </citsent>
         <aftsection>
            <nextsent>We presented here a novel method for filtering out antonymous phrasal paraphrase candidates, adapted from sentiment analysis literature, and tested in simulated low- and mid-resourced SMT tasks from English to two quite different languages.</nextsent>
            <nextsent>We used an antonymous word pair list extracted distributionally by extending a seed list.</nextsent>
            <nextsent>Then, the extended list, together with a negator list and a novel heuristic, were used to filter out antonymous paraphrase candidates.Finally, SMT models were augmented with the filtered paraphrases, yielding English-Chinese translation improvements of up to 1 BLEU from the corresponding non-filtered paraphrase-augmented model (up to 1.6 BLEU from the corresponding baselinemodel).</nextsent>
            <nextsent>Our method proved effective for models trained on both reduced and mid-large English chinese parallel texts.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>may refer to the planet Earth, dirt, or solid ground, depending on the context.</prevsent>
            <prevsent>The goal of Word Sense Induction (WSI) isto automatically discover the different senses by examining how a word is used.</prevsent>
         </prevsection>
         <citsent citstr=" E03-1020 ">
            This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003
            <papid>E03-1020</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>
               Furthermore,these discovered senses can be used to automatically expand lexical resources such as WordNet or FrameNet (Klapaftis and Manandhar, 2010
               <papid>N10-1010</papid>
               ).
            </nextsent>
            <nextsent>Discovering the multiple senses is frequently confounded by the relationships between a words senses.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The goal of Word Sense Induction (WSI) isto automatically discover the different senses by examining how a word is used.</prevsent>
            <prevsent>
               This unsupervised discovery process produces a sense inventory where the number of senses is corpus-driven and where senses may reflect additional usages not present in a predefined sense inventory, such as those for medicine or law (Dorow and Widdows, 2003
               <papid>E03-1020</papid>
               ).
            </prevsent>
         </prevsection>
         <citsent citstr=" N10-1010 ">
            Furthermore,these discovered senses can be used to automatically expand lexical resources such as WordNet or FrameNet (Klapaftis and Manandhar, 2010
            <papid>N10-1010</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Discovering the multiple senses is frequently confounded by the relationships between a words senses.</nextsent>
            <nextsent>While homonyms such as bass?</nextsent>
            <nextsent>or bank?</nextsent>
            <nextsent>have unrelated senses, many polysemous wordshave interrelated senses, with lexicographers of ten in disagreement for the number of fine-grained senses (Palmer et al, 2007).</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>according to WordNet, shown in Table 1, are similar in several aspect sand could be ascribed interchangeably in some contexts.</prevsent>
            <prevsent>The difficulty of automatically distinguishing two senses is proportional to their similarity because of the increasing likelihood of the two senses sharing similar contexts.</prevsent>
         </prevsection>
         <citsent citstr=" W02-0805 ">
            While the issue distinguishing between related senses is a recognized issue for Word Sense Disambiguation (Chugur et al, 2002
            <papid>W02-0805</papid>
            ; McCarthy, 2006),which uses supervised training to learn sense distinctions, measuring the impact of sense relatedness on the harder problem of WSI remains unad dressed.
         </citsent>
         <aftsection>
            <nextsent>
               The recent SemEval WSI tasks (Agirre and Soroa, 2007
               <papid>W07-2002</papid>
               ; Manandhar and Klapaftis, 2009) have provided a standard framework for evaluating WSI systems, with a controlled training corpus designed to limit sense ambiguity in the example contexts.
            </nextsent>
            <nextsent>However, given the potential relatedness of a words senses, we view it necessary to consider how WSI methods perform relative to the degree of contextualambiguity.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>The difficulty of automatically distinguishing two senses is proportional to their similarity because of the increasing likelihood of the two senses sharing similar contexts.</prevsent>
            <prevsent>
               While the issue distinguishing between related senses is a recognized issue for Word Sense Disambiguation (Chugur et al, 2002
               <papid>W02-0805</papid>
               ; McCarthy, 2006),which uses supervised training to learn sense distinctions, measuring the impact of sense relatedness on the harder problem of WSI remains unad dressed.
            </prevsent>
         </prevsection>
         <citsent citstr=" W07-2002 ">
            The recent SemEval WSI tasks (Agirre and Soroa, 2007
            <papid>W07-2002</papid>
            ; Manandhar and Klapaftis, 2009) have provided a standard framework for evaluating WSI systems, with a controlled training corpus designed to limit sense ambiguity in the example contexts.
         </citsent>
         <aftsection>
            <nextsent>However, given the potential relatedness of a words senses, we view it necessary to consider how WSI methods perform relative to the degree of contextualambiguity.</nextsent>
            <nextsent>Our goal is therefore to quantify the similarity at which a WSI approach is unable to distinguish between two senses, which reflects the sense granularity at which the approach operates.We propose two new evaluations.</nextsent>
            <nextsent>The first, described in Section 4, uses a similarity-based pseudo word discrimination task to measure the discrimination capability for related senses along a graded scale of similarity.</nextsent>
            <nextsent>As a second evaluation, in 113 1 the collection of rules imposed by authority 2 legal document setting forth rules governing a particular kind of activity 3 a rule or body of rules of conduct inherent in human nature and essential to or binding upon human society 4 a generalization that describes recurring facts or events in nature Table 1: Definitions for the top four senses of law?</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Clustering Contexts to Discover Senses.</section>
      <citcontext>
         <prevsection>
            <prevsent>For both evaluations, we examine twenty different WSI clustering based models through combining five feature types and four clustering algorithms.</prevsent>
            <prevsent>These models were selected to be representative of a wide class of existing algorithms as a way of influence future algorithmic directions based on the current models performance.</prevsent>
         </prevsection>
         <citsent citstr=" N06-2036 ">
            Frequently, WSI is treated as an unsupervised clustering problem: The contexts in which a word appears are clustered in order to discover its senses(Navigli, 2009
            <papid>N06-2036</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We selected four diverse clustering algorithms for evaluation based on three criteria: (1) the ability to automatically determine the final number of clusters given an upper bound or a set of parameters, (2) an efficient run time, and (3)high quality results in either WSI or other fields related to text analysis.</nextsent>
            <nextsent>The first criteria is essential for WSI; the final number of senses must be derived without supervision in order to reflect the true number of senses present in the corpus.</nextsent>
            <nextsent>K-Means K-Means builds clusters based on the similarity between two data points.</nextsent>
            <nextsent>Clusters grow by assigning data points to the cluster with the most similar centroid.</nextsent>
         </aftsection>
      </citcontext>
      <tag>BG</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Clustering Contexts to Discover Senses.</section>
      <citcontext>
         <prevsection>
            <prevsent>We choose initial seeds at random and use the H2 criterion function (Zhao and Karypis, 2001).</prevsent>
            <prevsent>Although K-Means is efficient and widely used, it requires the number of clusters to be specified a priori.</prevsent>
         </prevsection>
         <citsent citstr=" N06-4007 ">
            Therefore, we follow the WSI model of Pedersen and Kulkarni (2006)
            <papid>N06-4007</papid>
            and use the Gap statistic (Tibshirani et al, 2000) to automatically determine the number of clusters.
         </citsent>
         <aftsection>
            <nextsent>The Gap Statistic runs K-Means repeatedly with different values of K , ranging from 1 to some sensible maximum.</nextsent>
            <nextsent>The Gap Statistic first induces adata model from the feature distributions of the initial dataset and then for each K , creates a set of artificial datasets by sampling from the derived model.K is increased until the gap?, i.e. the distance between the objective function of the original dataset and the average objective function of the artificial datasets, is larger then the gap for the previous K value.</nextsent>
            <nextsent>We calculate the gap using 10 artificial datasets sampled from the model.Spectral Clustering Spectral Clustering interprets a datasets elements as vertices in graph with edges based on their similarity (Ng et al, 2001).Clusters are found by identifying the graph partition that produces the minimum conductance between every partition.</nextsent>
            <nextsent>This can be thought of as trying to find small islands that are connected by as few bridges as possible.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Clustering Contexts to Discover Senses.</section>
      <citcontext>
         <prevsection>
            <prevsent>This can be thought of as trying to find small islands that are connected by as few bridges as possible.</prevsent>
            <prevsent>We refer the reader to (von Luxburg, 2007) for further technical details.</prevsent>
         </prevsection>
         <citsent citstr=" W10-4173 ">
            To our knowledge, only He et al (2010)
            <papid>W10-4173</papid>
            have applied spectral clustering to WSI, which was performed on a Chinese dataset.
         </citsent>
         <aftsection>
            <nextsent>However, the algorithm used by He et al requires the number of clusters to be specified.We instead use a hybrid spectral clustering algorithm, first applied to information retrieval (Cheng et al, 2006), that automatically selects the number of clusters.</nextsent>
            <nextsent>This algorithm recursively partitions a dataset in half by finding the cut that produces the minimum conductance, which builds a tree of partitions.</nextsent>
            <nextsent>This split is done until either every data point is in its own partition or a maximum number of partitions is found.</nextsent>
            <nextsent>Partitions are then dynamically merged, starting at leaf partitions, based on a clustering criteria.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Modeling Context.</section>
      <citcontext>
         <prevsection>
            <prevsent>For each clustering algorithm, we consider five context models that represent the types of lexical features used by the majority of WSI approaches.Co-Occurrence Contexts formed from word co occurrence are the most common in WSI algorithms.</prevsent>
            <prevsent>For each occurrence of a word, those words within a certain range are counted as features.</prevsent>
         </prevsection>
         <citsent citstr=" E06-1018 ">
            Prior work has used a variety of context sizes, e.g. words inthe same sentence (Bordag, 2006
            <papid>E06-1018</papid>
            ), in nearby lexical positions (Gauch and Futrelle, 1993), or within a paragraph-sized context window (Pedersen, 2010
            <papid>S10-1081</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We consider two co-occurrence context models: a 5-word and a 25-word window.</nextsent>
            <nextsent>We note that in co-occurrence-based word space algorithms, smaller context sizes have shown to better capture paradag matic similarity, while larger sizes capture semantic associativity (Peirsman et al, 2008; Utsumi, 2010).Dependency-Relations Dependency parsing creates a syntax tree where words are directly linked according to their relation.</nextsent>
            <nextsent>These links refine co occurrence based contexts by utilizing syntactic indications of how words are related.</nextsent>
            <nextsent>Dependency parsed features have proven highly effective for word representations in many NLP applications, e.g., (Pado?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Modeling Context.</section>
      <citcontext>
         <prevsection>
            <prevsent>For each clustering algorithm, we consider five context models that represent the types of lexical features used by the majority of WSI approaches.Co-Occurrence Contexts formed from word co occurrence are the most common in WSI algorithms.</prevsent>
            <prevsent>For each occurrence of a word, those words within a certain range are counted as features.</prevsent>
         </prevsection>
         <citsent citstr=" S10-1081 ">
            Prior work has used a variety of context sizes, e.g. words inthe same sentence (Bordag, 2006
            <papid>E06-1018</papid>
            ), in nearby lexical positions (Gauch and Futrelle, 1993), or within a paragraph-sized context window (Pedersen, 2010
            <papid>S10-1081</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>We consider two co-occurrence context models: a 5-word and a 25-word window.</nextsent>
            <nextsent>We note that in co-occurrence-based word space algorithms, smaller context sizes have shown to better capture paradag matic similarity, while larger sizes capture semantic associativity (Peirsman et al, 2008; Utsumi, 2010).Dependency-Relations Dependency parsing creates a syntax tree where words are directly linked according to their relation.</nextsent>
            <nextsent>These links refine co occurrence based contexts by utilizing syntactic indications of how words are related.</nextsent>
            <nextsent>Dependency parsed features have proven highly effective for word representations in many NLP applications, e.g., (Pado?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Modeling Context.</section>
      <citcontext>
         <prevsection>
            <prevsent>and Lapata, 2007; Baroni et al, 2010).</prevsent>
            <prevsent>We follow Pantel and Lin (2002) and Dorow and Widdows (2003) using the sentence as contexts and all words with a dependency path of length 3 or less, with the last word and its relation as a feature.</prevsent>
         </prevsection>
         <citsent citstr=" S10-1078 ">
            We note that recently Kern et al (2010)
            <papid>S10-1078</papid>
            achieved good WSI performance with only a small, manually-tuned subset of all relations as context.
         </citsent>
         <aftsection>
            <nextsent>Word Ordering Word ordering can provide a mild form of syntactic information (Jones et al,2006; Sahlgren et al, 2008).</nextsent>
            <nextsent>While other syntac 115tic features may provide significantly more information, word ordering is efficient to compute and provides an alternative source of syntactic information for knowledge-lean systems or for languages where NLP tools are not readily available.Because we treat word ordering as a syntactic feature, we limit the context to words occurring in the same sentence.</nextsent>
            <nextsent>A feature is the combination of a co-occurring word and its relative position, i.e. the same word in different positions is treated as two separate features.Parts of Speech Part of speech tagging can provide a preliminary coarse-grained sense disambiguation of a words contextual features, where a word may have as many senses as it does parts of speech.</nextsent>
            <nextsent>For example, consider an occurrence of house?</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Modeling Context.</section>
      <citcontext>
         <prevsection>
            <prevsent>Labeling address?</prevsent>
            <prevsent>with its part of speech provides for more semantic information on its meaning, which further constrains the sense of house.?</prevsent>
         </prevsection>
         <citsent citstr=" W97-0322 ">
            Prior work (Pedersen and Bruce, 1997
            <papid>W97-0322</papid>
            )has suggested that this information can improve performance, but to our knowledge, the impact of POS features has not been evaluated in isolation.Each context is formed from the containing sen tence; a feature is a combination of each word and its part of speech, e.g., board-NOUN?
         </citsent>
         <aftsection>
            <nextsent>is distinct from board-VERB.?</nextsent>
            <nextsent>The proposed methodology measures the ability of a WSI approach to distinguish between related senses.However, generating a large corpus with manually labeled sense assignments and sense similarity judgements is prohibitively expensive.</nextsent>
            <nextsent>Therefore, we employ a pseudo-word discrimination task where a base word and a second word, its confounder,are replaced throughout the corpus with a pseudo word.</nextsent>
            <nextsent>The objective is then to determine which of the words was originally present given the context of an occurrence of the pseudo-word.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>WSI Performance on Related Senses.</section>
      <citcontext>
         <prevsection>
            <prevsent>Therefore, we employ a pseudo-word discrimination task where a base word and a second word, its confounder,are replaced throughout the corpus with a pseudo word.</prevsent>
            <prevsent>The objective is then to determine which of the words was originally present given the context of an occurrence of the pseudo-word.</prevsent>
         </prevsection>
         <citsent citstr=" W07-2037 ">
            Due to not requiring manual annotation, this type of task was initially proposed as a substitute for word sense disambiguation (Schutze, 1992; Gale et al, 1992
            <papid>W07-2037</papid>
            ) and for selectional preferences (Clark and Weir, 2002
            <papid>N01-1013</papid>
            ).Following the suggestions of Chambers and Ju festival laws offices 0.13660 interests 0.18289 play 0.13751 politics 0.20440 convention 0.20296 governments 0.29125 tournament 0.29007 regulations 0.40761 concerts 0.48348 legislation 0.56112 Table 2: Example confounders for festival?
         </citsent>
         <aftsection>
            <nextsent>and laws?</nextsent>
            <nextsent>and their similaritiesrafsky (2010) on designing pseudo-words, pseudo words were created from words with the same partof speech and equal frequency in the training corpus.</nextsent>
            <nextsent>We selected nouns occurring more than 5,000 times in a 2009 Wikipedia snapshot and then drew 5,000 contexts for each.</nextsent>
            <nextsent>
               The snapshot was tagged with the Stanford Part of Speech Tagger (Toutanova et al, 2003
               <papid>N03-1033</papid>
               ) and parsed with the Malt Parser (Nivre et al, 2006).To evaluate the impact of sense similarity, pseudo words were created from word pairs with a broad range of lexical similarities.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>WSI Performance on Related Senses.</section>
      <citcontext>
         <prevsection>
            <prevsent>Therefore, we employ a pseudo-word discrimination task where a base word and a second word, its confounder,are replaced throughout the corpus with a pseudo word.</prevsent>
            <prevsent>The objective is then to determine which of the words was originally present given the context of an occurrence of the pseudo-word.</prevsent>
         </prevsection>
         <citsent citstr=" N01-1013 ">
            Due to not requiring manual annotation, this type of task was initially proposed as a substitute for word sense disambiguation (Schutze, 1992; Gale et al, 1992
            <papid>W07-2037</papid>
            ) and for selectional preferences (Clark and Weir, 2002
            <papid>N01-1013</papid>
            ).Following the suggestions of Chambers and Ju festival laws offices 0.13660 interests 0.18289 play 0.13751 politics 0.20440 convention 0.20296 governments 0.29125 tournament 0.29007 regulations 0.40761 concerts 0.48348 legislation 0.56112 Table 2: Example confounders for festival?
         </citsent>
         <aftsection>
            <nextsent>and laws?</nextsent>
            <nextsent>and their similaritiesrafsky (2010) on designing pseudo-words, pseudo words were created from words with the same partof speech and equal frequency in the training corpus.</nextsent>
            <nextsent>We selected nouns occurring more than 5,000 times in a 2009 Wikipedia snapshot and then drew 5,000 contexts for each.</nextsent>
            <nextsent>
               The snapshot was tagged with the Stanford Part of Speech Tagger (Toutanova et al, 2003
               <papid>N03-1033</papid>
               ) and parsed with the Malt Parser (Nivre et al, 2006).To evaluate the impact of sense similarity, pseudo words were created from word pairs with a broad range of lexical similarities.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>WSI Performance on Related Senses.</section>
      <citcontext>
         <prevsection>
            <prevsent>and their similaritiesrafsky (2010) on designing pseudo-words, pseudo words were created from words with the same partof speech and equal frequency in the training corpus.</prevsent>
            <prevsent>We selected nouns occurring more than 5,000 times in a 2009 Wikipedia snapshot and then drew 5,000 contexts for each.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1033 ">
            The snapshot was tagged with the Stanford Part of Speech Tagger (Toutanova et al, 2003
            <papid>N03-1033</papid>
            ) and parsed with the Malt Parser (Nivre et al, 2006).To evaluate the impact of sense similarity, pseudo words were created from word pairs with a broad range of lexical similarities.
         </citsent>
         <aftsection>
            <nextsent>We selected lexical similarity as an approximation of sense similarity in order to model the hypothesis that similar senses may appear in similar contexts.</nextsent>
            <nextsent>Similarity scores were calculated using cosine similarity on contextual distributions built from a sliding 2 word window over the Wikipedia corpus.</nextsent>
            <nextsent>Table 2 highlights several example confounders and their similarities with the base term.</nextsent>
            <nextsent>In total, we generated 5000 term confounder pairs from 98 base terms, with a mean of 51 confounders per term.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Sense Confusion in SemEval-2 Task 14.</section>
      <citcontext>
         <prevsection>
            <prevsent>Systems are provided with an unlabeled training corpus consisting of 879,807 multi-sentence contexts for 100 polysemous words, comprised of 50 nouns and 50verbs.</prevsent>
            <prevsent>Systems induce sense representations for target words from the training corpus and then use those representations to label the senses of the target words in unseen contexts from a test corpus.</prevsent>
         </prevsection>
         <citsent citstr=" N09-4006 ">
            The induced senses are then evaluated against the gold standard labels OntoNotes (Hovy et al, 2006
            <papid>N09-4006</papid>
            ) senses labels for the test corpus.
         </citsent>
         <aftsection>
            <nextsent>
               For our evaluation,we use both the two contrasting unsupervised measures, the paired FScore (Artiles et al, 2009
               <papid>D09-1056</papid>
               ) and the V-Measure (Rosenberg and Hirsch berg, 2007
               <papid>D07-1043</papid>
               ), and a supervised measure.
            </nextsent>
            <nextsent>For each metric, we use the evaluation framework provided by the organizers of SemEval-2 Task 14.1The V-Measure rates the homogeneity and completeness of a clustering solution.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Sense Confusion in SemEval-2 Task 14.</section>
      <citcontext>
         <prevsection>
            <prevsent>Systems induce sense representations for target words from the training corpus and then use those representations to label the senses of the target words in unseen contexts from a test corpus.</prevsent>
            <prevsent>
               The induced senses are then evaluated against the gold standard labels OntoNotes (Hovy et al, 2006
               <papid>N09-4006</papid>
               ) senses labels for the test corpus.
            </prevsent>
         </prevsection>
         <citsent citstr=" D09-1056 ">
            For our evaluation,we use both the two contrasting unsupervised measures, the paired FScore (Artiles et al, 2009
            <papid>D09-1056</papid>
            ) and the V-Measure (Rosenberg and Hirsch berg, 2007
            <papid>D07-1043</papid>
            ), and a supervised measure.
         </citsent>
         <aftsection>
            <nextsent>For each metric, we use the evaluation framework provided by the organizers of SemEval-2 Task 14.1The V-Measure rates the homogeneity and completeness of a clustering solution.</nextsent>
            <nextsent>Solutions that have word clusters formed from one gold-standard sense are homogeneous; completeness measures the degree to which a gold-standard senses instances are assigned to a single cluster.</nextsent>
            <nextsent>The paired FScore measures two types of overlap of a solution and thegold standard in cluster assignments for all in pairwise combination of instances.</nextsent>
            <nextsent>This score tends to penalize solutions with many small clusters and highly heterogeneous clusters (Manandhar and Kla paftis, 2009).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Sense Confusion in SemEval-2 Task 14.</section>
      <citcontext>
         <prevsection>
            <prevsent>Systems induce sense representations for target words from the training corpus and then use those representations to label the senses of the target words in unseen contexts from a test corpus.</prevsent>
            <prevsent>
               The induced senses are then evaluated against the gold standard labels OntoNotes (Hovy et al, 2006
               <papid>N09-4006</papid>
               ) senses labels for the test corpus.
            </prevsent>
         </prevsection>
         <citsent citstr=" D07-1043 ">
            For our evaluation,we use both the two contrasting unsupervised measures, the paired FScore (Artiles et al, 2009
            <papid>D09-1056</papid>
            ) and the V-Measure (Rosenberg and Hirsch berg, 2007
            <papid>D07-1043</papid>
            ), and a supervised measure.
         </citsent>
         <aftsection>
            <nextsent>For each metric, we use the evaluation framework provided by the organizers of SemEval-2 Task 14.1The V-Measure rates the homogeneity and completeness of a clustering solution.</nextsent>
            <nextsent>Solutions that have word clusters formed from one gold-standard sense are homogeneous; completeness measures the degree to which a gold-standard senses instances are assigned to a single cluster.</nextsent>
            <nextsent>The paired FScore measures two types of overlap of a solution and thegold standard in cluster assignments for all in pairwise combination of instances.</nextsent>
            <nextsent>This score tends to penalize solutions with many small clusters and highly heterogeneous clusters (Manandhar and Kla paftis, 2009).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Sense Confusion in SemEval-2 Task 14.</section>
      <citcontext>
         <prevsection>
            <prevsent>Each OntoNotes sense si is mapped to a set of WordNet 3.0 senses Si = {wn1, . . .</prevsent>
            <prevsent>, wnn} using 2We suspect that this is in part because a words OntoNotes senses have been designed to minimize sense confusion.</prevsent>
         </prevsection>
         <citsent citstr=" N04-3012 ">
            the sense mapping provided by the CoNLL shared task.3 The sense similarity for two OntoNotes senses is computed using one of two methods: sim = 1|S1||S2| ? wniS1,wnjS2 JCN(wni, wnj), (1) or sim = argmax wniS1,wnjS2 JCN(wni, wnj), (2)where JCN indicates the Jiang-Conrath similarity of two WordNet senses, calculated using WordNet::Similarity (Pedersen et al, 2004
            <papid>N04-3012</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Eq.</nextsent>
            <nextsent>1 computes similarity as the average similarity of all pairwise WordNet sense combinations, while Eq.</nextsent>
            <nextsent>2 uses the highest similarity.</nextsent>
            <nextsent>The resulting OntoNote sense similarities range from 0 to 1, with 1 being maximally similar.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Sense Confusion in SemEval-2 Task 14.</section>
      <citcontext>
         <prevsection>
            <prevsent>We did not find any consistent trends between the.</prevsent>
            <prevsent>V-Measure, purity, and p-value among these models.</prevsent>
         </prevsection>
         <citsent citstr=" S10-1081 ">
            The top F-Scoring models all used either a first or second order co-occurrence feature space similar to ours (Kern et al, 2010; Pedersen, 2010
            <papid>S10-1081</papid>
            ), whereas the top supervised score was achieved by a graph based system (Klapaftis and Manandhar, 2008).
         </citsent>
         <aftsection>
            <nextsent>We presented a two evaluation for WSI approaches and examined the performance of a wide range of algorithms.</nextsent>
            <nextsent>The results raise a potential issue forclustering-based WSI approaches: sense discrimination degrades notably as the sense relatedness increases.</nextsent>
            <nextsent>We highlight three potential avenues for future research.</nextsent>
            <nextsent>
               First, this methodology should be applied to additional WSI models, such as graph based (Klapaftis and Manandhar, 2008; Navigli and Crisafulli, 2010
               <papid>D10-1012</papid>
               ) and probabilistic models (Dinu and Lapata, 2010
               <papid>D10-1113</papid>
               ; Elshamy et al, 2010
               <papid>S10-1082</papid>
               ).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Future Work and Conclusion.</section>
      <citcontext>
         <prevsection>
            <prevsent>The results raise a potential issue forclustering-based WSI approaches: sense discrimination degrades notably as the sense relatedness increases.</prevsent>
            <prevsent>We highlight three potential avenues for future research.</prevsent>
         </prevsection>
         <citsent citstr=" D10-1012 ">
            First, this methodology should be applied to additional WSI models, such as graph based (Klapaftis and Manandhar, 2008; Navigli and Crisafulli, 2010
            <papid>D10-1012</papid>
            ) and probabilistic models (Dinu and Lapata, 2010
            <papid>D10-1113</papid>
            ; Elshamy et al, 2010
            <papid>S10-1082</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Second, weplan to extend the analysis to different sense distributions, varying number of senses, and for human annotated sense similarity data.</nextsent>
            <nextsent>Third, this evaluation makes the simplifying assumption of one sense per instance; however, Erk et al (2009) note that the relations between senses may cause a single word instance to evoke multiple senses within the same context.</nextsent>
            <nextsent>Therefore, a future experiment should consider how WSI systems might address learning senses given the presence of multiple, similar senses for a single instance.All models, associated datasets, testing framework, and scores have been released as a part of the open-source S-Space Package (Jurgens and Stevens, 2010b).5 5 http://code.google.com/p/airhead-research/ 121</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Future Work and Conclusion.</section>
      <citcontext>
         <prevsection>
            <prevsent>The results raise a potential issue forclustering-based WSI approaches: sense discrimination degrades notably as the sense relatedness increases.</prevsent>
            <prevsent>We highlight three potential avenues for future research.</prevsent>
         </prevsection>
         <citsent citstr=" D10-1113 ">
            First, this methodology should be applied to additional WSI models, such as graph based (Klapaftis and Manandhar, 2008; Navigli and Crisafulli, 2010
            <papid>D10-1012</papid>
            ) and probabilistic models (Dinu and Lapata, 2010
            <papid>D10-1113</papid>
            ; Elshamy et al, 2010
            <papid>S10-1082</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Second, weplan to extend the analysis to different sense distributions, varying number of senses, and for human annotated sense similarity data.</nextsent>
            <nextsent>Third, this evaluation makes the simplifying assumption of one sense per instance; however, Erk et al (2009) note that the relations between senses may cause a single word instance to evoke multiple senses within the same context.</nextsent>
            <nextsent>Therefore, a future experiment should consider how WSI systems might address learning senses given the presence of multiple, similar senses for a single instance.All models, associated datasets, testing framework, and scores have been released as a part of the open-source S-Space Package (Jurgens and Stevens, 2010b).5 5 http://code.google.com/p/airhead-research/ 121</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" W11-2214.xml">measuring the impact of sense similarity on word sense induction</title>
      <section>Future Work and Conclusion.</section>
      <citcontext>
         <prevsection>
            <prevsent>The results raise a potential issue forclustering-based WSI approaches: sense discrimination degrades notably as the sense relatedness increases.</prevsent>
            <prevsent>We highlight three potential avenues for future research.</prevsent>
         </prevsection>
         <citsent citstr=" S10-1082 ">
            First, this methodology should be applied to additional WSI models, such as graph based (Klapaftis and Manandhar, 2008; Navigli and Crisafulli, 2010
            <papid>D10-1012</papid>
            ) and probabilistic models (Dinu and Lapata, 2010
            <papid>D10-1113</papid>
            ; Elshamy et al, 2010
            <papid>S10-1082</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Second, weplan to extend the analysis to different sense distributions, varying number of senses, and for human annotated sense similarity data.</nextsent>
            <nextsent>Third, this evaluation makes the simplifying assumption of one sense per instance; however, Erk et al (2009) note that the relations between senses may cause a single word instance to evoke multiple senses within the same context.</nextsent>
            <nextsent>Therefore, a future experiment should consider how WSI systems might address learning senses given the presence of multiple, similar senses for a single instance.All models, associated datasets, testing framework, and scores have been released as a part of the open-source S-Space Package (Jurgens and Stevens, 2010b).5 5 http://code.google.com/p/airhead-research/ 121</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>As a consequence, many methods have been proposed for generating large paraphrase resources (Lin and Pantel, 2001; Szpektor et al2004; Dolan et al2004).</prevsent>
            <prevsent>One ofthe intuitively appropriate data sources for such collections are parallel or comparable corpora: if two texts are translations of the same foreign document, or if they describe the same underlying scenario, they should contain a reasonable number of sentence pairs that convey the same meaning.</prevsent>
         </prevsection>
         <citsent citstr=" N03-1003 ">
            Most approaches that extract paraphrases from parallel texts employ some type of pattern match ing: sentences with the same meaning are assumed to share many n-grams (Barzilay and Lee, 2003
            <papid>N03-1003</papid>
            ; Callison-Burch, 2008
            <papid>D08-1021</papid>
            , among others), many words in their context (Barzilay and McKeown, 2001
            <papid>P01-1008</papid>
            ) or certain slots in a dependency path (Lin and Pantel, 2001; Szpektor et al2004).
         </citsent>
         <aftsection>
            <nextsent>Discourse structure has only marginally been considered for this task: For example, Dolan et al2004) extract the first sentences from comparable articles and take them as paraphrases.</nextsent>
            <nextsent>
               Another approach (Delger and Zweigenbaum, 2009
               <papid>W09-3102</papid>
               ) matches similar paragraphs incomparable texts, creating smaller comparable documents for paraphrase extraction.We believe that discourse structure delivers important information for the extraction of paraphrases.
            </nextsent>
            <nextsent>Sentences that play the same role in a certain discourse and have a similar discourse context can be paraphrases, even if a semantic similarity model does not consider them very similar.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>As a consequence, many methods have been proposed for generating large paraphrase resources (Lin and Pantel, 2001; Szpektor et al2004; Dolan et al2004).</prevsent>
            <prevsent>One ofthe intuitively appropriate data sources for such collections are parallel or comparable corpora: if two texts are translations of the same foreign document, or if they describe the same underlying scenario, they should contain a reasonable number of sentence pairs that convey the same meaning.</prevsent>
         </prevsection>
         <citsent citstr=" D08-1021 ">
            Most approaches that extract paraphrases from parallel texts employ some type of pattern match ing: sentences with the same meaning are assumed to share many n-grams (Barzilay and Lee, 2003
            <papid>N03-1003</papid>
            ; Callison-Burch, 2008
            <papid>D08-1021</papid>
            , among others), many words in their context (Barzilay and McKeown, 2001
            <papid>P01-1008</papid>
            ) or certain slots in a dependency path (Lin and Pantel, 2001; Szpektor et al2004).
         </citsent>
         <aftsection>
            <nextsent>Discourse structure has only marginally been considered for this task: For example, Dolan et al2004) extract the first sentences from comparable articles and take them as paraphrases.</nextsent>
            <nextsent>
               Another approach (Delger and Zweigenbaum, 2009
               <papid>W09-3102</papid>
               ) matches similar paragraphs incomparable texts, creating smaller comparable documents for paraphrase extraction.We believe that discourse structure delivers important information for the extraction of paraphrases.
            </nextsent>
            <nextsent>Sentences that play the same role in a certain discourse and have a similar discourse context can be paraphrases, even if a semantic similarity model does not consider them very similar.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>As a consequence, many methods have been proposed for generating large paraphrase resources (Lin and Pantel, 2001; Szpektor et al2004; Dolan et al2004).</prevsent>
            <prevsent>One ofthe intuitively appropriate data sources for such collections are parallel or comparable corpora: if two texts are translations of the same foreign document, or if they describe the same underlying scenario, they should contain a reasonable number of sentence pairs that convey the same meaning.</prevsent>
         </prevsection>
         <citsent citstr=" P01-1008 ">
            Most approaches that extract paraphrases from parallel texts employ some type of pattern match ing: sentences with the same meaning are assumed to share many n-grams (Barzilay and Lee, 2003
            <papid>N03-1003</papid>
            ; Callison-Burch, 2008
            <papid>D08-1021</papid>
            , among others), many words in their context (Barzilay and McKeown, 2001
            <papid>P01-1008</papid>
            ) or certain slots in a dependency path (Lin and Pantel, 2001; Szpektor et al2004).
         </citsent>
         <aftsection>
            <nextsent>Discourse structure has only marginally been considered for this task: For example, Dolan et al2004) extract the first sentences from comparable articles and take them as paraphrases.</nextsent>
            <nextsent>
               Another approach (Delger and Zweigenbaum, 2009
               <papid>W09-3102</papid>
               ) matches similar paragraphs incomparable texts, creating smaller comparable documents for paraphrase extraction.We believe that discourse structure delivers important information for the extraction of paraphrases.
            </nextsent>
            <nextsent>Sentences that play the same role in a certain discourse and have a similar discourse context can be paraphrases, even if a semantic similarity model does not consider them very similar.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Introduction</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Most approaches that extract paraphrases from parallel texts employ some type of pattern match ing: sentences with the same meaning are assumed to share many n-grams (Barzilay and Lee, 2003
               <papid>N03-1003</papid>
               ; Callison-Burch, 2008
               <papid>D08-1021</papid>
               , among others), many words in their context (Barzilay and McKeown, 2001
               <papid>P01-1008</papid>
               ) or certain slots in a dependency path (Lin and Pantel, 2001; Szpektor et al2004).
            </prevsent>
            <prevsent>Discourse structure has only marginally been considered for this task: For example, Dolan et al2004) extract the first sentences from comparable articles and take them as paraphrases.</prevsent>
         </prevsection>
         <citsent citstr=" W09-3102 ">
            Another approach (Delger and Zweigenbaum, 2009
            <papid>W09-3102</papid>
            ) matches similar paragraphs incomparable texts, creating smaller comparable documents for paraphrase extraction.We believe that discourse structure delivers important information for the extraction of paraphrases.
         </citsent>
         <aftsection>
            <nextsent>Sentences that play the same role in a certain discourse and have a similar discourse context can be paraphrases, even if a semantic similarity model does not consider them very similar.</nextsent>
            <nextsent>This extends the widely applied distributional hypothesis to the discourse level: According to the distributional hypothesis, entities are similar if they share similar contexts.</nextsent>
            <nextsent>In our case, entities are whole sentences, and contexts are discourse units.</nextsent>
            <nextsent>Based on this assumption, we propose a novel method for collecting paraphrases from parallel texts using discourse information.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>Both parallel corpora and comparable corpora have been quite well studied.</prevsent>
            <prevsent>Barzilay and McKeown (2001) use different English translations of the same novels (i.e., monolingual parallel corpora),while others (Quirk et al2004) experiment on multiple sources of the same news/events, i.e., monolingual comparable corpora.</prevsent>
         </prevsection>
         <citsent citstr=" W11-1208 ">
            Commonly used (candidate) comparable corpora are news articles written by different news agencies within a limited time window (Wang and Callison-Burch, 2011
            <papid>W11-1208</papid>
            ).
         </citsent>
         <aftsection>
            <nextsent>Other studies focus on extracting paraphrases from large bilingual parallel corpora, which the machine translation (MT) community provides in many varieties.</nextsent>
            <nextsent>
               Bannard and Callison-Burch (2005)
               <papid>P05-1074</papid>
               as well as Zhao et al2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot phrase.
            </nextsent>
            <nextsent>As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be ex tracted.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Commonly used (candidate) comparable corpora are news articles written by different news agencies within a limited time window (Wang and Callison-Burch, 2011
               <papid>W11-1208</papid>
               ).
            </prevsent>
            <prevsent>Other studies focus on extracting paraphrases from large bilingual parallel corpora, which the machine translation (MT) community provides in many varieties.</prevsent>
         </prevsection>
         <citsent citstr=" P05-1074 ">
            Bannard and Callison-Burch (2005)
            <papid>P05-1074</papid>
            as well as Zhao et al2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot phrase.
         </citsent>
         <aftsection>
            <nextsent>As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted.</nextsent>
            <nextsent>
               The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999
               <papid>W99-0201</papid>
               ; Tomadaki and Salway, 2005).
            </nextsent>
            <nextsent>
               Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001
               <papid>P01-1008</papid>
               ; Barzilay and Lee, 2003
               <papid>N03-1003</papid>
               ; Dolan etal., 2004; Quirk et al2004).
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Bannard and Callison-Burch (2005)
               <papid>P05-1074</papid>
               as well as Zhao et al2008) take one language as the pivot and match two possible translations in the other languages as paraphrases if they share a common pivot phrase.
            </prevsent>
            <prevsent>As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be ex tracted.</prevsent>
         </prevsection>
         <citsent citstr=" W99-0201 ">
            The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999
            <papid>W99-0201</papid>
            ; Tomadaki and Salway, 2005).
         </citsent>
         <aftsection>
            <nextsent>
               Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001
               <papid>P01-1008</papid>
               ; Barzilay and Lee, 2003
               <papid>N03-1003</papid>
               ; Dolan etal., 2004; Quirk et al2004).
            </nextsent>
            <nextsent>Our approach for sentential paraphrase extraction is related to the one introduced by Barzilay and Lee (2003), who also employ multiple sequence alignment (MSA).</nextsent>
         </aftsection>
      </citcontext>
      <tag>NE</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted.</prevsent>
            <prevsent>
               The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999
               <papid>W99-0201</papid>
               ; Tomadaki and Salway, 2005).
            </prevsent>
         </prevsection>
         <citsent citstr=" P01-1008 ">
            Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001
            <papid>P01-1008</papid>
            ; Barzilay and Lee, 2003
            <papid>N03-1003</papid>
            ; Dolan etal., 2004; Quirk et al2004).
         </citsent>
         <aftsection>
            <nextsent>Our approach for sentential paraphrase extraction is related to the one introduced by Barzilay and Lee (2003), who also employ multiple sequence alignment (MSA).</nextsent>
            <nextsent>However, they use MSA at the sentence level rather than at the discourse level.</nextsent>
            <nextsent>We take some core ideas from our previous work on mining script information (Regneri et al2010).</nextsent>
            <nextsent>In this earlier work, we focused on event structures and their possible realizations in natural language.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>As parallel corpora have many alternative ways of expressing the same foreign language concept, large quantities of paraphrase pairs can be extracted.</prevsent>
            <prevsent>
               The paraphrasing task is also strongly related to cross-document event coreference resolution, which is tackled by similar techniques used by the available paraphrasing systems (Bagga and Baldwin, 1999
               <papid>W99-0201</papid>
               ; Tomadaki and Salway, 2005).
            </prevsent>
         </prevsection>
         <citsent citstr=" N03-1003 ">
            Most work in paraphrase acquisition has dealt with sentence-level paraphrases, e.g., (Barzilay and McKeown, 2001
            <papid>P01-1008</papid>
            ; Barzilay and Lee, 2003
            <papid>N03-1003</papid>
            ; Dolan etal., 2004; Quirk et al2004).
         </citsent>
         <aftsection>
            <nextsent>Our approach for sentential paraphrase extraction is related to the one introduced by Barzilay and Lee (2003), who also employ multiple sequence alignment (MSA).</nextsent>
            <nextsent>However, they use MSA at the sentence level rather than at the discourse level.</nextsent>
            <nextsent>We take some core ideas from our previous work on mining script information (Regneri et al2010).</nextsent>
            <nextsent>In this earlier work, we focused on event structures and their possible realizations in natural language.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>In this current work, we target the general task of extracting paraphrases for events rather than the much more specific script related task.</prevsent>
            <prevsent>The current approach uses a domain independent similarity measure instead of a specific hand-crafted similarity score and is thus applicable to standard texts.From an applicational point of view, sentential paraphrases are difficult to use in other NLP tasks.</prevsent>
         </prevsection>
         <citsent citstr=" W03-1609 ">
            At the phrasal level, interchangeable patterns (Shinyama et al2002; Shinyama and Sekine, 2003
            <papid>W03-1609</papid>
            )or inference rules (Lin and Pantel, 2001) are ex tracted.
         </citsent>
         <aftsection>
            <nextsent>In both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e.g., named entities (NE) or content words.</nextsent>
            <nextsent>
               They are quite successful in NE-centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like Recognizing Textual Entailment (Dinu and Wang, 2009
               <papid>E09-1025</papid>
               ).The research on general paraphrase fragment extraction at the sub-sentential level is mainly based 917 on phrase pair extraction techniques from the MTliterature.
            </nextsent>
            <nextsent>
               Munteanu and Marcu (2006)
               <papid>P06-1011</papid>
               extract sub sentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability.
            </nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>In both cases, each pattern or rule contains one or several slots, which are restricted to certain type of words, e.g., named entities (NE) or content words.</prevsent>
            <prevsent>
               They are quite successful in NE-centered tasks, like information extraction, but their level of generalization or coverage is insufficient for applications like Recognizing Textual Entailment (Dinu and Wang, 2009
               <papid>E09-1025</papid>
               ).The research on general paraphrase fragment extraction at the sub-sentential level is mainly based 917 on phrase pair extraction techniques from the MTliterature.
            </prevsent>
         </prevsection>
         <citsent citstr=" P06-1011 ">
            Munteanu and Marcu (2006)
            <papid>P06-1011</papid>
            extract sub sentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability.
         </citsent>
         <aftsection>
            <nextsent>Quirk et al2007) extract fragments using a generative model of noisy translations.</nextsent>
            <nextsent>
               Our own work (Wang and Callison-Burch, 2011
               <papid>W11-1208</papid>
               ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora.
            </nextsent>
            <nextsent>Our current approach also uses word-word alignment,however, we use syntactic dependency trees to compute grammatical fragments.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Related Work.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Munteanu and Marcu (2006)
               <papid>P06-1011</papid>
               extract sub sentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability.
            </prevsent>
            <prevsent>Quirk et al2007) extract fragments using a generative model of noisy translations.</prevsent>
         </prevsection>
         <citsent citstr=" W11-1208 ">
            Our own work (Wang and Callison-Burch, 2011
            <papid>W11-1208</papid>
            ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora.
         </citsent>
         <aftsection>
            <nextsent>Our current approach also uses word-word alignment,however, we use syntactic dependency trees to compute grammatical fragments.</nextsent>
            <nextsent>Our use of dependency trees is inspired by the constituent-tree-based experiments of Callison-Burch (2008).</nextsent>
            <nextsent>Previous approaches have shown that comparable texts provide a good basis for paraphrase extraction.</nextsent>
            <nextsent>We want to show that discourse structure is highly useful for precise and high-yield paraphrase collection from such corpora.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Sentence Matching with MSA.</section>
      <citcontext>
         <prevsection>
            <prevsent>venient size (40 to 150 sentences).</prevsent>
            <prevsent>The result is one comparable document collection per episode.</prevsent>
         </prevsection>
         <citsent citstr=" N09-2061 ">
            We applied a sentence splitter (Gillick, 2009
            <papid>N09-2061</papid>
            ) to the documents and treat them as sequences of sentences for further processing.Sequence alignment takes as its input two sequences consisting of elements of some alphabet, and an alphabet-specific score function cm over pairs of sequence elements.
         </citsent>
         <aftsection>
            <nextsent>For insertions and deletions, the algorithm additionally takes gap costs (cgap).</nextsent>
            <nextsent>Multiple Sequence Alignment generalizes pairwise alignment to arbitrarily many sequences.MSA has its main application area in bioinformat ics, where it is used to identify equivalent parts of DNA (Durbin et al1998).</nextsent>
            <nextsent>Our alphabet consists of sentences, and a sequence is an ordered sentence list constituting a recap.</nextsent>
            <nextsent>A Multiple Sequence Alignment results in a table like Fig.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Paraphrase Fragment Extraction.</section>
      <citcontext>
         <prevsection>
            <prevsent>5.1 Preprocessing.</prevsent>
            <prevsent>Before extracting paraphrase fragments, we first pre process all documents as follows:Stanford CoreNLP 2 provides a set of natural language analysis tools.</prevsent>
         </prevsection>
         <citsent citstr=" P03-1054 ">
            We use the part-ofspeech (POS) tagger, the named-entity recognizer, the parser (Klein and Manning, 2003
            <papid>P03-1054</papid>
            ), and the coreference resolution system (Lee etal., 2011).
         </citsent>
         <aftsection>
            <nextsent>In particular, the dependency structures of the parsers output are used for VP 2http://nlp.stanford.edu/software/ corenlp.shtml fragment extraction (Sec.</nextsent>
            <nextsent>5.3).</nextsent>
            <nextsent>The output from the coreference resolution system is used to cluster all mentions referring to the same entity and to select one as the representative mention.</nextsent>
            <nextsent>If the representative mention is not a pronoun, we modify the original texts by replacing all pronoun mentions in the cluster with the syntactic head of the representative mention.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Paraphrase Fragment Extraction.</section>
      <citcontext>
         <prevsection>
            <prevsent>If the representative mention is not a pronoun, we modify the original texts by replacing all pronoun mentions in the cluster with the syntactic head of the representative mention.</prevsent>
            <prevsent>Note that the coreference resolution system is applied to each recap as a whole.</prevsent>
         </prevsection>
         <citsent citstr=" J03-1002 ">
            GIZA++ (Och and Ney, 2003
            <papid>J03-1002</papid>
            ) is a widely used word aligner for MT systems.
         </citsent>
         <aftsection>
            <nextsent>We amend the input data by copying identical word pairs 10 times and adding them as additional sentence?</nextsent>
            <nextsent>pairs (Byrne et al2003), in order to emphasize the higher alignment probability between identical words.</nextsent>
            <nextsent>We run GIZA++ for bi-directional word alignment and obtain a lexical translation table.</nextsent>
            <nextsent>5.2 Fragment Extraction.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Paraphrase Fragment Extraction.</section>
      <citcontext>
         <prevsection>
            <prevsent>As mentioned in Sec.</prevsent>
            <prevsent>2, we choose to use alignment based approaches to this task, which allows us to use many existing MT techniques and tools.</prevsent>
         </prevsection>
         <citsent citstr=" P06-1011 ">
            We mainly follow our previous approach (Wang and Callison burch, 2011), which is a modified version of an approach by Munteanu and Marcu (2006)
            <papid>P06-1011</papid>
            on translation fragment extraction.
         </citsent>
         <aftsection>
            <nextsent>We briefly review the three-step procedure here and refer the reader to the original paper for more details: 1.</nextsent>
            <nextsent>Establish word-word alignment between each.</nextsent>
            <nextsent>sentence pair using GIZA++; 2.</nextsent>
            <nextsent>Smooth the alignment based on lexical occur-.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Paraphrase Fragment Extraction.</section>
      <citcontext>
         <prevsection>
            <prevsent>Smoothing (step 2) is done for each word by taking the average score ofit and its four neighbor words.</prevsent>
            <prevsent>All the word alignments (excluding stop-words) with positive scores are selected as candidate fragment elements.</prevsent>
         </prevsection>
         <citsent citstr=" W11-1208 ">
            Provided with the candidate fragment elements, we previously (Wang and Callison-Burch, 2011
            <papid>W11-1208</papid>
            ) used a chunker3 to finalize the output fragments, in order to follow the linguistic definition of a (para-) phrase.
         </citsent>
         <aftsection>
            <nextsent>We extend this step in the current system by applying a dependency parser to constrain the boundary of the fragments (Sec.</nextsent>
            <nextsent>5.3).</nextsent>
            <nextsent>Finally, we filter out trivial fragment pairs, such as identical or the original sentence pairs.</nextsent>
            <nextsent>5.3 VP-fragment Extraction.</nextsent>
         </aftsection>
      </citcontext>
      <tag>AAN</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Evaluation.</section>
      <citcontext>
         <prevsection>
            <prevsent>Looking at the paraphrases,27% of the 590 instances in the paraphrase coll category are proper paraphrases, and 73% of them contain additional information that does not belong to the paraphrased part.</prevsent>
            <prevsent>Experimental SetupWe compute precision, recall and f-score with respect to the gold standard (paraphrases are members of paraphrasecoll), taking f-score as follows: f -score = 2 ? precision ? recall precision+ recall We also compute accuracy as the overall fraction of correct labels (negative and positive ones).Our main system uses MSA (denoted by MSA af terwards) with vector-based similarities (VEC) as a scoring function.</prevsent>
         </prevsection>
         <citsent citstr=" C04-1072 ">
            The gap costs are optimized for f-score, resulting in cgap = 0.4 To show the contribution of MSAs structural component and compare it to the vector models contribution, we create a second MSA-based system that uses MSA with BLEU scores (Papineni et al., 2002
            <papid>C04-1072</papid>
            ) as scoring function (MSA+BLEU).
         </citsent>
         <aftsection>
            <nextsent>BLEU establishes the average 1-to-4-gram overlap of twosentences.</nextsent>
            <nextsent>The gap costs for this baseline were optimized separately, ending up with cgap = 1.In order to quantify the contribution of the alignment, we create a discourse-unaware baseline by dropping the MSA and using a state-of-the-art clustering algorithm (Noack, 2007) fed with the vector space model scores (CLUSTER+VEC).</nextsent>
            <nextsent>The algorithm partitions the set of sentences into paraphrase clusters such that the most similar sentences end up in one cluster.</nextsent>
            <nextsent>This does not require any parameter tuning.We also show a baseline that uses the clustering algorithm with BLEU scores (CLUSTER+BLEU).</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Evaluation.</section>
      <citcontext>
         <prevsection>
            <prevsent>One possible explanation lies in picking f-score as objective for the optimization of the gap costs forMSA: For the naturally more restrictive word overlap measure, this leads to a more recall-oriented system with a low threshold for aligning sentences, whereas the gap costs for the vector-based system favors a more restrictive alignment with more precise results.The comparison of the two MSA-based systems highlights the great benefit of using structural knowledge: Both MSA+BLEU and MSA+VEC have comparable f-scores and accuracy.</prevsent>
            <prevsent>The advantage from using the vector-space model that is still obvious for the clustering baselines is nearly evened out when adding discourse knowledge as a backbone.</prevsent>
         </prevsection>
         <citsent citstr=" I05-5002 ">
            However, the vector model still results in nominally higher precision and accuracy.It is hard to do a direct comparison with state of-the-art paraphrase recognition systems, because most are evaluated on different corpora, e.g., the Microsoft paraphrase corpus (Dolan and Brockett, 2005
            <papid>I05-5002</papid>
            , MSR).
         </citsent>
         <aftsection>
            <nextsent>We cannot apply our system to theMSR corpus, because we take complete texts as in put, while the MSR corpus solely delivers sentence pairs.</nextsent>
            <nextsent>While the MSR corpus is larger than our collection, the wording variations in its paraphrase pairs are usually lower than for our examples.</nextsent>
            <nextsent>
               Thus the final numbers of previous approaches might be vaguely comparable with our results: Das and Smith (2009)
               <papid>P09-1053</papid>
               present two systems reaching f-scores of 0.82 and 0.83, with a precision of 0.75 and 0.80.
            </nextsent>
            <nextsent>Both precision and f-scores of our msa-based systems lie within the same range.</nextsent>
         </aftsection>
      </citcontext>
      <tag>MD</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Evaluation.</section>
      <citcontext>
         <prevsection>
            <prevsent>We cannot apply our system to theMSR corpus, because we take complete texts as in put, while the MSR corpus solely delivers sentence pairs.</prevsent>
            <prevsent>While the MSR corpus is larger than our collection, the wording variations in its paraphrase pairs are usually lower than for our examples.</prevsent>
         </prevsection>
         <citsent citstr=" P09-1053 ">
            Thus the final numbers of previous approaches might be vaguely comparable with our results: Das and Smith (2009)
            <papid>P09-1053</papid>
            present two systems reaching f-scores of 0.82 and 0.83, with a precision of 0.75 and 0.80.
         </citsent>
         <aftsection>
            <nextsent>Both precision and f-scores of our msa-based systems lie within the same range.</nextsent>
            <nextsent>
               Heilman and Smith (2010)
               <papid>N10-1145</papid>
               introduce a recall-oriented system, which reaches an f-score of 0.81 by a precision of 0.76.
            </nextsent>
            <nextsent>Compared to this system, our approach results in better precision values.All further computations bases on the system using MSA and the vector space model (MSA+VEC),because it achieves the highest precision and accuracy values.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
   <cited>
      <title id=" D12-1084.xml">using discourse information for paraphrase extraction</title>
      <section>Evaluation.</section>
      <citcontext>
         <prevsection>
            <prevsent>
               Thus the final numbers of previous approaches might be vaguely comparable with our results: Das and Smith (2009)
               <papid>P09-1053</papid>
               present two systems reaching f-scores of 0.82 and 0.83, with a precision of 0.75 and 0.80.
            </prevsent>
            <prevsent>Both precision and f-scores of our msa-based systems lie within the same range.</prevsent>
         </prevsection>
         <citsent citstr=" N10-1145 ">
            Heilman and Smith (2010)
            <papid>N10-1145</papid>
            introduce a recall-oriented system, which reaches an f-score of 0.81 by a precision of 0.76.
         </citsent>
         <aftsection>
            <nextsent>Compared to this system, our approach results in better precision values.All further computations bases on the system using MSA and the vector space model (MSA+VEC),because it achieves the highest precision and accuracy values.</nextsent>
            <nextsent>6.2 Paraphrase Fragment Evaluation.</nextsent>
            <nextsent>We also manually evaluate precision on paraphrase fragments, and additionally describe the productivity of the different setups, providing some intuition about the methods?</nextsent>
            <nextsent>recall.</nextsent>
         </aftsection>
      </citcontext>
      <tag>CM</tag>
   </cited>
</paper>